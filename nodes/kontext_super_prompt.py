"""
Kontext Super Prompt Node
Kontextè¶…çº§æç¤ºè¯ç”ŸæˆèŠ‚ç‚¹ - å¤ç°Visual Prompt Editorå®Œæ•´åŠŸèƒ½

æ¥æ”¶ğŸ¨ LRPG Canvasçš„å›¾å±‚ä¿¡æ¯ï¼Œæä¾›å…¨é¢ç¼–è¾‘åŠŸèƒ½ï¼š
- å±€éƒ¨ç¼–è¾‘ï¼šé’ˆå¯¹é€‰å®šå›¾å±‚çš„ç²¾ç¡®ç¼–è¾‘
- å…¨å±€ç¼–è¾‘ï¼šæ•´ä½“å›¾åƒå¤„ç†æ“ä½œ  
- æ–‡å­—ç¼–è¾‘ï¼šæ–‡æœ¬å†…å®¹ç¼–è¾‘å’Œæ“ä½œ
- ä¸“ä¸šæ“ä½œï¼šé«˜çº§ä¸“ä¸šç¼–è¾‘å·¥å…·
- è‡ªåŠ¨ç”Ÿæˆä¿®é¥°çº¦æŸæ€§æç¤ºè¯
"""

import json
import base64
import time
import random
import os
import sys
from typing import Dict, List, Any, Tuple, Optional
from datetime import datetime

# æ·»åŠ èŠ‚ç‚¹ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„ä»¥å¯¼å…¥å…¶ä»–èŠ‚ç‚¹
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# Optional dependencies
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None

try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

try:
    from server import PromptServer
    COMFY_AVAILABLE = True
except ImportError:
    COMFY_AVAILABLE = False

CATEGORY_TYPE = "ğŸ¨ LRPG Canvas"

class KontextSuperPrompt:
    """
    Kontextè¶…çº§æç¤ºè¯ç”Ÿæˆå™¨èŠ‚ç‚¹
    å¤ç°Visual Prompt Editorçš„å®Œæ•´ç¼–è¾‘åŠŸèƒ½
    """
    
    # åŸºç¡€é”™è¯¯å¤„ç†çš„æç¤ºè¯æ˜ å°„ - åªä¿ç•™æœ€å¸¸ç”¨çš„æ˜ å°„
    BASIC_PROMPT_MAPPING = {
        # åŸºç¡€çº¦æŸæ€§æç¤ºè¯
        'natural blending': 'è‡ªç„¶èåˆ',
        'improved detail': 'ç»†èŠ‚æ”¹å–„', 
        'professional quality': 'ä¸“ä¸šå“è´¨',
        'seamless integration': 'æ— ç¼é›†æˆ',
        
        # åŸºç¡€ä¿®é¥°æ€§æç¤ºè¯
        'enhanced quality': 'å¢å¼ºè´¨é‡',
        'improved visual impact': 'æå‡è§†è§‰æ•ˆæœ',
        'professional finish': 'ä¸“ä¸šå®Œæˆåº¦',
        'artistic excellence': 'è‰ºæœ¯å“è¶Š'
    }
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "layer_info": ("LAYER_INFO",),
                "image": ("IMAGE",),
            },
            "hidden": {
                "unique_id": "UNIQUE_ID",
                "tab_mode": (["manual", "api", "ollama"], {"default": "manual"}),
                "edit_mode": (["å±€éƒ¨ç¼–è¾‘", "å…¨å±€ç¼–è¾‘", "æ–‡å­—ç¼–è¾‘", "ä¸“ä¸šæ“ä½œ"], {"default": "å±€éƒ¨ç¼–è¾‘"}),
                "operation_type": ("STRING", {"default": "", "multiline": False}),
                "description": ("STRING", {"default": "", "multiline": True}),
                "constraint_prompts": ("STRING", {"default": "", "multiline": True}),
                "decorative_prompts": ("STRING", {"default": "", "multiline": True}),
                "selected_layers": ("STRING", {"default": "", "multiline": True}),
                "auto_generate": ("BOOLEAN", {"default": True}),
                "generated_prompt": ("STRING", {"default": "", "multiline": True}),
                
                # APIé€‰é¡¹å¡å‚æ•°
                "api_provider": ("STRING", {"default": "siliconflow"}),
                "api_key": ("STRING", {"default": ""}),
                "api_model": ("STRING", {"default": "deepseek-ai/DeepSeek-V3"}),
                "api_editing_intent": ("STRING", {"default": "general_editing"}),
                "api_processing_style": ("STRING", {"default": "auto_smart"}),
                "api_seed": ("INT", {"default": 0}),
                "api_custom_guidance": ("STRING", {"default": "", "multiline": True}),
                
                # Ollamaé€‰é¡¹å¡å‚æ•°
                "ollama_url": ("STRING", {"default": "http://127.0.0.1:11434"}),
                "ollama_model": ("STRING", {"default": ""}),
                "ollama_temperature": ("FLOAT", {"default": 0.7}),
                "ollama_editing_intent": ("STRING", {"default": "general_editing"}),
                "ollama_processing_style": ("STRING", {"default": "auto_smart"}),
                "ollama_seed": ("INT", {"default": 42}),
                "ollama_custom_guidance": ("STRING", {"default": "", "multiline": True}),
                "ollama_enable_visual": ("BOOLEAN", {"default": False}),
                "ollama_auto_unload": ("BOOLEAN", {"default": False}),
            },
        }
    
    RETURN_TYPES = ("IMAGE", "STRING")
    RETURN_NAMES = ("edited_image", "generated_prompt")
    FUNCTION = "process_super_prompt"
    CATEGORY = CATEGORY_TYPE
    OUTPUT_NODE = False
    
    @classmethod
    def IS_CHANGED(cls, **kwargs):
        # å¼ºåˆ¶æ¯æ¬¡éƒ½é‡æ–°æ‰§è¡Œï¼ŒåŒæ—¶å¼ºåˆ¶åˆ·æ–°èŠ‚ç‚¹å®šä¹‰
        import time
        return str(time.time()) + "_force_refresh"
    
    def process_super_prompt(self, layer_info, image, tab_mode="manual", unique_id="", edit_mode="å±€éƒ¨ç¼–è¾‘", 
                           operation_type="", description="", constraint_prompts="", 
                           decorative_prompts="", selected_layers="", auto_generate=True, 
                           generated_prompt="", 
                           # APIé€‰é¡¹å¡å‚æ•°
                           api_provider="siliconflow", api_key="", api_model="deepseek-ai/DeepSeek-V3",
                           api_editing_intent="general_editing", api_processing_style="auto_smart",
                           api_seed=0, api_custom_guidance="",
                           # Ollamaé€‰é¡¹å¡å‚æ•°  
                           ollama_url="http://127.0.0.1:11434", ollama_model="", ollama_temperature=0.7,
                           ollama_editing_intent="general_editing", ollama_processing_style="auto_smart",
                           ollama_seed=42, ollama_custom_guidance="", ollama_enable_visual=False,
                           ollama_auto_unload=False):
        """
        å¤„ç†Kontextè¶…çº§æç¤ºè¯ç”Ÿæˆ
        """
        try:
            
            # æ ¹æ®é€‰é¡¹å¡æ¨¡å¼å¤„ç†
            if tab_mode == "api" and generated_prompt and generated_prompt.strip():
                final_generated_prompt = generated_prompt.strip()
            elif tab_mode == "api" and api_key:
                final_generated_prompt = self.process_api_mode(
                    layer_info, description, api_provider, api_key, api_model,
                    api_editing_intent, api_processing_style, api_seed, 
                    api_custom_guidance, image
                )
            elif tab_mode == "ollama" and ollama_model:
                final_generated_prompt = self.process_ollama_mode(
                    layer_info, description, ollama_url, ollama_model, ollama_temperature,
                    ollama_editing_intent, ollama_processing_style, ollama_seed,
                    ollama_custom_guidance, ollama_enable_visual, ollama_auto_unload, image
                )
            elif generated_prompt and generated_prompt.strip():
                final_generated_prompt = generated_prompt.strip()
            else:
                # è§£æå›¾å±‚ä¿¡æ¯
                parsed_layer_info = self.parse_layer_info(layer_info)
                
                # è§£æé€‰ä¸­çš„å›¾å±‚
                selected_layer_ids = self.parse_selected_layers(selected_layers)
                
                # è§£æçº¦æŸæ€§å’Œä¿®é¥°æ€§æç¤ºè¯
                constraint_list = self.parse_prompt_list(constraint_prompts)
                decorative_list = self.parse_prompt_list(decorative_prompts)
                
                # ç”ŸæˆåŸºç¡€fallbackæç¤ºè¯
                positive_prompt, negative_prompt, full_description = self.generate_basic_fallback_prompts(
                    edit_mode=edit_mode,
                    operation_type=operation_type,
                    description=description,
                    constraint_prompts=constraint_list,
                    decorative_prompts=decorative_list
                )
                
                # åˆå¹¶æ‰€æœ‰æç¤ºè¯ä¿¡æ¯ä¸ºä¸€ä¸ªå®Œæ•´çš„ç”Ÿæˆæç¤ºè¯
                final_generated_prompt = f"{positive_prompt}\n\nNegative: {negative_prompt}\n\n{full_description}"
            
            # æ„å»ºç¼–è¾‘æ•°æ®ï¼ˆç”¨äºè°ƒè¯•å’Œæ‰©å±•ï¼‰
            edit_data = {
                'node_id': unique_id,
                'edit_mode': edit_mode,
                'operation_type': operation_type,
                'description': description,
                'generated_prompt_source': 'frontend' if generated_prompt and generated_prompt.strip() else 'backend',
                'timestamp': time.time()
            }
            
            return (image, final_generated_prompt)
            
        except Exception as e:
            print(f"[Kontext Super Prompt] å¤„ç†é”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()
            
            # è¿”å›é»˜è®¤å€¼
            default_edit_data = {
                'node_id': unique_id,
                'edit_mode': edit_mode,
                'error': str(e),
                'timestamp': time.time()
            }
            return (image, "å¤„ç†å‡ºé”™ï¼š" + str(e))
    
    def parse_layer_info(self, layer_info):
        """è§£æå›¾å±‚ä¿¡æ¯"""
        if isinstance(layer_info, dict):
            return layer_info
        return {}
    
    def parse_selected_layers(self, selected_layers_str):
        """è§£æé€‰ä¸­çš„å›¾å±‚"""
        if not selected_layers_str:
            return []
        try:
            return json.loads(selected_layers_str)
        except:
            return []
    
    def parse_prompt_list(self, prompt_str):
        """è§£ææç¤ºè¯åˆ—è¡¨"""
        if not prompt_str:
            return []
        
        # æ”¯æŒå¤šç§åˆ†éš”ç¬¦
        prompts = []
        for line in prompt_str.split('\n'):
            line = line.strip()
            if line:
                # æ”¯æŒé€—å·åˆ†éš”
                if ',' in line:
                    prompts.extend([p.strip() for p in line.split(',') if p.strip()])
                else:
                    prompts.append(line)
        return prompts
    
    def translate_basic_prompts(self, prompts):
        """å°†åŸºç¡€è‹±æ–‡æç¤ºè¯è½¬æ¢ä¸ºä¸­æ–‡æ˜¾ç¤º"""
        translated = []
        for prompt in prompts:
            if prompt in self.BASIC_PROMPT_MAPPING:
                translated.append(self.BASIC_PROMPT_MAPPING[prompt])
            else:
                translated.append(prompt)  # ä¿æŒåŸæ–‡ï¼Œå¦‚æœæ²¡æœ‰æ˜ å°„
        return translated
    
    def generate_fallback_prompt(self, edit_mode, operation_type, description):
        """ç”ŸæˆåŸºç¡€fallbackæç¤ºè¯ - ä»…åœ¨å‰ç«¯æœªæä¾›æ—¶ä½¿ç”¨"""
        # åŸºç¡€æç¤ºè¯æ¨¡æ¿
        basic_templates = {
            'change_color': f'change color to {description or "specified color"}',
            'blur_background': f'blur background while keeping {description or "subject"} sharp',
            'enhance_quality': f'enhance quality of {description or "image"}',
        }
        
        # åŸºç¡€çº¦æŸå’Œä¿®é¥°è¯
        basic_constraints = ['natural blending', 'seamless integration']
        basic_decoratives = ['improved detail', 'enhanced quality']
        
        # æ„å»ºåŸºç¡€æç¤ºè¯
        if operation_type and operation_type in basic_templates:
            base_prompt = basic_templates[operation_type]
        else:
            base_prompt = f"{edit_mode}: {description or 'apply editing'}"
        
        return base_prompt, basic_constraints, basic_decoratives
    
    def generate_basic_fallback_prompts(self, edit_mode, operation_type, description, 
                                       constraint_prompts, decorative_prompts):
        """ç”ŸæˆåŸºç¡€fallbackæç¤ºè¯ - ä»…åœ¨å‰ç«¯å®Œå…¨å¤±æ•ˆæ—¶ä½¿ç”¨"""
        # ä½¿ç”¨ç²¾ç®€çš„fallbackç”Ÿæˆå™¨
        base_prompt, basic_constraints, basic_decoratives = self.generate_fallback_prompt(
            edit_mode, operation_type, description
        )
        
        # ç»„åˆæç¤ºè¯
        all_constraints = constraint_prompts + basic_constraints
        all_decoratives = decorative_prompts + basic_decoratives
        
        # æ„å»ºæ­£å‘æç¤ºè¯
        positive_parts = [base_prompt]
        if all_constraints:
            positive_parts.extend(all_constraints[:3])  # é™åˆ¶æ•°é‡
        if all_decoratives:
            positive_parts.extend(all_decoratives[:2])   # é™åˆ¶æ•°é‡
        
        positive_prompt = ", ".join(positive_parts)
        
        # åŸºç¡€è´Ÿå‘æç¤ºè¯
        negative_prompt = "artifacts, distortions, unnatural appearance, poor quality, inconsistencies, blurry, low quality, artifacts, distorted, unnatural, poor composition, bad anatomy, incorrect proportions"
        
        # æ„å»ºå®Œæ•´æè¿°
        full_description_parts = [
            f"ç¼–è¾‘æ¨¡å¼ï¼š{edit_mode}",
            f"æ“ä½œç±»å‹ï¼š{operation_type or 'æœªæŒ‡å®š'}",
            f"æè¿°ï¼š{description or 'æœªæä¾›'}",
        ]
        
        if all_constraints:
            constraint_display = self.translate_basic_prompts(all_constraints[:3])
            full_description_parts.append(f"çº¦æŸæ€§æç¤ºè¯ï¼š{', '.join(constraint_display)}")
        
        if all_decoratives:
            decorative_display = self.translate_basic_prompts(all_decoratives[:2])
            full_description_parts.append(f"ä¿®é¥°æ€§æç¤ºè¯ï¼š{', '.join(decorative_display)}")
        
        full_description = " | ".join(full_description_parts)
        
        return positive_prompt, negative_prompt, full_description
    
    def process_api_mode(self, layer_info, description, api_provider, api_key, api_model,
                        editing_intent, processing_style, seed, custom_guidance, image):
        """å¤„ç†APIæ¨¡å¼çš„æç¤ºè¯ç”Ÿæˆ"""
        try:
            import requests
            import re
            import hashlib
            
            if not api_key:
                return f"APIå¯†é’¥ä¸ºç©º: {description or 'æ— æè¿°'}"
            
            # APIæä¾›å•†é…ç½®
            api_configs = {
                'siliconflow': {
                    'base_url': 'https://api.siliconflow.cn/v1/chat/completions',
                    'default_model': 'deepseek-ai/DeepSeek-V3'
                },
                'zhipu': {
                    'base_url': 'https://open.bigmodel.cn/api/paas/v4/chat/completions',
                    'default_model': 'glm-4.5'
                },
                'deepseek': {
                    'base_url': 'https://api.deepseek.com/v1/chat/completions',
                    'default_model': 'deepseek-chat'
                }
            }
            
            # è·å–APIé…ç½®
            api_config = api_configs.get(api_provider, api_configs['siliconflow'])
            model = api_model or api_config['default_model']
            
            # æ„å»ºç³»ç»Ÿæç¤ºè¯ - å¼ºåˆ¶è‹±æ–‡ç‰ˆ v1.3.4
            system_prompt = """You are an English-only image editing specialist. 

CRITICAL: You MUST output in ENGLISH ONLY. Never use Chinese, Japanese, Korean or any other language.

RULES:
1. Output ONE complete ENGLISH instruction (30-60 words)
2. Use ENGLISH color names and terms only
3. All output must be in proper English
4. If user input is in another language, translate to English

FORMAT: [English verb] [target] to/into [English color/result], [English quality terms].

Example: Transform the marked area into deep navy blue, preserving texture naturally.

REMEMBER: ENGLISH ONLY. No Chinese characters. No other languages."""
            
            if editing_intent == "creative_enhancement":
                system_prompt += "\n- Prioritize artistic and creative improvements"
            elif editing_intent == "technical_correction":
                system_prompt += "\n- Focus on technical accuracy and corrections"
            elif editing_intent == "style_transformation":
                system_prompt += "\n- Emphasize style changes and artistic transformation"
            
            if processing_style == "auto_smart":
                system_prompt += "\n- Use intelligent automatic processing"
            elif processing_style == "manual_precise":
                system_prompt += "\n- Require precise manual control"
            elif processing_style == "balanced_hybrid":
                system_prompt += "\n- Balance automatic and manual approaches"
            
            # æ·»åŠ éšæœºå…ƒç´ ç¡®ä¿æ¯æ¬¡ç”Ÿæˆä¸åŒ
            import time
            random_seed = int(time.time() * 1000) % 1000000
            
            # æ„å»ºç”¨æˆ·æç¤ºè¯ - å¼ºåˆ¶è‹±æ–‡
            user_prompt = f"Generate ENGLISH instruction for: {description}"
            if custom_guidance:
                user_prompt += f" Additional: {custom_guidance}"
            user_prompt += f" (v{random_seed})"  # æ·»åŠ å˜ä½“æ ‡è¯†
            user_prompt += "\nOUTPUT IN ENGLISH ONLY. No Chinese or other languages."
            
            # å‘é€APIè¯·æ±‚
            headers = {
                'Content-Type': 'application/json',
                'Authorization': f'Bearer {api_key}'
            }
            
            data = {
                'model': model,
                'messages': [
                    {'role': 'system', 'content': system_prompt},
                    {'role': 'user', 'content': user_prompt}
                ],
                'temperature': 0.7 + (random_seed % 20) / 100,  # 0.7-0.89çš„éšæœºæ¸©åº¦
                'max_tokens': 200,  # è®¾ç½®ä¸º200ï¼Œå¹³è¡¡è´¨é‡å’Œtokenæ¶ˆè€—
                'top_p': 0.9,
                'presence_penalty': 0.1,  # é¿å…é‡å¤
                'frequency_penalty': 0.1,  # å¢åŠ å¤šæ ·æ€§
                'language': 'en'  # å¼ºåˆ¶è‹±æ–‡è¾“å‡ºï¼ˆæŸäº›APIæ”¯æŒï¼‰
            }
            
            response = requests.post(api_config['base_url'], headers=headers, json=data, timeout=30)
            response.raise_for_status()
            
            result = response.json()
            api_response = result['choices'][0]['message']['content']
            
            # è°ƒè¯•ï¼šæ˜¾ç¤ºåŸå§‹å“åº”
            
            # æ¸…ç†å“åº”ï¼Œæå–çº¯å‡€æç¤ºè¯
            cleaned_response = self._clean_api_response(api_response)
            
            # è°ƒè¯•ï¼šæ˜¾ç¤ºæ¸…ç†åçš„å“åº”
            
            
            return cleaned_response
                
        except Exception as e:
            print(f"[Kontext Super Prompt] APIæ¨¡å¼å¤„ç†é”™è¯¯: {e}")
            return f"APIå¤„ç†é”™è¯¯: {description or 'æ— æè¿°'}"
    
    def process_ollama_mode(self, layer_info, description, ollama_url, ollama_model, 
                           temperature, editing_intent, processing_style, seed,
                           custom_guidance, enable_visual, auto_unload, image):
        """å¤„ç†Ollamaæ¨¡å¼çš„æç¤ºè¯ç”Ÿæˆ - å¼ºåˆ¶è‹±æ–‡è¾“å‡º"""
        try:
            import requests
            
            # æ„å»ºå¼ºåˆ¶è‹±æ–‡çš„ç³»ç»Ÿæç¤ºè¯
            system_prompt = """You are an ENGLISH-ONLY image editing assistant using Ollama.

CRITICAL RULES:
1. Output in ENGLISH ONLY
2. Never use Chinese characters or any other language
3. Generate ONE clear English instruction (30-60 words)
4. Use proper English color names and terms

If input is in Chinese, translate to English first.

FORMAT: [English verb] [target] to [English result], [quality terms].

REMEMBER: ENGLISH ONLY OUTPUT."""
            
            # æ„å»ºç”¨æˆ·æç¤ºè¯
            user_prompt = f"Generate ENGLISH editing instruction for: {description}"
            if custom_guidance:
                user_prompt += f" Additional: {custom_guidance}"
            user_prompt += "\nOUTPUT IN ENGLISH ONLY."
            
            # è°ƒç”¨Ollama API
            response = requests.post(
                f"{ollama_url}/api/generate",
                json={
                    "model": ollama_model,
                    "prompt": user_prompt,
                    "system": system_prompt,
                    "temperature": temperature,
                    "seed": seed,
                    "stream": False,
                    "options": {
                        "num_predict": 200,  # é™åˆ¶è¾“å‡ºé•¿åº¦
                        "stop": ["\n\n", "###", "---"],  # åœæ­¢æ ‡è®°
                    }
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                generated_text = result.get('response', '')
                
                # æ¸…ç†å’ŒéªŒè¯è¾“å‡º
                cleaned_text = self._clean_api_response(generated_text)
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
                if any('\u4e00' <= char <= '\u9fff' for char in cleaned_text):
                    # å¦‚æœåŒ…å«ä¸­æ–‡ï¼Œè¿”å›é»˜è®¤è‹±æ–‡
                    return f"Transform marked area as requested: {description}"
                
                return cleaned_text if cleaned_text else f"Transform marked area: {description}"
            else:
                return f"Ollama request failed: {description}"
                
        except Exception as e:
            print(f"[Kontext Super Prompt] Ollamaæ¨¡å¼å¤„ç†é”™è¯¯: {e}")
            # è¿”å›è‹±æ–‡fallback
            return f"Apply editing to marked area: {description}"
    
    def _clean_api_response(self, response):
        """æ¸…ç†APIå“åº”ï¼Œæå–çº¯å‡€è‹±æ–‡æç¤ºè¯"""
        import re
        
        if not response:
            return response
        
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡ï¼Œå¦‚æœæ˜¯çº¯ä¸­æ–‡å“åº”åˆ™è¿”å›é»˜è®¤è‹±æ–‡
        if any('\u4e00' <= char <= '\u9fff' for char in response):
            # å°è¯•æå–è‹±æ–‡éƒ¨åˆ†
            english_parts = re.findall(r'[a-zA-Z][a-zA-Z\s,\.\-]+', response)
            if english_parts:
                # åˆå¹¶è‹±æ–‡éƒ¨åˆ†
                english_text = ' '.join(english_parts)
                if len(english_text) > 20:  # å¦‚æœè‹±æ–‡éƒ¨åˆ†è¶³å¤Ÿé•¿
                    return english_text.strip()
            # å¦åˆ™è¿”å›é»˜è®¤è‹±æ–‡æŒ‡ä»¤
            return "Transform the marked area as specified in the editing request"
        
        # å¦‚æœå“åº”åŒ…å«å¤šä¸ªPromptç¼–å·ï¼Œåªæå–ç¬¬ä¸€ä¸ª
        if '### Prompt' in response or 'Prompt 1:' in response:
            
            # å°è¯•æå–ç¬¬ä¸€ä¸ªå¼•å·å†…çš„æç¤ºè¯
            first_quoted_match = re.search(r'"([^"]{30,})"', response)
            if first_quoted_match:
                return first_quoted_match.group(1).strip()
            
            # å°è¯•æå–ç¬¬ä¸€ä¸ªæç¤ºè¯æ®µè½
            first_prompt_match = re.search(r'(?:Prompt \d+:.*?)"([^"]+)"', response, re.DOTALL)
            if first_prompt_match:
                return first_prompt_match.group(1).strip()
        
        # å°è¯•æå–å¼•å·ä¸­çš„æç¤ºè¯
        quoted_match = re.search(r'"([^"]{30,})"', response)
        if quoted_match:
            return quoted_match.group(1).strip()
        
        # æ¸…ç†æ ‡é¢˜å’Œå‰ç¼€
        patterns_to_remove = [
            r'^###.*$',            # ç§»é™¤Markdownæ ‡é¢˜
            r'^Prompt \d+:.*$',    # ç§»é™¤"Prompt 1:"ç­‰
            r'^---.*$',            # ç§»é™¤åˆ†éš”çº¿
            r'^.*?prompt:\s*',     # ç§»é™¤promptå‰ç¼€
        ]
        
        cleaned = response.strip()
        
        # å°è¯•æå–ä»£ç å—ä¸­çš„æç¤ºè¯
        code_block_match = re.search(r'```[^`]*?\n(.*?)\n```', response, re.DOTALL)
        if code_block_match and len(code_block_match.group(1).strip()) > 20:
            return code_block_match.group(1).strip()
        
        # åº”ç”¨æ¸…ç†æ¨¡å¼
        for pattern in patterns_to_remove:
            cleaned = re.sub(pattern, '', cleaned, flags=re.MULTILINE | re.IGNORECASE)
        
        # æ¸…ç†å¤šä½™ç©ºè¡Œ
        cleaned = re.sub(r'\n{2,}', '\n', cleaned).strip()
        
        # å¦‚æœæ²¡æœ‰åšä»»ä½•å¤„ç†æˆ–ç»“æœå¤ªçŸ­ï¼Œè¿”å›åŸå§‹å†…å®¹
        if not cleaned or len(cleaned) < 10:
            return response.strip()
        
        return cleaned.strip()


# æ³¨å†ŒèŠ‚ç‚¹
NODE_CLASS_MAPPINGS = {
    "KontextSuperPrompt": KontextSuperPrompt,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "KontextSuperPrompt": "ğŸ¯ Kontext Super Prompt",
}

