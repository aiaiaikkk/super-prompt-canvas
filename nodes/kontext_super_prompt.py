"""
Kontext Super Prompt Node
Kontextè¶…çº§æç¤ºè¯ç”ŸæˆèŠ‚ç‚¹ - å¤ç°Visual Prompt Editorå®Œæ•´åŠŸèƒ½

æ¥æ”¶ğŸ¨ LRPG Canvasçš„å›¾å±‚ä¿¡æ¯ï¼Œæä¾›å…¨é¢ç¼–è¾‘åŠŸèƒ½ï¼š
- å±€éƒ¨ç¼–è¾‘ï¼šé’ˆå¯¹é€‰å®šå›¾å±‚çš„ç²¾ç¡®ç¼–è¾‘
- å…¨å±€ç¼–è¾‘ï¼šæ•´ä½“å›¾åƒå¤„ç†æ“ä½œ  
- æ–‡å­—ç¼–è¾‘ï¼šæ–‡æœ¬å†…å®¹ç¼–è¾‘å’Œæ“ä½œ
- ä¸“ä¸šæ“ä½œï¼šé«˜çº§ä¸“ä¸šç¼–è¾‘å·¥å…·
- è‡ªåŠ¨ç”Ÿæˆä¿®é¥°çº¦æŸæ€§æç¤ºè¯
"""

import json
import base64
import time
import random
import os
import sys
from typing import Dict, List, Any, Tuple, Optional
from datetime import datetime

# æ·»åŠ èŠ‚ç‚¹ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„ä»¥å¯¼å…¥å…¶ä»–èŠ‚ç‚¹
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# Optional dependencies
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None

try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    np = None

try:
    from server import PromptServer
    COMFY_AVAILABLE = True
except ImportError:
    COMFY_AVAILABLE = False

CATEGORY_TYPE = "ğŸ¨ LRPG Canvas"

class KontextSuperPrompt:
    """
    Kontextè¶…çº§æç¤ºè¯ç”Ÿæˆå™¨èŠ‚ç‚¹
    å¤ç°Visual Prompt Editorçš„å®Œæ•´ç¼–è¾‘åŠŸèƒ½
    """
    
    # åŸºç¡€é”™è¯¯å¤„ç†çš„æç¤ºè¯æ˜ å°„ - åªä¿ç•™æœ€å¸¸ç”¨çš„æ˜ å°„
    BASIC_PROMPT_MAPPING = {
        # åŸºç¡€çº¦æŸæ€§æç¤ºè¯
        'natural blending': 'è‡ªç„¶èåˆ',
        'improved detail': 'ç»†èŠ‚æ”¹å–„', 
        'professional quality': 'ä¸“ä¸šå“è´¨',
        'seamless integration': 'æ— ç¼é›†æˆ',
        
        # åŸºç¡€ä¿®é¥°æ€§æç¤ºè¯
        'enhanced quality': 'å¢å¼ºè´¨é‡',
        'improved visual impact': 'æå‡è§†è§‰æ•ˆæœ',
        'professional finish': 'ä¸“ä¸šå®Œæˆåº¦',
        'artistic excellence': 'è‰ºæœ¯å“è¶Š'
    }
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "layer_info": ("LAYER_INFO",),
                "image": ("IMAGE",),
                "tab_mode": (["manual", "api", "ollama"], {"default": "manual"}),
            },
            "hidden": {
                "unique_id": "UNIQUE_ID",
                "edit_mode": (["å±€éƒ¨ç¼–è¾‘", "å…¨å±€ç¼–è¾‘", "æ–‡å­—ç¼–è¾‘", "ä¸“ä¸šæ“ä½œ"], {"default": "å±€éƒ¨ç¼–è¾‘"}),
                "operation_type": ("STRING", {"default": "", "multiline": False}),
                "description": ("STRING", {"default": "", "multiline": True}),
                "constraint_prompts": ("STRING", {"default": "", "multiline": True}),
                "decorative_prompts": ("STRING", {"default": "", "multiline": True}),
                "selected_layers": ("STRING", {"default": "", "multiline": True}),
                "auto_generate": ("BOOLEAN", {"default": True}),
                "generated_prompt": ("STRING", {"default": "", "multiline": True}),
                
                # APIé€‰é¡¹å¡å‚æ•°
                "api_provider": ("STRING", {"default": "siliconflow"}),
                "api_key": ("STRING", {"default": ""}),
                "api_model": ("STRING", {"default": "deepseek-ai/DeepSeek-V3"}),
                "api_editing_intent": ("STRING", {"default": "general_editing"}),
                "api_processing_style": ("STRING", {"default": "auto_smart"}),
                "api_seed": ("INT", {"default": 0}),
                "api_custom_guidance": ("STRING", {"default": "", "multiline": True}),
                
                # Ollamaé€‰é¡¹å¡å‚æ•°
                "ollama_url": ("STRING", {"default": "http://127.0.0.1:11434"}),
                "ollama_model": ("STRING", {"default": ""}),
                "ollama_temperature": ("FLOAT", {"default": 0.7}),
                "ollama_editing_intent": ("STRING", {"default": "general_editing"}),
                "ollama_processing_style": ("STRING", {"default": "auto_smart"}),
                "ollama_seed": ("INT", {"default": 42}),
                "ollama_custom_guidance": ("STRING", {"default": "", "multiline": True}),
                "ollama_enable_visual": ("BOOLEAN", {"default": False}),
                "ollama_auto_unload": ("BOOLEAN", {"default": False}),
            },
        }
    
    RETURN_TYPES = ("IMAGE", "STRING")
    RETURN_NAMES = ("edited_image", "generated_prompt")
    FUNCTION = "process_super_prompt"
    CATEGORY = CATEGORY_TYPE
    OUTPUT_NODE = False
    
    @classmethod
    def IS_CHANGED(cls, **kwargs):
        # å¼ºåˆ¶æ¯æ¬¡éƒ½é‡æ–°æ‰§è¡Œ
        return float(time.time())
    
    def process_super_prompt(self, layer_info, image, tab_mode="manual", unique_id="", edit_mode="å±€éƒ¨ç¼–è¾‘", 
                           operation_type="", description="", constraint_prompts="", 
                           decorative_prompts="", selected_layers="", auto_generate=True, 
                           generated_prompt="", 
                           # APIé€‰é¡¹å¡å‚æ•°
                           api_provider="siliconflow", api_key="", api_model="deepseek-ai/DeepSeek-V3",
                           api_editing_intent="general_editing", api_processing_style="auto_smart",
                           api_seed=0, api_custom_guidance="",
                           # Ollamaé€‰é¡¹å¡å‚æ•°  
                           ollama_url="http://127.0.0.1:11434", ollama_model="", ollama_temperature=0.7,
                           ollama_editing_intent="general_editing", ollama_processing_style="auto_smart",
                           ollama_seed=42, ollama_custom_guidance="", ollama_enable_visual=False,
                           ollama_auto_unload=False):
        """
        å¤„ç†Kontextè¶…çº§æç¤ºè¯ç”Ÿæˆ
        """
        try:
            print(f"[Kontext Super Prompt] å¼€å§‹å¤„ç†è¶…çº§æç¤ºè¯ç”Ÿæˆï¼ŒèŠ‚ç‚¹ID: {unique_id}")
            print(f"[Kontext Super Prompt] é€‰é¡¹å¡æ¨¡å¼: {tab_mode}")
            print(f"[Kontext Super Prompt] ç¼–è¾‘æ¨¡å¼: {edit_mode}")
            print(f"[Kontext Super Prompt] æè¿°: '{description}'")
            
            # æ ¹æ®é€‰é¡¹å¡æ¨¡å¼å¤„ç†
            if tab_mode == "api" and api_key:
                print("[Kontext Super Prompt] ä½¿ç”¨APIæ¨¡å¼ç”Ÿæˆæç¤ºè¯")
                final_generated_prompt = self.process_api_mode(
                    layer_info, description, api_provider, api_key, api_model,
                    api_editing_intent, api_processing_style, api_seed, 
                    api_custom_guidance, image
                )
            elif tab_mode == "ollama" and ollama_model:
                print("[Kontext Super Prompt] ä½¿ç”¨Ollamaæ¨¡å¼ç”Ÿæˆæç¤ºè¯")
                final_generated_prompt = self.process_ollama_mode(
                    layer_info, description, ollama_url, ollama_model, ollama_temperature,
                    ollama_editing_intent, ollama_processing_style, ollama_seed,
                    ollama_custom_guidance, ollama_enable_visual, ollama_auto_unload, image
                )
            elif generated_prompt and generated_prompt.strip():
                print("[Kontext Super Prompt] ä½¿ç”¨å‰ç«¯ç”Ÿæˆçš„æç¤ºè¯")
                final_generated_prompt = generated_prompt.strip()
            else:
                print("[Kontext Super Prompt] ä½¿ç”¨æ‰‹åŠ¨æ¨¡å¼ç”Ÿæˆæç¤ºè¯")
                # è§£æå›¾å±‚ä¿¡æ¯
                parsed_layer_info = self.parse_layer_info(layer_info)
                
                # è§£æé€‰ä¸­çš„å›¾å±‚
                selected_layer_ids = self.parse_selected_layers(selected_layers)
                
                # è§£æçº¦æŸæ€§å’Œä¿®é¥°æ€§æç¤ºè¯
                constraint_list = self.parse_prompt_list(constraint_prompts)
                decorative_list = self.parse_prompt_list(decorative_prompts)
                
                # ç”ŸæˆåŸºç¡€fallbackæç¤ºè¯
                positive_prompt, negative_prompt, full_description = self.generate_basic_fallback_prompts(
                    edit_mode=edit_mode,
                    operation_type=operation_type,
                    description=description,
                    constraint_prompts=constraint_list,
                    decorative_prompts=decorative_list
                )
                
                # åˆå¹¶æ‰€æœ‰æç¤ºè¯ä¿¡æ¯ä¸ºä¸€ä¸ªå®Œæ•´çš„ç”Ÿæˆæç¤ºè¯
                final_generated_prompt = f"{positive_prompt}\n\nNegative: {negative_prompt}\n\n{full_description}"
            
            # æ„å»ºç¼–è¾‘æ•°æ®ï¼ˆç”¨äºè°ƒè¯•å’Œæ‰©å±•ï¼‰
            edit_data = {
                'node_id': unique_id,
                'edit_mode': edit_mode,
                'operation_type': operation_type,
                'description': description,
                'generated_prompt_source': 'frontend' if generated_prompt and generated_prompt.strip() else 'backend',
                'timestamp': time.time()
            }
            
            print(f"[Kontext Super Prompt] æœ€ç»ˆç”Ÿæˆæç¤ºè¯æ¥æº: {edit_data['generated_prompt_source']}")
            return (image, final_generated_prompt)
            
        except Exception as e:
            print(f"[Kontext Super Prompt] å¤„ç†é”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()
            
            # è¿”å›é»˜è®¤å€¼
            default_edit_data = {
                'node_id': unique_id,
                'edit_mode': edit_mode,
                'error': str(e),
                'timestamp': time.time()
            }
            return (image, "å¤„ç†å‡ºé”™ï¼š" + str(e))
    
    def parse_layer_info(self, layer_info):
        """è§£æå›¾å±‚ä¿¡æ¯"""
        if isinstance(layer_info, dict):
            return layer_info
        return {}
    
    def parse_selected_layers(self, selected_layers_str):
        """è§£æé€‰ä¸­çš„å›¾å±‚"""
        if not selected_layers_str:
            return []
        try:
            return json.loads(selected_layers_str)
        except:
            return []
    
    def parse_prompt_list(self, prompt_str):
        """è§£ææç¤ºè¯åˆ—è¡¨"""
        if not prompt_str:
            return []
        
        # æ”¯æŒå¤šç§åˆ†éš”ç¬¦
        prompts = []
        for line in prompt_str.split('\n'):
            line = line.strip()
            if line:
                # æ”¯æŒé€—å·åˆ†éš”
                if ',' in line:
                    prompts.extend([p.strip() for p in line.split(',') if p.strip()])
                else:
                    prompts.append(line)
        return prompts
    
    def translate_basic_prompts(self, prompts):
        """å°†åŸºç¡€è‹±æ–‡æç¤ºè¯è½¬æ¢ä¸ºä¸­æ–‡æ˜¾ç¤º"""
        translated = []
        for prompt in prompts:
            if prompt in self.BASIC_PROMPT_MAPPING:
                translated.append(self.BASIC_PROMPT_MAPPING[prompt])
            else:
                translated.append(prompt)  # ä¿æŒåŸæ–‡ï¼Œå¦‚æœæ²¡æœ‰æ˜ å°„
        return translated
    
    def generate_fallback_prompt(self, edit_mode, operation_type, description):
        """ç”ŸæˆåŸºç¡€fallbackæç¤ºè¯ - ä»…åœ¨å‰ç«¯æœªæä¾›æ—¶ä½¿ç”¨"""
        # åŸºç¡€æç¤ºè¯æ¨¡æ¿
        basic_templates = {
            'change_color': f'change color to {description or "specified color"}',
            'blur_background': f'blur background while keeping {description or "subject"} sharp',
            'enhance_quality': f'enhance quality of {description or "image"}',
        }
        
        # åŸºç¡€çº¦æŸå’Œä¿®é¥°è¯
        basic_constraints = ['natural blending', 'seamless integration']
        basic_decoratives = ['improved detail', 'enhanced quality']
        
        # æ„å»ºåŸºç¡€æç¤ºè¯
        if operation_type and operation_type in basic_templates:
            base_prompt = basic_templates[operation_type]
        else:
            base_prompt = f"{edit_mode}: {description or 'apply editing'}"
        
        return base_prompt, basic_constraints, basic_decoratives
    
    def generate_basic_fallback_prompts(self, edit_mode, operation_type, description, 
                                       constraint_prompts, decorative_prompts):
        """ç”ŸæˆåŸºç¡€fallbackæç¤ºè¯ - ä»…åœ¨å‰ç«¯å®Œå…¨å¤±æ•ˆæ—¶ä½¿ç”¨"""
        # ä½¿ç”¨ç²¾ç®€çš„fallbackç”Ÿæˆå™¨
        base_prompt, basic_constraints, basic_decoratives = self.generate_fallback_prompt(
            edit_mode, operation_type, description
        )
        
        # ç»„åˆæç¤ºè¯
        all_constraints = constraint_prompts + basic_constraints
        all_decoratives = decorative_prompts + basic_decoratives
        
        # æ„å»ºæ­£å‘æç¤ºè¯
        positive_parts = [base_prompt]
        if all_constraints:
            positive_parts.extend(all_constraints[:3])  # é™åˆ¶æ•°é‡
        if all_decoratives:
            positive_parts.extend(all_decoratives[:2])   # é™åˆ¶æ•°é‡
        
        positive_prompt = ", ".join(positive_parts)
        
        # åŸºç¡€è´Ÿå‘æç¤ºè¯
        negative_prompt = "artifacts, distortions, unnatural appearance, poor quality, inconsistencies, blurry, low quality, artifacts, distorted, unnatural, poor composition, bad anatomy, incorrect proportions"
        
        # æ„å»ºå®Œæ•´æè¿°
        full_description_parts = [
            f"ç¼–è¾‘æ¨¡å¼ï¼š{edit_mode}",
            f"æ“ä½œç±»å‹ï¼š{operation_type or 'æœªæŒ‡å®š'}",
            f"æè¿°ï¼š{description or 'æœªæä¾›'}",
        ]
        
        if all_constraints:
            constraint_display = self.translate_basic_prompts(all_constraints[:3])
            full_description_parts.append(f"çº¦æŸæ€§æç¤ºè¯ï¼š{', '.join(constraint_display)}")
        
        if all_decoratives:
            decorative_display = self.translate_basic_prompts(all_decoratives[:2])
            full_description_parts.append(f"ä¿®é¥°æ€§æç¤ºè¯ï¼š{', '.join(decorative_display)}")
        
        full_description = " | ".join(full_description_parts)
        
        return positive_prompt, negative_prompt, full_description
    
    def process_api_mode(self, layer_info, description, api_provider, api_key, api_model,
                        editing_intent, processing_style, seed, custom_guidance, image):
        """å¤„ç†APIæ¨¡å¼çš„æç¤ºè¯ç”Ÿæˆ"""
        try:
            from API_flux_kontext_enhancer import APIFluxKontextEnhancer
            
            # åˆ›å»ºAPIå¢å¼ºå™¨å®ä¾‹
            api_enhancer = APIFluxKontextEnhancer()
            
            # è½¬æ¢å›¾å±‚ä¿¡æ¯ä¸ºJSONå­—ç¬¦ä¸²
            layer_info_str = json.dumps(layer_info) if isinstance(layer_info, dict) else str(layer_info)
            
            # è°ƒç”¨APIå¢å¼ºå™¨
            enhanced_instructions, system_prompt = api_enhancer.enhance_flux_instructions(
                api_provider=api_provider,
                api_key=api_key,
                model_preset=api_model,
                custom_model="",
                layer_info=layer_info_str,
                edit_description=description,
                editing_intent=editing_intent,
                processing_style=processing_style,
                seed=seed,
                custom_guidance=custom_guidance,
                load_saved_guidance="none",
                save_guidance_name="",
                save_guidance_button=False,
                image=image
            )
            
            if enhanced_instructions:
                return enhanced_instructions
            else:
                print("[Kontext Super Prompt] APIæ¨¡å¼ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨fallback")
                return f"APIç”Ÿæˆå¤±è´¥: {description or 'æ— æè¿°'}"
                
        except Exception as e:
            print(f"[Kontext Super Prompt] APIæ¨¡å¼å¤„ç†é”™è¯¯: {e}")
            return f"APIå¤„ç†é”™è¯¯: {description or 'æ— æè¿°'}"
    
    def process_ollama_mode(self, layer_info, description, ollama_url, ollama_model, 
                           temperature, editing_intent, processing_style, seed,
                           custom_guidance, enable_visual, auto_unload, image):
        """å¤„ç†Ollamaæ¨¡å¼çš„æç¤ºè¯ç”Ÿæˆ"""
        try:
            from ollama_flux_kontext_enhancer import OllamaFluxKontextEnhancerV2
            
            # åˆ›å»ºOllamaå¢å¼ºå™¨å®ä¾‹
            ollama_enhancer = OllamaFluxKontextEnhancerV2()
            
            # è½¬æ¢å›¾å±‚ä¿¡æ¯ä¸ºJSONå­—ç¬¦ä¸²
            layer_info_str = json.dumps(layer_info) if isinstance(layer_info, dict) else str(layer_info)
            
            # è°ƒç”¨Ollamaå¢å¼ºå™¨
            enhanced_instructions, system_prompt = ollama_enhancer.enhance_flux_instructions(
                layer_info=layer_info_str,
                edit_description=description,
                model=ollama_model,
                auto_unload_model=auto_unload,
                editing_intent=editing_intent,
                processing_style=processing_style,
                image=image,
                url=ollama_url,
                temperature=temperature,
                enable_visual_analysis=enable_visual,
                seed=seed,
                load_saved_guidance="none",
                save_guidance=False,
                guidance_name="",
                custom_guidance=custom_guidance
            )
            
            if enhanced_instructions:
                return enhanced_instructions
            else:
                print("[Kontext Super Prompt] Ollamaæ¨¡å¼ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨fallback")
                return f"Ollamaç”Ÿæˆå¤±è´¥: {description or 'æ— æè¿°'}"
                
        except Exception as e:
            print(f"[Kontext Super Prompt] Ollamaæ¨¡å¼å¤„ç†é”™è¯¯: {e}")
            return f"Ollamaå¤„ç†é”™è¯¯: {description or 'æ— æè¿°'}"


# æ³¨å†ŒèŠ‚ç‚¹
NODE_CLASS_MAPPINGS = {
    "KontextSuperPrompt": KontextSuperPrompt,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "KontextSuperPrompt": "ğŸ¯ Kontext Super Prompt",
}

print("[Kontext Super Prompt] Kontext Super Prompt node registered")