"""
Ollama Service Manager Node
OllamaæœåŠ¡ç®¡ç†èŠ‚ç‚¹ - ä¸€é”®å¯åŠ¨/åœæ­¢OllamaæœåŠ¡

æä¾›å¯è§†åŒ–çš„OllamaæœåŠ¡æ§åˆ¶ç•Œé¢
"""

import subprocess
import time
import psutil
import requests
import os
import platform
from typing import Optional, Dict, Any

try:
    from server import PromptServer
    from aiohttp import web
    WEB_AVAILABLE = True
except ImportError:
    WEB_AVAILABLE = False

CATEGORY_TYPE = "ğŸ¨ LRPG Canvas"

class OllamaServiceManager:
    """
    ğŸ¦™ Ollama Service Manager
    
    ä¸€é”®å¯åŠ¨/åœæ­¢OllamaæœåŠ¡çš„ç®¡ç†èŠ‚ç‚¹
    """
    
    # ç±»çº§åˆ«çš„è¿›ç¨‹ç®¡ç†
    _ollama_process = None
    _service_status = "stopped"  # stopped, starting, running, stopping
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {},
            "hidden": {
                "unique_id": "UNIQUE_ID",
            },
        }
    
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("status",)
    FUNCTION = "manage_service"
    CATEGORY = CATEGORY_TYPE
    OUTPUT_NODE = True
    
    @classmethod
    def IS_CHANGED(cls, **kwargs):
        # å¼ºåˆ¶æ¯æ¬¡éƒ½é‡æ–°æ‰§è¡Œä»¥æ›´æ–°çŠ¶æ€
        return float(time.time())
    
    def manage_service(self, unique_id=""):
        """
        ç®¡ç†OllamaæœåŠ¡
        """
        try:
            # æ£€æµ‹å½“å‰æœåŠ¡çŠ¶æ€
            status = self.check_ollama_status()
            print(f"[Ollama Service Manager] å½“å‰çŠ¶æ€: {status}")
            
            return (f"OllamaæœåŠ¡çŠ¶æ€: {status}",)
            
        except Exception as e:
            print(f"[Ollama Service Manager] é”™è¯¯: {str(e)}")
            return (f"é”™è¯¯: {str(e)}",)
    
    @classmethod
    def check_ollama_status(cls) -> str:
        """æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€"""
        try:
            # æ–¹æ³•1: æ£€æŸ¥ç«¯å£11434æ˜¯å¦å¼€æ”¾
            response = requests.get("http://localhost:11434/api/tags", timeout=2)
            if response.status_code == 200:
                cls._service_status = "running"
                return "è¿è¡Œä¸­"
        except:
            pass
        
        # æ–¹æ³•2: æ£€æŸ¥è¿›ç¨‹
        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
            try:
                if proc.info['name'] and 'ollama' in proc.info['name'].lower():
                    if proc.info['cmdline'] and any('serve' in str(cmd) for cmd in proc.info['cmdline']):
                        cls._service_status = "running"
                        return "è¿è¡Œä¸­"
            except:
                continue
        
        cls._service_status = "stopped"
        return "å·²åœæ­¢"
    
    @classmethod
    def start_ollama_service(cls) -> Dict[str, Any]:
        """å¯åŠ¨OllamaæœåŠ¡"""
        try:
            # æ£€æŸ¥æ˜¯å¦å·²ç»è¿è¡Œ
            if cls.check_ollama_status() == "è¿è¡Œä¸­":
                return {"success": True, "message": "OllamaæœåŠ¡å·²åœ¨è¿è¡Œ"}
            
            cls._service_status = "starting"
            
            # ç¡®å®šæ“ä½œç³»ç»Ÿå’Œå‘½ä»¤
            system = platform.system().lower()
            if system == "windows":
                cmd = ["ollama.exe", "serve"]
                # Windowsä¸‹åˆ›å»ºæ–°çš„æ§åˆ¶å°çª—å£
                cls._ollama_process = subprocess.Popen(
                    cmd,
                    creationflags=subprocess.CREATE_NEW_CONSOLE,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE
                )
            else:
                cmd = ["ollama", "serve"]
                cls._ollama_process = subprocess.Popen(
                    cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE
                )
            
            # ç­‰å¾…æœåŠ¡å¯åŠ¨
            for i in range(10):  # æœ€å¤šç­‰å¾…10ç§’
                time.sleep(1)
                if cls.check_ollama_status() == "è¿è¡Œä¸­":
                    return {"success": True, "message": "OllamaæœåŠ¡å¯åŠ¨æˆåŠŸ"}
            
            return {"success": False, "message": "OllamaæœåŠ¡å¯åŠ¨è¶…æ—¶"}
            
        except FileNotFoundError:
            cls._service_status = "stopped"
            return {"success": False, "message": "Ollamaæœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…Ollama"}
        except Exception as e:
            cls._service_status = "stopped"
            return {"success": False, "message": f"å¯åŠ¨å¤±è´¥: {str(e)}"}
    
    @classmethod
    def stop_ollama_service(cls) -> Dict[str, Any]:
        """åœæ­¢OllamaæœåŠ¡"""
        try:
            cls._service_status = "stopping"
            
            # å¦‚æœæœ‰è®°å½•çš„è¿›ç¨‹ï¼Œå…ˆå°è¯•ç»ˆæ­¢
            if cls._ollama_process:
                try:
                    cls._ollama_process.terminate()
                    cls._ollama_process.wait(timeout=5)
                except:
                    try:
                        cls._ollama_process.kill()
                    except:
                        pass
                finally:
                    cls._ollama_process = None
            
            # æŸ¥æ‰¾å¹¶ç»ˆæ­¢æ‰€æœ‰Ollamaè¿›ç¨‹
            terminated_count = 0
            for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
                try:
                    if proc.info['name'] and 'ollama' in proc.info['name'].lower():
                        if proc.info['cmdline'] and any('serve' in str(cmd) for cmd in proc.info['cmdline']):
                            proc.terminate()
                            terminated_count += 1
                except:
                    continue
            
            # ç­‰å¾…è¿›ç¨‹å®Œå…¨é€€å‡º
            time.sleep(2)
            
            # éªŒè¯æ˜¯å¦çœŸçš„åœæ­¢äº†
            if cls.check_ollama_status() == "å·²åœæ­¢":
                return {"success": True, "message": f"OllamaæœåŠ¡å·²åœæ­¢ (ç»ˆæ­¢äº†{terminated_count}ä¸ªè¿›ç¨‹)"}
            else:
                return {"success": False, "message": "éƒ¨åˆ†è¿›ç¨‹å¯èƒ½ä»åœ¨è¿è¡Œ"}
                
        except Exception as e:
            cls._service_status = "stopped"
            return {"success": False, "message": f"åœæ­¢å¤±è´¥: {str(e)}"}
    
    @classmethod
    def unload_ollama_models(cls) -> Dict[str, Any]:
        """é‡Šæ”¾Ollamaæ¨¡å‹å†…å­˜"""
        try:
            # æ£€æŸ¥æœåŠ¡æ˜¯å¦è¿è¡Œ
            if cls.check_ollama_status() != "è¿è¡Œä¸­":
                return {"success": False, "message": "OllamaæœåŠ¡æœªè¿è¡Œ"}
            
            # æ–¹æ³•1: é€šè¿‡APIé‡Šæ”¾æ‰€æœ‰æ¨¡å‹
            try:
                response = requests.post(
                    "http://localhost:11434/api/generate",
                    json={"model": "", "keep_alive": 0},
                    timeout=10
                )
                if response.status_code == 200:
                    return {"success": True, "message": "æ‰€æœ‰æ¨¡å‹å†…å­˜å·²é‡Šæ”¾"}
            except Exception as api_error:
                print(f"[Ollama Manager] APIé‡Šæ”¾å¤±è´¥: {api_error}")
            
            # æ–¹æ³•2: é‡å¯æœåŠ¡æ¥é‡Šæ”¾å†…å­˜
            print("[Ollama Manager] å°è¯•é€šè¿‡é‡å¯æœåŠ¡é‡Šæ”¾å†…å­˜...")
            stop_result = cls.stop_ollama_service()
            if not stop_result["success"]:
                return {"success": False, "message": f"åœæ­¢æœåŠ¡å¤±è´¥: {stop_result['message']}"}
            
            time.sleep(2)  # ç­‰å¾…æœåŠ¡å®Œå…¨åœæ­¢
            
            start_result = cls.start_ollama_service()
            if start_result["success"]:
                return {"success": True, "message": "æœåŠ¡å·²é‡å¯ï¼Œæ¨¡å‹å†…å­˜å·²é‡Šæ”¾"}
            else:
                return {"success": False, "message": f"é‡å¯å¤±è´¥: {start_result['message']}"}
                
        except Exception as e:
            return {"success": False, "message": f"é‡Šæ”¾æ¨¡å‹å¤±è´¥: {str(e)}"}

# Web APIæ¥å£
if WEB_AVAILABLE:
    @PromptServer.instance.routes.post("/ollama_service_control")
    async def ollama_service_control(request):
        """OllamaæœåŠ¡æ§åˆ¶API"""
        try:
            data = await request.json()
            action = data.get('action', '')
            
            if action == "status":
                status = OllamaServiceManager.check_ollama_status()
                return web.json_response({
                    "success": True,
                    "status": status,
                    "message": f"å½“å‰çŠ¶æ€: {status}"
                })
            
            elif action == "start":
                result = OllamaServiceManager.start_ollama_service()
                return web.json_response(result)
            
            elif action == "stop":
                result = OllamaServiceManager.stop_ollama_service()
                return web.json_response(result)
            
            elif action == "unload":
                result = OllamaServiceManager.unload_ollama_models()
                return web.json_response(result)
            
            else:
                return web.json_response({
                    "success": False,
                    "message": "æ— æ•ˆçš„æ“ä½œ"
                }, status=400)
                
        except Exception as e:
            return web.json_response({
                "success": False,
                "message": f"APIé”™è¯¯: {str(e)}"
            }, status=500)

    @PromptServer.instance.routes.post("/ollama_flux_enhancer/get_models")
    async def get_ollama_models(request):
        """è·å–Ollamaæ¨¡å‹åˆ—è¡¨API"""
        try:
            data = await request.json()
            url = data.get('url', 'http://127.0.0.1:11434')
            
            # æ£€æŸ¥æœåŠ¡çŠ¶æ€
            if OllamaServiceManager.check_ollama_status() != "è¿è¡Œä¸­":
                return web.json_response([])
            
            # è·å–æ¨¡å‹åˆ—è¡¨
            try:
                # ä½¿ç”¨æä¾›çš„URLæˆ–é»˜è®¤URL
                api_url = f"{url}/api/tags"
                response = requests.get(api_url, timeout=5)
                
                if response.status_code == 200:
                    models_data = response.json()
                    # æå–æ¨¡å‹åç§°
                    model_names = []
                    if 'models' in models_data:
                        for model in models_data['models']:
                            if 'name' in model:
                                model_names.append(model['name'])
                    
                    print(f"[Ollama API] æˆåŠŸè·å–åˆ° {len(model_names)} ä¸ªæ¨¡å‹")
                    return web.json_response(model_names)
                else:
                    print(f"[Ollama API] è¯·æ±‚å¤±è´¥: {response.status_code}")
                    return web.json_response([])
                    
            except Exception as api_error:
                print(f"[Ollama API] è¿æ¥å¤±è´¥: {str(api_error)}")
                return web.json_response([])
                
        except Exception as e:
            print(f"[Ollama API] å¤„ç†è¯·æ±‚å¤±è´¥: {str(e)}")
            return web.json_response([], status=500)

# æ³¨å†ŒèŠ‚ç‚¹
NODE_CLASS_MAPPINGS = {
    "OllamaServiceManager": OllamaServiceManager,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "OllamaServiceManager": "ğŸ¦™ Ollama Service Manager",
}

print("[Ollama Service Manager] Ollama Service Manager node registered")