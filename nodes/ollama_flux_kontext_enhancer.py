"""
OllamaFluxKontextEnhancer Node
Ollamaé›†æˆçš„Flux Kontextæç¤ºè¯å¢å¼ºèŠ‚ç‚¹

å°†VisualPromptEditorçš„æ ‡æ³¨æ•°æ®é€šè¿‡æœ¬åœ°Ollamaæ¨¡å‹è½¬æ¢ä¸º
Flux Kontextä¼˜åŒ–çš„ç»“æ„åŒ–ç¼–è¾‘æŒ‡ä»¤
"""

import json
import time
import traceback
from typing import Dict, List, Any, Tuple, Optional
from datetime import datetime

try:
    from ollama import Client
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False
    print("Warning: ollama package not found. Please install with: pip install ollama")

try:
    from aiohttp import web
    from server import PromptServer
    WEB_AVAILABLE = True
except ImportError:
    WEB_AVAILABLE = False


class OllamaFluxKontextEnhancerV2:
    """
    ğŸ¤– Ollama Flux Kontext Enhancer
    
    é€šè¿‡æœ¬åœ°Ollamaæ¨¡å‹å°†VisualPromptEditorçš„æ ‡æ³¨æ•°æ®
    è½¬æ¢ä¸ºFlux Kontextä¼˜åŒ–çš„ç»“æ„åŒ–ç¼–è¾‘æŒ‡ä»¤
    """
    
    # ç±»çº§åˆ«çš„ç¼“å­˜å˜é‡
    _cached_models = None
    _cache_timestamp = 0
    _cache_duration = 30  # ç¼“å­˜30ç§’
    
    @classmethod
    def get_available_models(cls, url="http://127.0.0.1:11434", force_refresh=False):
        """åŠ¨æ€è·å–å¯ç”¨çš„Ollamaæ¨¡å‹åˆ—è¡¨ - é€šç”¨ç‰ˆæœ¬ï¼Œæ”¯æŒä»»ä½•å·²å®‰è£…çš„æ¨¡å‹"""
        
        import time
        current_time = time.time()
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        if (not force_refresh and 
            cls._cached_models is not None and 
            current_time - cls._cache_timestamp < cls._cache_duration):
            print(f"ğŸ“‹ ä½¿ç”¨ç¼“å­˜çš„æ¨¡å‹åˆ—è¡¨: {cls._cached_models}")
            return cls._cached_models
        
        def try_http_api(api_url):
            """å°è¯•é€šè¿‡HTTP APIè·å–æ¨¡å‹åˆ—è¡¨"""
            try:
                import requests
                response = requests.get(f"{api_url}/api/tags", timeout=10)
                if response.status_code == 200:
                    models_data = response.json()
                    models = models_data.get('models', [])
                    
                    model_names = []
                    for model in models:
                        if isinstance(model, dict):
                            # å°è¯•å¤šç§å¯èƒ½çš„å­—æ®µå
                            name = (model.get('name') or 
                                   model.get('model') or 
                                   model.get('id') or 
                                   model.get('model_id'))
                            if name:
                                model_names.append(name)
                                print(f"âœ… HTTP APIæ£€æµ‹åˆ°æ¨¡å‹: {name}")
                    
                    return model_names
            except Exception as e:
                print(f"HTTP APIæ£€æµ‹å¤±è´¥: {e}")
                return []
        
        def try_ollama_client(api_url):
            """å°è¯•é€šè¿‡Ollamaå®¢æˆ·ç«¯è·å–æ¨¡å‹åˆ—è¡¨"""
            try:
                if not OLLAMA_AVAILABLE:
                    return []
                
                from ollama import Client
                client = Client(host=api_url)
                models_response = client.list()
                
                if isinstance(models_response, dict):
                    models = models_response.get('models', [])
                elif hasattr(models_response, 'models'):
                    models = models_response.models
                else:
                    models = []
                
                model_names = []
                for model in models:
                    name = None
                    
                    if isinstance(model, dict):
                        name = (model.get('name') or 
                               model.get('model') or 
                               model.get('id'))
                    elif hasattr(model, 'name'):
                        name = model.name
                    elif hasattr(model, 'model'):
                        name = model.model
                    else:
                        # å°è¯•ä»å­—ç¬¦ä¸²è¡¨ç¤ºä¸­æå–æ¨¡å‹å
                        model_str = str(model)
                        if "name=" in model_str or "model=" in model_str:
                            # æ”¯æŒå¤šç§æ ¼å¼: name='xxx', model='xxx'
                            for prefix in ["name='", "model='"]:
                                if prefix in model_str:
                                    start = model_str.find(prefix) + len(prefix)
                                    end = model_str.find("'", start)
                                    if end > start:
                                        name = model_str[start:end]
                                        break
                    
                    if name:
                        model_names.append(name)
                        print(f"âœ… Ollama Clientæ£€æµ‹åˆ°æ¨¡å‹: {name}")
                
                return model_names
                
            except Exception as e:
                print(f"Ollama Clientæ£€æµ‹å¤±è´¥: {e}")
                return []
        
        # å¼€å§‹æ£€æµ‹æµç¨‹
        print(f"ğŸ” å¼€å§‹æ£€æµ‹Ollamaæ¨¡å‹ (URL: {url})")
        
        # å°è¯•å¤šç§URLæ ¼å¼
        urls_to_try = [
            url,
            "http://127.0.0.1:11434",
            "http://localhost:11434",
            "http://0.0.0.0:11434"
        ]
        
        all_models = set()  # ä½¿ç”¨é›†åˆé¿å…é‡å¤
        
        for test_url in urls_to_try:
            try:
                # æ–¹æ³•1: HTTP API
                http_models = try_http_api(test_url)
                if http_models:
                    all_models.update(http_models)
                    print(f"ğŸŒ ä» {test_url} é€šè¿‡HTTP APIè·å–åˆ° {len(http_models)} ä¸ªæ¨¡å‹")
                
                # æ–¹æ³•2: Ollama Client
                client_models = try_ollama_client(test_url)
                if client_models:
                    all_models.update(client_models)
                    print(f"ğŸ”— ä» {test_url} é€šè¿‡Ollama Clientè·å–åˆ° {len(client_models)} ä¸ªæ¨¡å‹")
                
                # å¦‚æœå·²ç»æ‰¾åˆ°æ¨¡å‹ï¼Œå¯ä»¥æå‰é€€å‡º
                if all_models:
                    break
                    
            except Exception as e:
                print(f"âš ï¸ æµ‹è¯•URL {test_url} å¤±è´¥: {e}")
                continue
        
        # è½¬æ¢ä¸ºæ’åºçš„åˆ—è¡¨
        model_list = sorted(list(all_models))
        
        if model_list:
            print(f"ğŸ¯ æ€»å…±æ£€æµ‹åˆ° {len(model_list)} ä¸ªå”¯ä¸€æ¨¡å‹:")
            for i, model in enumerate(model_list, 1):
                print(f"   {i}. {model}")
            
            # æ›´æ–°ç¼“å­˜
            cls._cached_models = model_list
            cls._cache_timestamp = current_time
            print(f"ğŸ’¾ æ¨¡å‹åˆ—è¡¨å·²ç¼“å­˜ï¼Œæœ‰æ•ˆæœŸ {cls._cache_duration} ç§’")
            
            return model_list
        
        # å¦‚æœå®Œå…¨æ²¡æœ‰æ£€æµ‹åˆ°æ¨¡å‹ï¼Œè¿”å›ä¸€ä¸ªé€šç”¨çš„å¤‡ç”¨æ¨¡å‹
        print("âš ï¸ æ— æ³•æ£€æµ‹åˆ°ä»»ä½•æ¨¡å‹ï¼Œè¿”å›é€šç”¨å¤‡ç”¨åˆ—è¡¨")
        fallback_models = ["ollama-model-not-found"]
        print("ğŸ’¡ è¯·ç¡®ä¿:")
        print("   1. OllamaæœåŠ¡æ­£åœ¨è¿è¡Œ (ollama serve)")
        print("   2. å·²å®‰è£…è‡³å°‘ä¸€ä¸ªæ¨¡å‹ (ollama pull <model_name>)")
        print("   3. æœåŠ¡å¯ä»¥è®¿é—® (curl http://localhost:11434/api/tags)")
        
        # å³ä½¿æ˜¯fallbackä¹Ÿè¦ç¼“å­˜ï¼Œé¿å…é‡å¤é”™è¯¯æ£€æµ‹
        cls._cached_models = fallback_models
        cls._cache_timestamp = current_time
        
        return fallback_models

    @classmethod
    def refresh_model_cache(cls):
        """æ‰‹åŠ¨åˆ·æ–°æ¨¡å‹ç¼“å­˜"""
        print("ğŸ”„ æ‰‹åŠ¨åˆ·æ–°æ¨¡å‹ç¼“å­˜...")
        cls._cached_models = None
        cls._cache_timestamp = 0
        return cls.get_available_models(force_refresh=True)

    @classmethod
    def get_template_content_for_placeholder(cls, guidance_style, guidance_template):
        """è·å–æ¨¡æ¿å†…å®¹ç”¨äºplaceholderæ˜¾ç¤º"""
        try:
            # å¯¼å…¥guidance_templatesæ¨¡å—
            from .guidance_templates import PRESET_GUIDANCE, TEMPLATE_LIBRARY
            
            # æ ¹æ®guidance_styleé€‰æ‹©å†…å®¹
            if guidance_style == "custom":
                # è‡ªå®šä¹‰æ¨¡å¼ä¿ç•™å®Œæ•´æç¤ºæ–‡å­—
                return """è¾“å…¥æ‚¨çš„è‡ªå®šä¹‰AIå¼•å¯¼æŒ‡ä»¤...

ä¾‹å¦‚ï¼š
ä½ æ˜¯ä¸“ä¸šçš„å›¾åƒç¼–è¾‘ä¸“å®¶ï¼Œè¯·å°†æ ‡æ³¨æ•°æ®è½¬æ¢ä¸ºç®€æ´æ˜äº†çš„ç¼–è¾‘æŒ‡ä»¤ã€‚é‡ç‚¹å…³æ³¨ï¼š
1. ä¿æŒæŒ‡ä»¤ç®€æ´
2. ç¡®ä¿æ“ä½œç²¾ç¡®
3. ç»´æŒé£æ ¼ä¸€è‡´æ€§

æ›´å¤šç¤ºä¾‹è¯·æŸ¥çœ‹guidance_templateé€‰é¡¹ã€‚"""
            elif guidance_style == "template":
                if guidance_template and guidance_template != "none" and guidance_template in TEMPLATE_LIBRARY:
                    template_content = TEMPLATE_LIBRARY[guidance_template]["prompt"]
                    # æˆªå–å‰200ä¸ªå­—ç¬¦ç”¨äºplaceholderæ˜¾ç¤º
                    preview = template_content[:200].replace('\n', ' ').strip()
                    return f"å½“å‰æ¨¡æ¿: {TEMPLATE_LIBRARY[guidance_template]['name']}\n\n{preview}..."
                else:
                    return "é€‰æ‹©ä¸€ä¸ªæ¨¡æ¿åå°†åœ¨æ­¤æ˜¾ç¤ºé¢„è§ˆ..."
            else:
                # æ˜¾ç¤ºé¢„è®¾é£æ ¼çš„å†…å®¹
                if guidance_style in PRESET_GUIDANCE:
                    preset_content = PRESET_GUIDANCE[guidance_style]["prompt"]
                    # æˆªå–å‰200ä¸ªå­—ç¬¦ç”¨äºplaceholderæ˜¾ç¤º
                    preview = preset_content[:200].replace('\n', ' ').strip()
                    return f"å½“å‰é£æ ¼: {PRESET_GUIDANCE[guidance_style]['name']}\n\n{preview}..."
                else:
                    return """è¾“å…¥æ‚¨çš„è‡ªå®šä¹‰AIå¼•å¯¼æŒ‡ä»¤...

ä¾‹å¦‚ï¼š
ä½ æ˜¯ä¸“ä¸šçš„å›¾åƒç¼–è¾‘ä¸“å®¶ï¼Œè¯·å°†æ ‡æ³¨æ•°æ®è½¬æ¢ä¸ºç®€æ´æ˜äº†çš„ç¼–è¾‘æŒ‡ä»¤ã€‚é‡ç‚¹å…³æ³¨ï¼š
1. ä¿æŒæŒ‡ä»¤ç®€æ´
2. ç¡®ä¿æ“ä½œç²¾ç¡®
3. ç»´æŒé£æ ¼ä¸€è‡´æ€§

æ›´å¤šç¤ºä¾‹è¯·æŸ¥çœ‹guidance_templateé€‰é¡¹ã€‚"""
        except Exception as e:
            print(f"è·å–æ¨¡æ¿å†…å®¹å¤±è´¥: {e}")
            return """è¾“å…¥æ‚¨çš„è‡ªå®šä¹‰AIå¼•å¯¼æŒ‡ä»¤...

ä¾‹å¦‚ï¼š
ä½ æ˜¯ä¸“ä¸šçš„å›¾åƒç¼–è¾‘ä¸“å®¶ï¼Œè¯·å°†æ ‡æ³¨æ•°æ®è½¬æ¢ä¸ºç®€æ´æ˜äº†çš„ç¼–è¾‘æŒ‡ä»¤ã€‚é‡ç‚¹å…³æ³¨ï¼š
1. ä¿æŒæŒ‡ä»¤ç®€æ´
2. ç¡®ä¿æ“ä½œç²¾ç¡®
3. ç»´æŒé£æ ¼ä¸€è‡´æ€§

æ›´å¤šç¤ºä¾‹è¯·æŸ¥çœ‹guidance_templateé€‰é¡¹ã€‚"""

    @classmethod
    def INPUT_TYPES(cls):
        # åŠ¨æ€è·å–æ¨¡å‹åˆ—è¡¨
        available_models = cls.get_available_models()
        default_model = available_models[0] if available_models else "ollama-model-not-found"
        
        # ç¡®ä¿default_modelåœ¨available_modelsä¸­
        if default_model not in available_models:
            default_model = available_models[0] if available_models else "ollama-model-not-found"
        
        # åŠ¨æ€ç”Ÿæˆplaceholderå†…å®¹
        default_placeholder = cls.get_template_content_for_placeholder("efficient_concise", "none")
        
        return {
            "required": {
                "annotation_data": ("STRING", {
                    "forceInput": True,
                    "tooltip": "æ¥è‡ªVisualPromptEditorçš„æ ‡æ³¨JSONæ•°æ®ï¼ˆè¿æ¥è¾“å…¥ï¼‰"
                }),
                "image": ("IMAGE", {
                    "tooltip": "æ¥è‡ªVisualPromptEditorçš„å¤„ç†åå›¾åƒï¼ˆç”¨äºè§†è§‰åˆ†æï¼‰"
                }),
                "edit_description": ("STRING", {
                    "multiline": True,
                    "default": "",
                    "placeholder": "æè¿°ä½ æƒ³åšçš„ç¼–è¾‘æ“ä½œ...\n\nä¾‹å¦‚ï¼š\n- åœ¨çº¢è‰²çŸ©å½¢åŒºåŸŸå¢åŠ ä¸€æ£µæ ‘\n- å°†è“è‰²æ ‡è®°åŒºåŸŸçš„è½¦è¾†æ”¹ä¸ºçº¢è‰²\n- ç§»é™¤åœ†å½¢åŒºåŸŸçš„äººç‰©\n- å°†é»„è‰²åŒºåŸŸçš„å¤©ç©ºæ”¹ä¸ºæ™©éœæ•ˆæœ",
                    "tooltip": "æè¿°ä½ æƒ³è¦åšçš„ç¼–è¾‘æ“ä½œï¼Œç»“åˆæ ‡æ³¨ä¿¡æ¯ç”Ÿæˆç²¾å‡†çš„ç¼–è¾‘æŒ‡ä»¤"
                }),
                "model": (available_models, {
                    "default": default_model,
                    "tooltip": "é€‰æ‹©Ollamaæ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆ (è‡ªåŠ¨ä»OllamaæœåŠ¡è·å–)"
                }),  # åŠ¨æ€å¡«å……æ¨¡å‹åˆ—è¡¨
                "edit_instruction_type": ([
                    "auto_detect",          # ğŸ”„ è‡ªåŠ¨æ ¹æ®æ“ä½œç±»å‹é€‰æ‹©æœ€ä½³ç­–ç•¥
                    "spatial_precise",      # ç©ºé—´ç²¾å‡†ç¼–è¾‘
                    "semantic_enhanced",    # è¯­ä¹‰å¢å¼ºç¼–è¾‘  
                    "style_coherent",       # é£æ ¼ä¸€è‡´æ€§ç¼–è¾‘
                    "content_aware",        # å†…å®¹æ„ŸçŸ¥ç¼–è¾‘
                    "multi_region",         # å¤šåŒºåŸŸåè°ƒç¼–è¾‘
                    "custom"                # è‡ªå®šä¹‰æŒ‡ä»¤
                ], {
                    "default": "auto_detect",
                    "tooltip": "é€‰æ‹©ç¼–è¾‘æŒ‡ä»¤çš„ç”Ÿæˆç­–ç•¥ (auto_detectæ ¹æ®æ“ä½œç±»å‹è‡ªåŠ¨é€‰æ‹©)"
                }),
                "output_format": ([
                    "flux_kontext_standard",  # Flux Kontextæ ‡å‡†æ ¼å¼
                    "structured_json",        # ç»“æ„åŒ–JSON
                    "natural_language"        # è‡ªç„¶è¯­è¨€æè¿°
                ], {
                    "default": "flux_kontext_standard",
                    "tooltip": "é€‰æ‹©è¾“å‡ºçš„æç¤ºè¯æ ¼å¼"
                }),
            },
            "optional": {
                "url": ("STRING", {
                    "default": "http://127.0.0.1:11434",
                    "tooltip": "OllamaæœåŠ¡åœ°å€"
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.1,
                    "max": 1.0,
                    "step": 0.1,
                    "tooltip": "ç”Ÿæˆæ¸©åº¦ (åˆ›æ„æ€§æ§åˆ¶)"
                }),
                "language": (["chinese", "english", "bilingual"], {
                    "default": "chinese",
                    "tooltip": "é€‰æ‹©è¾“å‡ºè¯­è¨€ï¼šä¸­æ–‡ã€è‹±æ–‡æˆ–åŒè¯­"
                }),
                "enable_visual_analysis": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "å¯ç”¨è§†è§‰åˆ†æï¼ˆä»…å¯¹æ”¯æŒè§†è§‰çš„å¤šæ¨¡æ€æ¨¡å‹æœ‰æ•ˆï¼Œå¦‚qwen-vlã€llavaç­‰ï¼‰"
                }),
                "guidance_style": ([
                    "efficient_concise",   # é«˜æ•ˆç®€æ´ (é»˜è®¤)
                    "natural_creative",    # è‡ªç„¶åˆ›æ„
                    "technical_precise",   # æŠ€æœ¯ç²¾ç¡®
                    "template",           # æ¨¡æ¿é€‰æ‹©
                    "custom"              # è‡ªå®šä¹‰
                ], {
                    "default": "efficient_concise",
                    "tooltip": "é€‰æ‹©AIå¼•å¯¼è¯æœ¯é£æ ¼ï¼šé«˜æ•ˆç®€æ´é€‚åˆå¿«é€Ÿç¼–è¾‘ï¼Œè‡ªç„¶åˆ›æ„é€‚åˆè‰ºæœ¯è®¾è®¡ï¼ŒæŠ€æœ¯ç²¾ç¡®é€‚åˆä¸“ä¸šç”¨é€”ï¼Œæ¨¡æ¿é€‰æ‹©å¸¸ç”¨é¢„è®¾ï¼Œè‡ªå®šä¹‰å…è®¸å®Œå…¨æ§åˆ¶"
                }),
                "guidance_template": ([
                    "none",               # æ— æ¨¡æ¿
                    "ecommerce_product",  # ç”µå•†äº§å“
                    "portrait_beauty",    # äººåƒç¾åŒ–
                    "creative_design",    # åˆ›æ„è®¾è®¡
                    "architecture_photo", # å»ºç­‘æ‘„å½±
                    "food_photography",   # ç¾é£Ÿæ‘„å½±
                    "fashion_retail",     # æ—¶å°šé›¶å”®
                    "landscape_nature"    # é£æ™¯è‡ªç„¶
                ], {
                    "default": "none",
                    "tooltip": "é€‰æ‹©ä¸“ç”¨å¼•å¯¼æ¨¡æ¿ï¼ˆå½“guidance_styleä¸ºtemplateæ—¶ä½¿ç”¨ï¼‰"
                }),
                "custom_guidance": ("STRING", {
                    "default": "",
                    "multiline": True,
                    "placeholder": default_placeholder,
                    "tooltip": "å½“guidance_styleä¸º'custom'æ—¶ï¼Œåœ¨æ­¤è¾“å…¥æ‚¨çš„ä¸“ç”¨AIå¼•å¯¼æŒ‡ä»¤ã€‚placeholderä¼šæ ¹æ®å½“å‰é€‰æ‹©çš„guidance_styleå’Œguidance_templateåŠ¨æ€æ˜¾ç¤ºé¢„è§ˆå†…å®¹ã€‚"
                })
            }
        }
    
    @classmethod
    def VALIDATE_INPUTS(cls, **kwargs):
        """éªŒè¯è¾“å…¥å‚æ•°"""
        model = kwargs.get('model', '')
        
        # å¦‚æœmodelä¸ºç©ºï¼Œå°è¯•è·å–å¯ç”¨æ¨¡å‹å¹¶ä½¿ç”¨ç¬¬ä¸€ä¸ª
        if not model or model == '':
            available_models = cls.get_available_models()
            if available_models:
                # è¿”å›Trueè¡¨ç¤ºéªŒè¯é€šè¿‡ï¼ŒComfyUIä¼šä½¿ç”¨é»˜è®¤å€¼
                return True
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨å¯ç”¨åˆ—è¡¨ä¸­
        available_models = cls.get_available_models()
        if model not in available_models:
            return f"Model '{model}' not found. Available models: {available_models}"
        
        return True
    
    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = (
        "flux_edit_instructions",  # Flux Kontextæ ¼å¼çš„ç¼–è¾‘æŒ‡ä»¤
        "system_prompt",           # å‘é€ç»™æ¨¡å‹çš„å®Œæ•´ç³»ç»ŸæŒ‡ä»¤
    )
    
    FUNCTION = "enhance_flux_instructions"
    CATEGORY = "kontext/ai_enhanced"
    DESCRIPTION = "ğŸ¤– é€šè¿‡Ollamaå¢å¼ºVisualPromptEditorçš„æ ‡æ³¨æ•°æ®ï¼Œç”ŸæˆFlux Kontextä¼˜åŒ–çš„ç»“æ„åŒ–ç¼–è¾‘æŒ‡ä»¤"
    
    def __init__(self):
        self.start_time = None
        self.debug_logs = []
    
    def enhance_flux_instructions(self, annotation_data: str, image, edit_description: str, model: str, 
                                edit_instruction_type: str, output_format: str,
                                url: str = "http://127.0.0.1:11434", temperature: float = 0.7,
                                language: str = "chinese", enable_visual_analysis: bool = False,
                                guidance_style: str = "efficient_concise",
                                guidance_template: str = "none", custom_guidance: str = ""):
        """é€šè¿‡Ollamaå¢å¼ºæ ‡æ³¨æ•°æ®ï¼Œç”ŸæˆFlux Kontextä¼˜åŒ–çš„ç¼–è¾‘æŒ‡ä»¤"""
        
        # è®¾ç½®ç§»é™¤å‚æ•°çš„é»˜è®¤å€¼
        reference_context = ""
        edit_intensity = 0.8
        preservation_mask = ""
        style_guidance = ""
        top_p = 0.9
        keep_alive = 5
        debug_mode = False  # ç§»é™¤debug_modeå‚æ•°ï¼Œå›ºå®šä¸ºFalse
        
        print(f"ğŸš€ OllamaFluxKontextEnhancerV2: å¼€å§‹æ‰§è¡Œenhance_flux_instructions")
        print(f"ğŸ“ annotation_dataé•¿åº¦: {len(annotation_data) if annotation_data else 0}")
        
        # å¯¼å…¥å¼•å¯¼è¯æœ¯ç®¡ç†å™¨
        try:
            from .guidance_templates import guidance_manager
        except ImportError:
            # å›é€€åˆ°ç»å¯¹å¯¼å…¥
            import sys
            import os
            sys.path.append(os.path.dirname(__file__))
            from guidance_templates import guidance_manager
        
        # æ„å»ºç³»ç»Ÿæç¤ºè¯ï¼ˆæ•´åˆå¼•å¯¼è¯æœ¯å’Œè¯­è¨€æ§åˆ¶ï¼‰
        enhanced_system_prompt = guidance_manager.build_system_prompt(
            guidance_style=guidance_style,
            guidance_template=guidance_template,
            custom_guidance=custom_guidance,
            load_saved_guidance="",
            language=language
        )
        print(f"ğŸ”§ ä½¿ç”¨å¼•å¯¼æ¨¡å¼: {guidance_style}")
        print(f"ğŸŒ è¾“å‡ºè¯­è¨€: {language}")
        print(f"ğŸ” è§†è§‰åˆ†æ: {'å¯ç”¨' if enable_visual_analysis else 'ç¦ç”¨'}")
        if guidance_style == "template" and guidance_template != "none":
            print(f"ğŸ“š ä½¿ç”¨æ¨¡æ¿: {guidance_template}")
        elif guidance_style == "custom" and custom_guidance.strip():
            print(f"ğŸ¯ ä½¿ç”¨è‡ªå®šä¹‰å¼•å¯¼: {len(custom_guidance)} å­—ç¬¦")
        
        print(f"ğŸ“ ç”Ÿæˆçš„ç³»ç»Ÿæç¤ºè¯é•¿åº¦: {len(enhanced_system_prompt)} å­—ç¬¦")
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {model}")
        print(f"ğŸ¯ ç¼–è¾‘ç­–ç•¥: {edit_instruction_type}")
        print(f"ğŸ“„ è¾“å‡ºæ ¼å¼: {output_format}")
        
        self.start_time = time.time()
        self.debug_logs = []
        
        try:
            # æ£€æŸ¥OllamaæœåŠ¡å¯ç”¨æ€§ï¼ˆé€šè¿‡HTTP APIï¼‰
            if not self._check_ollama_service(url):
                return self._create_fallback_output(
                    f"Ollama service not available at {url}. Please start ollama service.",
                    debug_mode
                )
            
            # è‡ªåŠ¨æ£€æµ‹æœ€ä½³ç¼–è¾‘ç­–ç•¥
            if edit_instruction_type == "auto_detect":
                edit_instruction_type = self._auto_detect_strategy(annotation_data, debug_mode)
                self._log_debug(f"ğŸ”„ è‡ªåŠ¨æ£€æµ‹åˆ°æœ€ä½³ç­–ç•¥: {edit_instruction_type}", debug_mode)
            
            self._log_debug(f"ğŸš€ å¼€å§‹å¤„ç† - æ¨¡å‹: {model}, ç­–ç•¥: {edit_instruction_type}", debug_mode)
            
            # 1. è§£ææ ‡æ³¨æ•°æ®
            annotations, parsed_data = self._parse_annotation_data(annotation_data, debug_mode)
            if not annotations:
                return self._create_fallback_output(
                    "No valid annotations found in annotation_data",
                    debug_mode
                )
            
            # 2. OllamaæœåŠ¡å·²é€šè¿‡å‰é¢çš„æ£€æŸ¥ç¡®è®¤å¯ç”¨
            self._log_debug(f"ğŸ”— ä½¿ç”¨OllamaæœåŠ¡: {url}", debug_mode)
            
            # 3. æ„å»ºç”¨æˆ·æç¤ºè¯ï¼ˆç³»ç»Ÿæç¤ºè¯å·²åœ¨ä¸Šé¢é€šè¿‡å¼•å¯¼è¯æœ¯ç³»ç»Ÿæ„å»ºï¼‰
            user_prompt = self._build_user_prompt(
                annotations, parsed_data, edit_description, reference_context, 
                edit_intensity, preservation_mask, style_guidance
            )
            
            self._log_debug(f"ğŸ“ ç”Ÿæˆçš„ç”¨æˆ·æç¤ºè¯é•¿åº¦: {len(user_prompt)} å­—ç¬¦", debug_mode)
            
            # 4. æ£€æŸ¥æ˜¯å¦éœ€è¦è§†è§‰åˆ†æ
            image_base64 = None
            if enable_visual_analysis:
                if self._is_multimodal_model(model):
                    image_base64 = self._encode_image_for_ollama(image, debug_mode)
                    if image_base64:
                        self._log_debug("ğŸ” å¯ç”¨è§†è§‰åˆ†ææ¨¡å¼", debug_mode)
                    else:
                        self._log_debug("âš ï¸ å›¾åƒç¼–ç å¤±è´¥ï¼Œå›é€€åˆ°çº¯æ–‡æœ¬æ¨¡å¼", debug_mode)
                else:
                    self._log_debug(f"âš ï¸ æ¨¡å‹ {model} ä¸æ”¯æŒè§†è§‰åˆ†æï¼Œå¿½ç•¥è§†è§‰è¾“å…¥", debug_mode)
            
            # 5. è°ƒç”¨Ollamaç”Ÿæˆå¢å¼ºæŒ‡ä»¤ï¼ˆä½¿ç”¨å¼•å¯¼è¯æœ¯ç³»ç»Ÿæ„å»ºçš„enhanced_system_promptï¼‰
            enhanced_instructions = self._generate_with_ollama(
                url, model, enhanced_system_prompt, user_prompt,
                temperature, top_p, keep_alive, debug_mode, image_base64
            )
            
            if not enhanced_instructions:
                return self._create_fallback_output(
                    "Failed to generate enhanced instructions from Ollama",
                    debug_mode
                )
            
            # 5. æ ¼å¼åŒ–è¾“å‡º
            flux_instructions = self._format_flux_instructions(
                enhanced_instructions, output_format, debug_mode
            )
            
            self._log_debug("âœ… å¤„ç†å®Œæˆ", debug_mode)
            
            return (flux_instructions, enhanced_system_prompt)
            
        except Exception as e:
            error_msg = f"Error in enhance_flux_instructions: {str(e)}"
            if debug_mode:
                error_msg += f"\n{traceback.format_exc()}"
            return self._create_fallback_output(error_msg, debug_mode)
    
    def _check_ollama_service(self, url: str) -> bool:
        """æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦å¯ç”¨"""
        try:
            import requests
            response = requests.get(f"{url}/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def _is_multimodal_model(self, model: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒè§†è§‰åˆ†æ"""
        multimodal_models = [
            "qwen-vl", "qwen2-vl", "qwen:vl", "qwen2:vl",
            "llava", "llava:latest", "llava:7b", "llava:13b", "llava:34b",
            "llava-llama3", "llava-phi3", "llava-code",
            "moondream", "cogvlm", "cogvlm2",
            "yi-vl", "internvl", "minicpm-v"
        ]
        
        model_lower = model.lower()
        for mm_model in multimodal_models:
            if mm_model in model_lower:
                return True
        return False
    
    def _encode_image_for_ollama(self, image, debug_mode: bool) -> Optional[str]:
        """å°†å›¾åƒç¼–ç ä¸ºOllamaå¯ç”¨çš„base64æ ¼å¼"""
        try:
            import torch
            import numpy as np
            from PIL import Image
            import io
            import base64
            
            # å¤„ç†ComfyUIçš„å›¾åƒæ ¼å¼ (tensor)
            if isinstance(image, torch.Tensor):
                # ComfyUIå›¾åƒæ ¼å¼: [batch, height, width, channels]
                if image.dim() == 4:
                    image = image[0]  # å–ç¬¬ä¸€å¼ å›¾åƒ
                
                # è½¬æ¢ä¸ºnumpy
                if image.dtype == torch.float32:
                    # ä»[0,1]èŒƒå›´è½¬æ¢åˆ°[0,255]
                    image_np = (image * 255).clamp(0, 255).byte().cpu().numpy()
                else:
                    image_np = image.cpu().numpy()
                
                # åˆ›å»ºPILå›¾åƒ
                pil_image = Image.fromarray(image_np, mode='RGB')
            else:
                # å¦‚æœå·²ç»æ˜¯PILå›¾åƒ
                pil_image = image
            
            # è½¬æ¢ä¸ºJPEGæ ¼å¼çš„base64
            buffer = io.BytesIO()
            pil_image.save(buffer, format='JPEG', quality=85)
            img_bytes = buffer.getvalue()
            img_base64 = base64.b64encode(img_bytes).decode('utf-8')
            
            self._log_debug(f"ğŸ–¼ï¸ å›¾åƒç¼–ç æˆåŠŸï¼Œbase64é•¿åº¦: {len(img_base64)} å­—ç¬¦", debug_mode)
            return img_base64
            
        except Exception as e:
            self._log_debug(f"âŒ å›¾åƒç¼–ç å¤±è´¥: {e}", debug_mode)
            return None
    
    def _auto_detect_strategy(self, annotation_data: str, debug_mode: bool) -> str:
        """æ ¹æ®annotationæ•°æ®è‡ªåŠ¨æ£€æµ‹æœ€ä½³ç¼–è¾‘ç­–ç•¥"""
        try:
            if not annotation_data or not annotation_data.strip():
                return "spatial_precise"  # é»˜è®¤ç­–ç•¥
            
            parsed_data = json.loads(annotation_data)
            operation_type = parsed_data.get('operation_type', '')
            
            # Visual Prompt Editoræ“ä½œç±»å‹åˆ°ç¼–è¾‘ç­–ç•¥çš„æ˜ å°„
            operation_to_strategy = {
                # ç©ºé—´ç²¾å‡†ç±»æ“ä½œ
                "change_color": "spatial_precise",
                "replace_object": "spatial_precise", 
                "resize_object": "spatial_precise",
                "geometric_warp": "spatial_precise",
                "perspective_transform": "spatial_precise",
                "precision_cutout": "spatial_precise",
                
                # è¯­ä¹‰å¢å¼ºç±»æ“ä½œ
                "add_object": "semantic_enhanced",
                "remove_object": "semantic_enhanced",
                "change_expression": "semantic_enhanced",
                "character_expression": "semantic_enhanced",
                "content_aware_fill": "semantic_enhanced",
                "seamless_removal": "semantic_enhanced",
                
                # é£æ ¼ä¸€è‡´æ€§ç±»æ“ä½œ
                "change_style": "style_coherent",
                "change_texture": "style_coherent",
                "global_style_transfer": "style_coherent",
                "style_blending": "style_coherent",
                "global_color_grade": "style_coherent",
                
                # å†…å®¹æ„ŸçŸ¥ç±»æ“ä½œ
                "enhance_quality": "content_aware",
                "change_pose": "content_aware",
                "change_clothing": "content_aware",
                "enhance_skin_texture": "content_aware",
                "detail_enhance": "content_aware",
                "realism_enhance": "content_aware",
                
                # å¤šåŒºåŸŸåè°ƒç±»æ“ä½œï¼ˆæ£€æµ‹å¤šä¸ªæ ‡æ³¨ï¼‰
                "global_enhance": "multi_region",
                "global_filter": "multi_region",
                "collage_integration": "multi_region",
                "texture_mixing": "multi_region"
            }
            
            # æ ¹æ®æ“ä½œç±»å‹é€‰æ‹©ç­–ç•¥
            detected_strategy = operation_to_strategy.get(operation_type, "spatial_precise")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¤šä¸ªæ ‡æ³¨åŒºåŸŸ
            annotations = parsed_data.get('annotations', [])
            if len(annotations) > 2:
                detected_strategy = "multi_region"
            
            self._log_debug(f"ğŸ¯ æ“ä½œç±»å‹: {operation_type} â†’ ç­–ç•¥: {detected_strategy}", debug_mode)
            return detected_strategy
            
        except Exception as e:
            self._log_debug(f"âš ï¸ ç­–ç•¥è‡ªåŠ¨æ£€æµ‹å¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤ç­–ç•¥", debug_mode)
            return "spatial_precise"
    
    def _parse_annotation_data(self, annotation_data: str, debug_mode: bool) -> Tuple[List[Dict], Dict]:
        """è§£ææ ‡æ³¨æ•°æ®"""
        try:
            if not annotation_data or not annotation_data.strip():
                self._log_debug("âš ï¸ æ ‡æ³¨æ•°æ®ä¸ºç©º", debug_mode)
                return [], {}
            
            parsed_data = json.loads(annotation_data)
            self._log_debug(f"ğŸ“Š è§£ææ ‡æ³¨æ•°æ®æˆåŠŸï¼Œæ•°æ®ç±»å‹: {type(parsed_data)}", debug_mode)
            
            # æå–annotations
            annotations = []
            if isinstance(parsed_data, dict):
                if "annotations" in parsed_data:
                    annotations = parsed_data["annotations"]
                elif "layers_data" in parsed_data:
                    annotations = parsed_data["layers_data"]
            elif isinstance(parsed_data, list):
                annotations = parsed_data
            
            self._log_debug(f"ğŸ“ æå–åˆ° {len(annotations)} ä¸ªæ ‡æ³¨", debug_mode)
            return annotations, parsed_data
            
        except json.JSONDecodeError as e:
            self._log_debug(f"âŒ JSONè§£æå¤±è´¥: {e}", debug_mode)
            return [], {}
        except Exception as e:
            self._log_debug(f"âŒ æ ‡æ³¨æ•°æ®è§£æå¼‚å¸¸: {e}", debug_mode)
            return [], {}
    
    def _connect_ollama(self, url: str, debug_mode: bool) -> Optional[object]:
        """è¿æ¥OllamaæœåŠ¡"""
        try:
            if not OLLAMA_AVAILABLE:
                self._log_debug("âŒ Ollamaæ¨¡å—ä¸å¯ç”¨", debug_mode)
                return None
                
            from ollama import Client
            client = Client(host=url)
            # æµ‹è¯•è¿æ¥
            models = client.list()
            self._log_debug(f"ğŸ”— Ollamaè¿æ¥æˆåŠŸï¼Œå¯ç”¨æ¨¡å‹æ•°: {len(models.get('models', []))}", debug_mode)
            return client
        except Exception as e:
            self._log_debug(f"âŒ Ollamaè¿æ¥å¤±è´¥: {e}", debug_mode)
            return None
    
    def _build_system_prompt(self, edit_instruction_type: str, output_format: str) -> str:
        """æ„å»ºç³»ç»Ÿæç¤ºè¯"""
        
        # ç¼–è¾‘ç­–ç•¥è¯´æ˜åŠå…·ä½“æŒ‡å¯¼
        strategy_descriptions = {
            "spatial_precise": {
                "description": "ä¸“æ³¨äºç²¾ç¡®çš„ç©ºé—´ä½ç½®æè¿°å’Œåæ ‡å®šä½",
                "guidance": """Focus on:
- Precise spatial positioning and coordinates
- Exact boundary definitions
- Clear geometric transformations
- Maintain spatial relationships
- Include specific pixel/region coordinates when possible"""
            },
            "semantic_enhanced": {
                "description": "å¼ºè°ƒè¯­ä¹‰ç†è§£å’Œå†…å®¹è¯†åˆ«ï¼Œç”Ÿæˆè¯­ä¹‰ä¸°å¯Œçš„ç¼–è¾‘æŒ‡ä»¤",
                "guidance": """Focus on:
- Object recognition and semantic understanding
- Context-aware content modifications
- Intelligent object relationships
- Meaningful content additions/removals
- Preserve semantic coherence"""
            },
            "style_coherent": {
                "description": "æ³¨é‡æ•´ä½“é£æ ¼çš„åè°ƒç»Ÿä¸€ï¼Œç¡®ä¿ç¼–è¾‘åçš„è§†è§‰ä¸€è‡´æ€§",
                "guidance": """Focus on:
- Visual style consistency
- Color harmony and palette coherence
- Texture and material uniformity
- Lighting and shadow consistency
- Overall aesthetic unity"""
            },
            "content_aware": {
                "description": "æ™ºèƒ½ç†è§£å›¾åƒå†…å®¹å’Œä¸Šä¸‹æ–‡ï¼Œç”Ÿæˆå†…å®¹æ„ŸçŸ¥çš„ç¼–è¾‘æŒ‡ä»¤",
                "guidance": """Focus on:
- Context-sensitive modifications
- Content-appropriate enhancements
- Scene understanding
- Natural content integration
- Preserve content authenticity"""
            },
            "multi_region": {
                "description": "å¤„ç†å¤šä¸ªæ ‡æ³¨åŒºåŸŸçš„åè°ƒå…³ç³»ï¼Œç¡®ä¿æ•´ä½“ç¼–è¾‘çš„å’Œè°æ€§",
                "guidance": """Focus on:
- Coordinate multiple region edits
- Ensure region-to-region harmony
- Balance competing modifications
- Maintain overall composition
- Synchronize related changes"""
            },
            "custom": {
                "description": "æ ¹æ®ç”¨æˆ·éœ€æ±‚ç”Ÿæˆè‡ªå®šä¹‰çš„ç¼–è¾‘æŒ‡ä»¤",
                "guidance": """Focus on:
- User-specific requirements
- Flexible adaptation to needs
- Custom modification approaches
- Personalized editing strategies"""
            }
        }
        
        # è¾“å‡ºæ ¼å¼è¯´æ˜
        format_descriptions = {
            "flux_kontext_standard": """Output EXACTLY in this format:

[EDIT_OPERATIONS]
operation_1: change the marked area to [target color/object]

[SPATIAL_CONSTRAINTS]
preserve_regions: ["background"]
blend_boundaries: "seamless"

[QUALITY_CONTROLS]
detail_level: "high"
consistency: "maintain_original_quality"
""",
            "structured_json": "è¾“å‡ºç»“æ„åŒ–çš„JSONæ ¼å¼ï¼ŒåŒ…å«æ“ä½œã€çº¦æŸå’Œè´¨é‡æ§åˆ¶ä¿¡æ¯",
            "natural_language": "è¾“å‡ºè‡ªç„¶æµç•…çš„è¯­è¨€æè¿°ï¼Œé€‚åˆç›´æ¥ä½œä¸ºæç¤ºè¯ä½¿ç”¨"
        }
        
        # è·å–å½“å‰ç­–ç•¥çš„æŒ‡å¯¼
        current_strategy = strategy_descriptions.get(edit_instruction_type, strategy_descriptions["spatial_precise"])
        
        system_prompt = f"""Generate image editing instructions in Flux Kontext format.

EDITING STRATEGY: {edit_instruction_type}
{current_strategy["guidance"]}

OUTPUT FORMAT:
{format_descriptions.get(output_format, "Standard format")}

Rules:
- Output ONLY the formatted instructions
- Do NOT include code, explanations, or comments
- Apply the specific editing strategy focus areas listed above
- Ensure instructions match the strategy requirements"""
        
        return system_prompt
    
    def _build_user_prompt(self, annotations: List[Dict], parsed_data: Dict,
                          edit_description: str = "", reference_context: str = "", edit_intensity: float = 0.8,
                          preservation_mask: str = "", style_guidance: str = "") -> str:
        """æ„å»ºç”¨æˆ·æç¤ºè¯"""
        
        prompt_parts = []
        
        # 1. ç¼–è¾‘æ„å›¾æè¿°ï¼ˆæœ€é‡è¦çš„ä¿¡æ¯ï¼‰
        if edit_description and edit_description.strip():
            prompt_parts.append("=== ç¼–è¾‘æ„å›¾ ===\nç”¨æˆ·è¦æ±‚: " + edit_description.strip())
        
        # 2. å›¾åƒæ ‡æ³¨ä¿¡æ¯
        prompt_parts.append("\n=== å›¾åƒæ ‡æ³¨ä¿¡æ¯ ===")
        for i, annotation in enumerate(annotations):
            annotation_desc = f"æ ‡æ³¨ {i+1}:"
            annotation_desc += f" ç±»å‹={annotation.get('type', 'unknown')}"
            annotation_desc += f" é¢œè‰²={annotation.get('color', '#000000')}"
            
            if 'start' in annotation and 'end' in annotation:
                start = annotation['start']
                end = annotation['end']
                annotation_desc += f" åæ ‡=({start.get('x', 0)},{start.get('y', 0)})-({end.get('x', 0)},{end.get('y', 0)})"
            
            prompt_parts.append(annotation_desc)
        
        # 3. æ“ä½œä¿¡æ¯
        if 'operation_type' in parsed_data:
            prompt_parts.append(f"\n=== æ“ä½œç±»å‹ ===")
            prompt_parts.append(f"æ“ä½œ: {parsed_data['operation_type']}")
        
        if 'target_description' in parsed_data:
            prompt_parts.append(f"ç›®æ ‡æè¿°: {parsed_data['target_description']}")
        
        # 4. å¢å¼ºæç¤ºè¯
        if 'constraint_prompts' in parsed_data and parsed_data['constraint_prompts']:
            prompt_parts.append(f"\n=== çº¦æŸæ€§æç¤ºè¯ ===")
            constraints = parsed_data['constraint_prompts']
            if isinstance(constraints, list):
                prompt_parts.append(", ".join(constraints))
            else:
                prompt_parts.append(str(constraints))
        
        if 'decorative_prompts' in parsed_data and parsed_data['decorative_prompts']:
            prompt_parts.append(f"\n=== ä¿®é¥°æ€§æç¤ºè¯ ===")
            decoratives = parsed_data['decorative_prompts']
            if isinstance(decoratives, list):
                prompt_parts.append(", ".join(decoratives))
            else:
                prompt_parts.append(str(decoratives))
        
        # 5. å‚è€ƒä¸Šä¸‹æ–‡
        if reference_context:
            prompt_parts.append(f"\n=== å‚è€ƒä¸Šä¸‹æ–‡ ===")
            prompt_parts.append(reference_context)
        
        # 6. ç¼–è¾‘å‚æ•°
        prompt_parts.append(f"\n=== ç¼–è¾‘å‚æ•° ===")
        prompt_parts.append(f"ç¼–è¾‘å¼ºåº¦: {edit_intensity}")
        
        if preservation_mask:
            prompt_parts.append(f"ä¿æŠ¤åŒºåŸŸ: {preservation_mask}")
        
        if style_guidance:
            prompt_parts.append(f"é£æ ¼æŒ‡å¯¼: {style_guidance}")
        
        # 7. ç”Ÿæˆè¦æ±‚
        prompt_parts.append(f"\n=== ç”Ÿæˆè¦æ±‚ ===")
        prompt_parts.append("è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ç”Ÿæˆä¼˜åŒ–çš„Flux Kontextç¼–è¾‘æŒ‡ä»¤ã€‚")
        prompt_parts.append("ç¡®ä¿æŒ‡ä»¤ç²¾ç¡®ã€å¯æ‰§è¡Œï¼Œå¹¶ç¬¦åˆæŒ‡å®šçš„è¾“å‡ºæ ¼å¼ã€‚")
        prompt_parts.append("é‡ç‚¹æ ¹æ®ç¼–è¾‘æ„å›¾å’Œæ ‡æ³¨ä¿¡æ¯çš„ç»“åˆæ¥ç”ŸæˆæŒ‡ä»¤ã€‚")
        
        return "\n".join(prompt_parts)
    
    def _generate_with_ollama(self, url: str, model: str, system_prompt: str,
                             user_prompt: str, temperature: float, top_p: float = 0.9,
                             keep_alive: int = 5, debug_mode: bool = False, 
                             image_base64: Optional[str] = None) -> Optional[str]:
        """ä½¿ç”¨Ollama HTTP APIç”Ÿæˆå¢å¼ºæŒ‡ä»¤"""
        try:
            import requests
            import json
            
            self._log_debug(f"ğŸ¤– è°ƒç”¨Ollamaæ¨¡å‹: {model} (HTTP API)", debug_mode)
            
            # é…ç½®ç”Ÿæˆå‚æ•°
            options = {
                "temperature": temperature,
                "top_p": top_p,
            }
            
            # å¯¹äºqwen3ç­‰æ”¯æŒthinkingæ¨¡å¼çš„æ¨¡å‹ï¼Œå°è¯•ç¦ç”¨thinkingè¾“å‡º
            if "qwen3" in model.lower():
                options.update({
                    "thinking": False,  # å°è¯•ç¦ç”¨thinkingæ¨¡å¼
                    "stream": False,    # ç¦ç”¨æµå¼è¾“å‡º
                })
                # åœ¨system promptä¸­æ˜ç¡®è¦æ±‚ä¸è¦thinking
                system_prompt += "\n\nIMPORTANT: Do not include any thinking process, reasoning steps, or <think> tags in your response. Output only the final formatted instructions."
            
            # æ„å»ºè¯·æ±‚æ•°æ®
            if image_base64:
                # å¯¹äºå¤šæ¨¡æ€æ¨¡å‹ï¼Œä½¿ç”¨chat APIæ ¼å¼
                payload = {
                    "model": model,
                    "messages": [
                        {
                            "role": "system",
                            "content": system_prompt
                        },
                        {
                            "role": "user",
                            "content": user_prompt,
                            "images": [image_base64]
                        }
                    ],
                    "options": options,
                    "keep_alive": f"{keep_alive}m",
                    "stream": False
                }
                api_endpoint = f"{url}/api/chat"
                self._log_debug("ğŸ–¼ï¸ ä½¿ç”¨å¤šæ¨¡æ€Chat API", debug_mode)
            else:
                # å¯¹äºçº¯æ–‡æœ¬æ¨¡å‹ï¼Œä½¿ç”¨ä¼ ç»Ÿçš„generate API
                payload = {
                    "model": model,
                    "prompt": user_prompt,
                    "system": system_prompt,
                    "options": options,
                    "keep_alive": f"{keep_alive}m",
                    "stream": False
                }
                api_endpoint = f"{url}/api/generate"
                self._log_debug("ğŸ“ ä½¿ç”¨çº¯æ–‡æœ¬Generate API", debug_mode)
            
            # å‘é€è¯·æ±‚åˆ°Ollama HTTP API
            response = requests.post(
                api_endpoint,
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                self._log_debug(f"ğŸ” Ollama APIå“åº”: {str(result)[:200]}...", debug_mode)
                
                generated_text = None
                
                # å¤„ç†ä¸åŒçš„APIå“åº”æ ¼å¼
                if image_base64:
                    # Chat APIå“åº”æ ¼å¼
                    if result and 'message' in result and 'content' in result['message']:
                        generated_text = result['message']['content'].strip()
                        self._log_debug("ğŸ–¼ï¸ è§£æChat APIå“åº”æˆåŠŸ", debug_mode)
                    else:
                        self._log_debug(f"âŒ Chat APIå“åº”æ ¼å¼é”™è¯¯: {result}", debug_mode)
                        return None
                else:
                    # Generate APIå“åº”æ ¼å¼
                    if result and 'response' in result:
                        generated_text = result['response'].strip()
                        self._log_debug("ğŸ“ è§£æGenerate APIå“åº”æˆåŠŸ", debug_mode)
                    else:
                        self._log_debug(f"âŒ Generate APIå“åº”ç¼ºå°‘'response'å­—æ®µ: {result}", debug_mode)
                        return None
                
                if generated_text:
                    # è¿‡æ»¤æ‰qwen3ç­‰æ¨¡å‹çš„thinkingå†…å®¹
                    filtered_text = self._filter_thinking_content(generated_text, debug_mode)
                    
                    self._log_debug(f"âœ… Ollamaç”ŸæˆæˆåŠŸï¼ŒåŸå§‹é•¿åº¦: {len(generated_text)}, è¿‡æ»¤åé•¿åº¦: {len(filtered_text)} å­—ç¬¦", debug_mode)
                    return filtered_text
                else:
                    return None
            else:
                self._log_debug(f"âŒ Ollama APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, å†…å®¹: {response.text}", debug_mode)
                return None
                
        except Exception as e:
            self._log_debug(f"âŒ Ollamaç”Ÿæˆå¤±è´¥: {e}", debug_mode)
            return None
    
    def _filter_thinking_content(self, text: str, debug_mode: bool) -> str:
        """è¿‡æ»¤æ‰æ¨¡å‹çš„thinkingå†…å®¹ (å¦‚qwen3çš„<think>æ ‡ç­¾)"""
        try:
            import re
            
            # è¿‡æ»¤å¸¸è§çš„thinkingæ ‡ç­¾æ ¼å¼
            thinking_patterns = [
                r'<think>.*?</think>',  # qwen3çš„<think>æ ‡ç­¾
                r'<thinking>.*?</thinking>',  # å…¶ä»–å¯èƒ½çš„thinkingæ ‡ç­¾
                r'<thought>.*?</thought>',  # thoughtæ ‡ç­¾
                r'æ€è€ƒ[:ï¼š].*?(?=\n|$)',  # ä¸­æ–‡"æ€è€ƒ:"å¼€å¤´çš„è¡Œ
                r'Let me think.*?(?=\n|$)',  # è‹±æ–‡thinkingå¼€å¤´
                r'I need to think.*?(?=\n|$)',  # å…¶ä»–thinkingè¡¨è¾¾
            ]
            
            filtered_text = text
            original_length = len(text)
            
            # åº”ç”¨æ‰€æœ‰è¿‡æ»¤è§„åˆ™
            for pattern in thinking_patterns:
                filtered_text = re.sub(pattern, '', filtered_text, flags=re.DOTALL | re.IGNORECASE)
            
            # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
            filtered_text = re.sub(r'\n\s*\n\s*\n', '\n\n', filtered_text)  # å¤šä¸ªç©ºè¡Œå˜æˆä¸¤ä¸ª
            filtered_text = filtered_text.strip()
            
            # å¦‚æœè¿‡æ»¤æ‰äº†å†…å®¹ï¼Œè®°å½•æ—¥å¿—
            if len(filtered_text) < original_length:
                self._log_debug(f"ğŸ§¹ è¿‡æ»¤æ‰thinkingå†…å®¹ï¼Œå‡å°‘äº† {original_length - len(filtered_text)} å­—ç¬¦", debug_mode)
            
            return filtered_text
            
        except Exception as e:
            self._log_debug(f"âš ï¸ thinkingå†…å®¹è¿‡æ»¤å¤±è´¥: {e}ï¼Œè¿”å›åŸå§‹å†…å®¹", debug_mode)
            return text
    
    def _format_flux_instructions(self, instructions: str, output_format: str, debug_mode: bool) -> str:
        """æ ¼å¼åŒ–FluxæŒ‡ä»¤"""
        try:
            if output_format == "flux_kontext_standard":
                # å¦‚æœå·²ç»æ˜¯æ ‡å‡†æ ¼å¼ï¼Œç›´æ¥è¿”å›
                if "[EDIT_OPERATIONS]" in instructions:
                    return instructions
                else:
                    # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                    formatted = f"""[EDIT_OPERATIONS]
operation_1: {instructions}

[SPATIAL_CONSTRAINTS]
preserve_regions: ["original_composition"]
blend_boundaries: "seamless"

[QUALITY_CONTROLS]
detail_level: "high"
consistency: "maintain_original_quality"
realism: "photorealistic"
"""
                    return formatted
            elif output_format == "structured_json":
                # è½¬æ¢ä¸ºJSONæ ¼å¼
                try:
                    json_output = {
                        "operations": [{"description": instructions}],
                        "constraints": ["preserve_original_composition"],
                        "quality": {"detail_level": "high", "consistency": "high"}
                    }
                    return json.dumps(json_output, indent=2, ensure_ascii=False)
                except:
                    return instructions
            else:
                # å…¶ä»–æ ¼å¼ç›´æ¥è¿”å›
                return instructions
                
        except Exception as e:
            self._log_debug(f"âš ï¸ æ ¼å¼åŒ–å¤±è´¥ï¼Œè¿”å›åŸå§‹å†…å®¹: {e}", debug_mode)
            return instructions
    
    def _generate_spatial_mappings(self, annotations: List[Dict], debug_mode: bool) -> str:
        """ç”Ÿæˆç©ºé—´æ˜ å°„ä¿¡æ¯"""
        try:
            mappings = {
                "regions": [],
                "coordinate_system": "absolute_pixels",
                "total_annotations": len(annotations)
            }
            
            for i, annotation in enumerate(annotations):
                region = {
                    "id": annotation.get("id", f"annotation_{i+1}"),
                    "type": annotation.get("type", "unknown"),
                    "color_code": annotation.get("color", "#000000"),
                    "number": annotation.get("number", i+1)
                }
                
                # æ·»åŠ åæ ‡ä¿¡æ¯
                if 'start' in annotation and 'end' in annotation:
                    start = annotation['start']
                    end = annotation['end']
                    region["coordinates"] = [
                        start.get('x', 0), start.get('y', 0),
                        end.get('x', 0), end.get('y', 0)
                    ]
                elif 'points' in annotation:
                    region["points"] = annotation['points']
                
                mappings["regions"].append(region)
            
            return json.dumps(mappings, indent=2, ensure_ascii=False)
            
        except Exception as e:
            self._log_debug(f"âš ï¸ ç©ºé—´æ˜ å°„ç”Ÿæˆå¤±è´¥: {e}", debug_mode)
            return f'{{"error": "Failed to generate spatial mappings: {str(e)}"}}'
    
    def _generate_processing_metadata(self, model: str, strategy: str, 
                                    annotation_count: int, debug_mode: bool) -> str:
        """ç”Ÿæˆå¤„ç†å…ƒæ•°æ®"""
        try:
            processing_time = time.time() - self.start_time if self.start_time else 0
            
            metadata = {
                "processing_time": f"{processing_time:.2f}s",
                "timestamp": datetime.now().isoformat(),
                "ollama_model_used": model,
                "edit_strategy": strategy,
                "annotations_processed": annotation_count,
                "enhancement_applied": True,
                "status": "success"
            }
            
            if debug_mode and self.debug_logs:
                metadata["debug_logs"] = self.debug_logs
            
            return json.dumps(metadata, indent=2, ensure_ascii=False)
            
        except Exception as e:
            return f'{{"error": "Failed to generate metadata: {str(e)}"}}'
    
    def _log_debug(self, message: str, debug_mode: bool):
        """è®°å½•è°ƒè¯•ä¿¡æ¯"""
        if debug_mode:
            timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
            log_message = f"[{timestamp}] {message}"
            self.debug_logs.append(log_message)
            print(log_message)  # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
    
    def _create_fallback_output(self, error_msg: str, debug_mode: bool) -> Tuple[str, str]:
        """åˆ›å»ºå¤±è´¥æ—¶çš„å›é€€è¾“å‡º"""
        self._log_debug(f"âŒ åˆ›å»ºå›é€€è¾“å‡º: {error_msg}", debug_mode)
        
        fallback_instructions = f"""[EDIT_OPERATIONS]
operation_1: Apply standard edit to marked regions
# Error: {error_msg}

[SPATIAL_CONSTRAINTS]
preserve_regions: ["all_unmarked_areas"]
blend_boundaries: "seamless"

[QUALITY_CONTROLS]
detail_level: "standard"
consistency: "maintain_original"
"""
        
        fallback_system_prompt = f"Error occurred during processing: {error_msg}"
        
        return (fallback_instructions, fallback_system_prompt)


# æ·»åŠ APIç«¯ç‚¹ç”¨äºåŠ¨æ€è·å–æ¨¡å‹
if WEB_AVAILABLE:
    @PromptServer.instance.routes.post("/ollama_flux_enhancer/get_models")
    async def get_models_endpoint(request):
        """è·å–å¯ç”¨çš„Ollamaæ¨¡å‹åˆ—è¡¨"""
        try:
            data = await request.json()
            url = data.get("url", "http://127.0.0.1:11434")
            
            if not OLLAMA_AVAILABLE:
                return web.json_response([])
            
            from ollama import Client
            client = Client(host=url)
            models_response = client.list()
            models = models_response.get('models', [])
            
            # æå–æ¨¡å‹åç§° - ä½¿ç”¨ä¸get_available_modelsç›¸åŒçš„é€»è¾‘
            model_names = []
            for model in models:
                if isinstance(model, dict):
                    name = model.get('model') or model.get('name')
                    if name:
                        model_names.append(name)
                elif hasattr(model, 'model'):
                    # å¤„ç†å¯¹è±¡ç±»å‹çš„æ¨¡å‹
                    model_names.append(model.model)
                else:
                    # å°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶æå–æ¨¡å‹å
                    model_str = str(model)
                    if "model='" in model_str:
                        # ä»å­—ç¬¦ä¸²ä¸­æå–æ¨¡å‹å ä¾‹: "model='qwen3:0.6b'"
                        start = model_str.find("model='") + 7
                        end = model_str.find("'", start)
                        if end > start:
                            model_names.append(model_str[start:end])
                        else:
                            # å¦‚æœæå–å¤±è´¥ï¼Œè·³è¿‡è¿™ä¸ªæ¨¡å‹
                            print(f"Warning: Failed to extract model name from: {model_str[:100]}...")
                    else:
                        # å¦‚æœæ ¼å¼ä¸åŒ¹é…ï¼Œè·³è¿‡è¿™ä¸ªæ¨¡å‹
                        print(f"Warning: Unknown model format: {model_str[:100]}...")
            
            return web.json_response(model_names)
            
        except Exception as e:
            print(f"Error fetching Ollama models: {e}")
            return web.json_response([])


# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "OllamaFluxKontextEnhancerV2": OllamaFluxKontextEnhancerV2,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "OllamaFluxKontextEnhancerV2": "ğŸ¤– Ollama Flux Kontext Enhancer V2",
}