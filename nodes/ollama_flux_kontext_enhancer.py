"""
OllamaFluxKontextEnhancer Node
Ollama-integrated Flux Kontext prompt enhancement node

Converts VisualPromptEditor annotation data through local Ollama models to
Flux Kontext-optimized structured editing instructions
"""

import json
import time
import traceback
from typing import Dict, List, Any, Tuple, Optional
from datetime import datetime

try:
    from ollama import Client
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False
    print("Warning: ollama package not found. Please install with: pip install ollama")

try:
    from aiohttp import web
    from server import PromptServer
    WEB_AVAILABLE = True
except ImportError:
    WEB_AVAILABLE = False


class OllamaFluxKontextEnhancerV2:
    """
    ğŸ¤– Ollama Flux Kontext Enhancer
    
    é€šè¿‡æœ¬åœ°Ollamaæ¨¡å‹å°†VisualPromptEditorçš„æ ‡æ³¨æ•°æ®
    è½¬æ¢ä¸ºFlux Kontextä¼˜åŒ–çš„ç»“æ„åŒ–ç¼–è¾‘æŒ‡ä»¤
    """
    
    # ç±»çº§åˆ«çš„ç¼“å­˜å˜é‡
    _cached_models = None
    _cache_timestamp = 0
    _cache_duration = 5  # ç¼“å­˜5ç§’ï¼Œå¿«é€Ÿæ›´æ–°æ–°æ¨¡å‹
    _last_successful_url = None  # è®°å½•æœ€åä¸€æ¬¡æˆåŠŸçš„URL
    
    
    @classmethod
    def get_available_models(cls, url=None, force_refresh=False):
        """åŠ¨æ€è·å–å¯ç”¨çš„Ollamaæ¨¡å‹åˆ—è¡¨ - é€šç”¨ç‰ˆæœ¬ï¼Œæ”¯æŒä»»ä½•å·²å®‰è£…çš„æ¨¡å‹"""
        
        import time
        import os
        current_time = time.time()
        
        # åŠ¨æ€è·å–Ollama URLé…ç½®
        if url is None:
            # ä¼˜å…ˆçº§ï¼šç¯å¢ƒå˜é‡ > é…ç½®æ–‡ä»¶ > é»˜è®¤å€¼
            url = (os.getenv('OLLAMA_URL') or 
                   os.getenv('OLLAMA_HOST') or 
                   os.getenv('OLLAMA_BASE_URL') or 
                   "http://127.0.0.1:11434")
        
        # Check if cache is valid
        if (not force_refresh and 
            cls._cached_models is not None and 
            current_time - cls._cache_timestamp < cls._cache_duration):
            return cls._cached_models
        
        def try_http_api(api_url):
            """Try to get model list via HTTP API"""
            try:
                import requests
                response = requests.get(f"{api_url}/api/tags", timeout=10)
                if response.status_code == 200:
                    models_data = response.json()
                    models = models_data.get('models', [])
                    
                    model_names = []
                    for model in models:
                        if isinstance(model, dict):
                            # Try multiple possible field names
                            name = (model.get('name') or 
                                   model.get('model') or 
                                   model.get('id') or 
                                   model.get('model_id'))
                            if name:
                                model_names.append(name)
                    
                    return model_names
            except Exception as e:
                print(f"HTTP API detection failed: {e}")
                return []
        
        def try_ollama_client(api_url):
            """Try to get model list via Ollama client"""
            try:
                if not OLLAMA_AVAILABLE:
                    return []
                
                from ollama import Client
                client = Client(host=api_url)
                models_response = client.list()
                
                if isinstance(models_response, dict):
                    models = models_response.get('models', [])
                elif hasattr(models_response, 'models'):
                    models = models_response.models
                else:
                    models = []
                
                model_names = []
                for model in models:
                    name = None
                    
                    if isinstance(model, dict):
                        name = (model.get('name') or 
                               model.get('model') or 
                               model.get('id'))
                    elif hasattr(model, 'name'):
                        name = model.name
                    elif hasattr(model, 'model'):
                        name = model.model
                    else:
                        # å°è¯•ä»å­—ç¬¦ä¸²è¡¨ç¤ºä¸­æå–æ¨¡å‹å
                        model_str = str(model)
                        if "name=" in model_str or "model=" in model_str:
                            # æ”¯æŒå¤šç§æ ¼å¼: name='xxx', model='xxx'
                            for prefix in ["name='", "model='"]:
                                if prefix in model_str:
                                    start = model_str.find(prefix) + len(prefix)
                                    end = model_str.find("'", start)
                                    if end > start:
                                        name = model_str[start:end]
                                        break
                    
                    if name:
                        model_names.append(name)
                        print(f"âœ… Ollama Client detected model: {name}")
                
                return model_names
                
            except Exception as e:
                print(f"Ollama Client detection failed: {e}")
                return []
        
        # Start model detection process
        print(f"Detecting Ollama models from URL: {url}")
        
        # Try multiple URL formats (smart detection)
        urls_to_try = [url]
        
        # Add common local address variants
        if url not in ["http://127.0.0.1:11434", "http://localhost:11434", "http://0.0.0.0:11434"]:
            urls_to_try.extend([
                "http://127.0.0.1:11434",
                "http://localhost:11434", 
                "http://0.0.0.0:11434"
            ])
        
        # Remove duplicates while preserving order
        urls_to_try = list(dict.fromkeys(urls_to_try))
        
        all_models = set()  # Use set to avoid duplicates
        successful_url = None
        
        for test_url in urls_to_try:
            try:
                # Method 1: HTTP API
                http_models = try_http_api(test_url)
                if http_models:
                    all_models.update(http_models)
                    successful_url = test_url
                    print(f"Found {len(http_models)} models via HTTP API from {test_url}")
                
                # Method 2: Ollama Client
                client_models = try_ollama_client(test_url)
                if client_models:
                    all_models.update(client_models)
                    successful_url = test_url
                    print(f"Found {len(client_models)} models via Ollama Client from {test_url}")
                
                # Exit early if models found
                if all_models:
                    break
                    
            except Exception as e:
                print(f"Failed to test URL {test_url}: {e}")
                continue
        
        # Convert to sorted list
        model_list = sorted(list(all_models))
        
        if model_list:
            print(f"Total {len(model_list)} unique models detected")
            
            # Update cache (including successful URL)
            cls._cached_models = model_list
            cls._cache_timestamp = current_time
            if successful_url:
                cls._last_successful_url = successful_url
            print(f"Model list cached for {cls._cache_duration} seconds")
            
            return model_list
        
        # If no models detected, return fallback
        print("Warning: No models detected, returning fallback list")
        fallback_models = ["ollama-model-not-found"]
        print("Please ensure:")
        print("   1. Ollama service is running (ollama serve)")
        print("   2. At least one model is installed (ollama pull <model_name>)")
        print("   3. Service is accessible (curl http://localhost:11434/api/tags)")
        
        # Cache fallback to avoid repeated error detection
        cls._cached_models = fallback_models
        cls._cache_timestamp = current_time
        
        return fallback_models

    @classmethod
    def refresh_model_cache(cls):
        """æ‰‹åŠ¨åˆ·æ–°æ¨¡å‹ç¼“å­˜"""
        print("ğŸ”„ Manually refreshing model cache...")
        cls._cached_models = None
        cls._cache_timestamp = 0
        return cls.get_available_models(force_refresh=True)

    @classmethod
    def get_template_content_for_placeholder(cls, guidance_style, guidance_template):
        """è·å–æ¨¡æ¿å†…å®¹ç”¨äºplaceholderæ˜¾ç¤º"""
        try:
            # å¯¼å…¥guidance_templatesæ¨¡å—
            from .guidance_templates import PRESET_GUIDANCE, TEMPLATE_LIBRARY
            
            # æ ¹æ®guidance_styleé€‰æ‹©å†…å®¹
            if guidance_style == "custom":
                # Custom mode retains complete prompt text
                return """Enter your custom AI guidance instructions...

For example:
You are a professional image editing expert. Please convert annotation data into clear and concise editing instructions. Focus on:
1. Keep instructions concise
2. Ensure precise operations
3. Maintain style consistency

For more examples, please check guidance_template options."""
            elif guidance_style == "template":
                if guidance_template and guidance_template != "none" and guidance_template in TEMPLATE_LIBRARY:
                    template_content = TEMPLATE_LIBRARY[guidance_template]["prompt"]
                    # æˆªå–å‰200ä¸ªå­—ç¬¦ç”¨äºplaceholderæ˜¾ç¤º
                    preview = template_content[:200].replace('\n', ' ').strip()
                    return f"Current template: {TEMPLATE_LIBRARY[guidance_template]['name']}\n\n{preview}..."
                else:
                    return "Preview will be displayed here after selecting a template..."
            else:
                # Display preset style content
                if guidance_style in PRESET_GUIDANCE:
                    preset_content = PRESET_GUIDANCE[guidance_style]["prompt"]
                    # æˆªå–å‰200ä¸ªå­—ç¬¦ç”¨äºplaceholderæ˜¾ç¤º
                    preview = preset_content[:200].replace('\n', ' ').strip()
                    return f"Current style: {PRESET_GUIDANCE[guidance_style]['name']}\n\n{preview}..."
                else:
                    return """Enter your custom AI guidance instructions...

For example:
You are a professional image editing expert. Please convert annotation data into clear and concise editing instructions. Focus on:
1. Keep instructions concise
2. Ensure precise operations
3. Maintain style consistency

For more examples, please check guidance_template options."""
        except Exception as e:
            print(f"Failed to get template content: {e}")
            return """Enter your custom AI guidance instructions...

For example:
You are a professional image editing expert. Please convert annotation data into clear and concise editing instructions. Focus on:
1. Keep instructions concise
2. Ensure precise operations
3. Maintain style consistency

For more examples, please check guidance_template options."""

    @classmethod
    def INPUT_TYPES(cls):
        # åŠ¨æ€è·å–å®é™…å¯ç”¨çš„Ollamaæ¨¡å‹åˆ—è¡¨ï¼Œæ¯æ¬¡éƒ½å¼ºåˆ¶åˆ·æ–°ä»¥è·å–æœ€æ–°æ¨¡å‹
        try:
            # æ¸…ç©ºç¼“å­˜ç¡®ä¿è·å–æœ€æ–°æ¨¡å‹åˆ—è¡¨
            cls._cached_models = None
            cls._cache_timestamp = 0
            available_models = cls.get_available_models(force_refresh=True)
            
            # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°æ¨¡å‹ï¼Œä½¿ç”¨å¤‡ç”¨é€‰é¡¹
            if not available_models or len(available_models) == 0:
                available_models = ["No models found - Start Ollama service"]
            else:
                # åœ¨åˆ—è¡¨å¼€å¤´æ·»åŠ åˆ·æ–°é€‰é¡¹
                available_models = ["ğŸ”„ Refresh model list"] + available_models
            
            # è®¾ç½®é»˜è®¤æ¨¡å‹ä¸ºç¬¬ä¸€ä¸ªå®é™…æ¨¡å‹ï¼ˆè·³è¿‡åˆ·æ–°é€‰é¡¹ï¼‰
            if len(available_models) > 1 and available_models[0] == "ğŸ”„ Refresh model list":
                default_model = available_models[1]
            else:
                default_model = available_models[0]
            
        except Exception as e:
            print(f"Failed to get dynamic model list: {e}")
            available_models = ["Error getting models - Check Ollama"]
            default_model = available_models[0]
        
        
        # åŠ¨æ€ç”Ÿæˆplaceholderå†…å®¹
        try:
            default_placeholder = cls.get_template_content_for_placeholder("efficient_concise", "none")
        except Exception as e:
            default_placeholder = "Enter your custom AI guidance instructions..."
        
        return {
            "required": {
                "annotation_data": ("STRING", {
                    "forceInput": True,
                    "tooltip": "Annotation JSON data from VisualPromptEditor (connected input)"
                }),
                "edit_description": ("STRING", {
                    "multiline": True,
                    "default": "",
                    "placeholder": "Describe the editing operations you want to perform...\n\nFor example:\n- Add a tree in the red rectangular area\n- Change the vehicle in the blue marked area to red\n- Remove the person in the circular area\n- Change the sky in the yellow area to sunset effect",
                    "tooltip": "Describe the editing operations you want to perform, combined with annotation information to generate precise editing instructions"
                }),
                "model": (available_models, {
                    "default": default_model,
                    "tooltip": "Select Ollama model. List is fetched in real-time from Ollama service, showing all currently available models."
                }),
                "edit_instruction_type": ([
                    "auto_detect",          # ğŸ”„ Automatically select best strategy based on operation type
                    "spatial_precise",      # Spatial precise editing
                    "semantic_enhanced",    # Semantic enhanced editing  
                    "style_coherent",       # Style coherent editing
                    "content_aware",        # Content aware editing
                    "multi_region",         # Multi-region coordinated editing
                    "custom"                # Custom instructions
                ], {
                    "default": "auto_detect",
                    "tooltip": "Select editing instruction generation strategy (auto_detect automatically selects based on operation type)"
                }),
                "guidance_style": ([
                    "efficient_concise",   # Efficient Concise (default)
                    "natural_creative",    # Natural Creative
                    "technical_precise",   # Technical Precise
                    "template",            # Template Selection
                    "custom"              # Custom User Input
                ], {
                    "default": "efficient_concise",
                    "tooltip": "Select AI guidance style: Efficient Concise for quick editing, Natural Creative for artistic design, Technical Precise for professional use, Template for common presets, Custom for user-defined guidance"
                }),
            },
            "optional": {
                "image": ("IMAGE", {
                    "tooltip": "Optional: Image for visual analysis (required only when visual models are supported)"
                }),
                "url": ("STRING", {
                    "default": "http://127.0.0.1:11434",
                    "tooltip": "Ollama service address"
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.1,
                    "max": 1.0,
                    "step": 0.1,
                    "tooltip": "Generation temperature (creativity control)"
                }),
                "enable_visual_analysis": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "Enable visual analysis (only effective for multimodal models that support vision, such as qwen-vl, llava, etc.)"
                }),
                "guidance_template": ([
                    "none",               # No Template
                    "ecommerce_product",  # E-commerce Product
                    "portrait_beauty",    # Portrait Beauty
                    "creative_design",    # Creative Design
                    "architecture_photo", # Architecture Photography
                    "food_photography",   # Food Photography
                    "fashion_retail",     # Fashion Retail
                    "landscape_nature"    # Landscape Nature
                ], {
                    "default": "none",
                    "tooltip": "Select specialized guidance template (used when guidance_style is template)"
                }),
                "seed": ("INT", {
                    "default": 42,
                    "min": 0,
                    "max": 2**32 - 1,
                    "tooltip": "Seed for controlling randomness in generation. Use the same seed for reproducible results."
                }),
                "custom_guidance": ("STRING", {
                    "multiline": True,
                    "default": "",
                    "placeholder": default_placeholder,
                    "tooltip": "Custom AI guidance instructions (used when guidance_style is custom)"
                }),
                "load_saved_guidance": (["none"], {
                    "default": "none",
                    "tooltip": "Load previously saved custom guidance (used when guidance_style is custom)"
                }),
            }
        }
    
    @classmethod
    def VALIDATE_INPUTS(cls, **kwargs):
        """éªŒè¯è¾“å…¥å‚æ•°"""
        model = kwargs.get('model', '')
        url = kwargs.get('url', 'http://127.0.0.1:11434')
        
        # å¦‚æœmodelä¸ºç©ºï¼Œå°è¯•è·å–å¯ç”¨æ¨¡å‹å¹¶ä½¿ç”¨ç¬¬ä¸€ä¸ª
        if not model or model == '':
            available_models = cls.get_available_models(url=url)
            if available_models:
                # è¿”å›Trueè¡¨ç¤ºéªŒè¯é€šè¿‡ï¼ŒComfyUIä¼šä½¿ç”¨é»˜è®¤å€¼
                return True
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨å¯ç”¨åˆ—è¡¨ä¸­ï¼Œå…ˆå°è¯•ç¼“å­˜çš„åˆ—è¡¨
        available_models = cls.get_available_models(url=url, force_refresh=False)
        if model not in available_models and model not in ["ollama-model-not-found", "è¯·å…ˆå¯åŠ¨OllamaæœåŠ¡"]:
            # å¦‚æœæ¨¡å‹ä¸åœ¨ç¼“å­˜ä¸­ï¼Œå¼ºåˆ¶åˆ·æ–°ä¸€æ¬¡
            available_models = cls.get_available_models(url=url, force_refresh=True)
            if model not in available_models:
                print(f"âš ï¸ Model '{model}' not in available list: {available_models}")
                # ä¸è¿”å›é”™è¯¯ï¼Œè®©ç”¨æˆ·çŸ¥é“ä½†ä»å¯ä»¥ç»§ç»­
                return True
        
        return True
    
    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = (
        "flux_edit_instructions",  # Flux Kontextæ ¼å¼çš„ç¼–è¾‘æŒ‡ä»¤
        "system_prompt",           # å‘é€ç»™æ¨¡å‹çš„å®Œæ•´ç³»ç»ŸæŒ‡ä»¤
    )
    
    FUNCTION = "enhance_flux_instructions"
    CATEGORY = "kontext/ai_enhanced"
    DESCRIPTION = "ğŸ¤– é€šè¿‡Ollamaå¢å¼ºVisualPromptEditorçš„æ ‡æ³¨æ•°æ®ï¼Œç”ŸæˆFlux Kontextä¼˜åŒ–çš„ç»“æ„åŒ–ç¼–è¾‘æŒ‡ä»¤"
    
    def __init__(self):
        self.start_time = None
        self.debug_logs = []
        # ç®€å•çš„å†…å­˜ç¼“å­˜
        self.cache = {}
        self.cache_max_size = 100
    
    def _get_cache_key(self, annotation_data: str, edit_description: str, 
                      edit_instruction_type: str, model: str, temperature: float,
                      guidance_style: str, guidance_template: str, seed: int,
                      custom_guidance: str = "", load_saved_guidance: str = "none") -> str:
        """ç”Ÿæˆç¼“å­˜é”®ï¼ŒåŒ…å«æ‰€æœ‰å‚æ•°"""
        import hashlib
        content = f"{annotation_data}|{edit_description}|{edit_instruction_type}|{model}|{temperature}|{guidance_style}|{guidance_template}|{seed}|{custom_guidance}|{load_saved_guidance}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _manage_cache(self):
        """ç®¡ç†ç¼“å­˜å¤§å°"""
        if len(self.cache) > self.cache_max_size:
            # åˆ é™¤æœ€æ—§çš„æ¡ç›®
            oldest_key = min(self.cache.keys(), key=lambda k: self.cache[k]['timestamp'])
            del self.cache[oldest_key]
            print(f"ğŸ—‘ï¸ Removed oldest cache entry, cache size: {len(self.cache)}")
    
    def enhance_flux_instructions(self, annotation_data: str, edit_description: str, model: str, 
                                edit_instruction_type: str,
                                image=None, url: str = "http://127.0.0.1:11434", temperature: float = 0.7,
                                enable_visual_analysis: bool = False,
                                guidance_style: str = "efficient_concise",
                                guidance_template: str = "none", seed: int = 42,
                                custom_guidance: str = "", load_saved_guidance: str = "none"):
        """é€šè¿‡Ollamaå¢å¼ºæ ‡æ³¨æ•°æ®ï¼Œç”ŸæˆFlux Kontextä¼˜åŒ–çš„ç¼–è¾‘æŒ‡ä»¤"""
        
        # Set default values for removed parameters
        reference_context = ""
        edit_intensity = 0.8
        preservation_mask = ""
        style_guidance = ""
        top_p = 0.9
        keep_alive = 5
        debug_mode = False  # Fixed debug_mode to False
        language = "english"  # Fixed language to English for downstream processing
        output_format = "natural_language"  # Fixed output format to natural language
        
        
        # å¯¼å…¥å¼•å¯¼è¯æœ¯ç®¡ç†å™¨
        try:
            from .guidance_templates import guidance_manager
        except ImportError:
            # å›é€€åˆ°ç»å¯¹å¯¼å…¥
            import sys
            import os
            sys.path.append(os.path.dirname(__file__))
            from guidance_templates import guidance_manager
        
        # æ„å»ºç³»ç»Ÿæç¤ºè¯ï¼ˆæ•´åˆå¼•å¯¼è¯æœ¯å’Œè¯­è¨€æ§åˆ¶ï¼‰
        enhanced_system_prompt = guidance_manager.build_system_prompt(
            guidance_style=guidance_style,
            guidance_template=guidance_template,
            custom_guidance=custom_guidance,
            load_saved_guidance=load_saved_guidance,
            language=language
        )
        print(f"Using guidance mode: {guidance_style}")
        print(f"Output language: {language}")
        print(f"Visual analysis: {'enabled' if enable_visual_analysis else 'disabled'}")
        
        print(f"Using model: {model}")
        print(f"Edit strategy: {edit_instruction_type}")
        print(f"Output format: {output_format}")
        
        # Smart model handling logic
        if model == "ğŸ”„ Refresh model list":
            # User clicked refresh, get latest model list and use first available
            print("Refreshing model list...")
            self.__class__._cached_models = None  # Clear cache
            self.__class__._cache_timestamp = 0
            available_models = self.get_available_models(url=url, force_refresh=True)
            if available_models:
                model = available_models[0]
                print(f"Model list refreshed, using: {model}")
            else:
                print("Error: No models found after refresh")
                return self._create_fallback_output("No models found after refresh", False)
        
        elif model == "custom-model-name":
            # User needs to manually input model name or add custom logic here
            print("Info: Please change this option to your actual model name")
            # Try auto-detection
            available_models = self.get_available_models(url=url, force_refresh=True)
            if available_models:
                model = available_models[0]
                print(f"Using detected model: {model}")
        
        elif model in ["No models found - Start Ollama service", "Error getting models - Check Ollama"]:
            # Handle error states
            print("Error: Please start Ollama service and ensure models are installed")
            return self._create_fallback_output("Ollama service not available or no models installed", False)
        
        else:
            # Validate if specified model is available
            available_models = self.get_available_models(url=url, force_refresh=False)
            if model not in available_models:
                print(f"Warning: Specified model {model} not available, detected models: {available_models}")
                if available_models:
                    print(f"Auto-switching to available model: {available_models[0]}")
                    model = available_models[0]
                else:
                    print("Error: No available models detected")
                    return self._create_fallback_output(f"Model {model} not available and no other models found", False)
        
        self.start_time = time.time()
        self.debug_logs = []
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = self._get_cache_key(
            annotation_data, edit_description, edit_instruction_type, 
            model, temperature, guidance_style, guidance_template, seed,
            custom_guidance, load_saved_guidance
        )
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self.cache:
            cached_result = self.cache[cache_key]
            cache_age = time.time() - cached_result['timestamp']
            if cache_age < 3600:  # ç¼“å­˜1å°æ—¶
                print(f"ğŸ¯ Using cached result (age: {cache_age:.1f}s)")
                return cached_result['result']
            else:
                # ç¼“å­˜è¿‡æœŸï¼Œåˆ é™¤
                del self.cache[cache_key]
                print(f"ğŸ—‘ï¸ Cache expired (age: {cache_age:.1f}s), regenerating")
        
        try:
            # æ£€æŸ¥OllamaæœåŠ¡å¯ç”¨æ€§ï¼ˆé€šè¿‡HTTP APIï¼‰
            if not self._check_ollama_service(url):
                return self._create_fallback_output(
                    f"Ollama service not available at {url}. Please start ollama service.",
                    debug_mode
                )
            
            # è‡ªåŠ¨æ£€æµ‹æœ€ä½³ç¼–è¾‘ç­–ç•¥
            if edit_instruction_type == "auto_detect":
                edit_instruction_type = self._auto_detect_strategy(annotation_data, debug_mode)
                self._log_debug(f"ğŸ”„ è‡ªåŠ¨æ£€æµ‹åˆ°æœ€ä½³ç­–ç•¥: {edit_instruction_type}", debug_mode)
            
            self._log_debug(f"ğŸš€ å¼€å§‹å¤„ç† - æ¨¡å‹: {model}, ç­–ç•¥: {edit_instruction_type}", debug_mode)
            
            # 1. è§£ææ ‡æ³¨æ•°æ®
            annotations, parsed_data = self._parse_annotation_data(annotation_data, debug_mode)
            if not annotations:
                return self._create_fallback_output(
                    "No valid annotations found in annotation_data",
                    debug_mode
                )
            
            # 2. OllamaæœåŠ¡å·²é€šè¿‡å‰é¢çš„æ£€æŸ¥ç¡®è®¤å¯ç”¨
            self._log_debug(f"ğŸ”— ä½¿ç”¨OllamaæœåŠ¡: {url}", debug_mode)
            
            # 3. æ„å»ºç”¨æˆ·æç¤ºè¯ï¼ˆç³»ç»Ÿæç¤ºè¯å·²åœ¨ä¸Šé¢é€šè¿‡å¼•å¯¼è¯æœ¯ç³»ç»Ÿæ„å»ºï¼‰
            user_prompt = self._build_user_prompt(
                annotations, parsed_data, edit_description, reference_context, 
                edit_intensity, preservation_mask, style_guidance, output_format
            )
            
            self._log_debug(f"ğŸ“ ç”Ÿæˆçš„ç”¨æˆ·æç¤ºè¯é•¿åº¦: {len(user_prompt)} å­—ç¬¦", debug_mode)
            
            # 4. æ£€æŸ¥æ˜¯å¦éœ€è¦è§†è§‰åˆ†æ
            image_base64 = None
            if enable_visual_analysis:
                if self._is_multimodal_model(model):
                    image_base64 = self._encode_image_for_ollama(image, debug_mode)
                    if image_base64:
                        self._log_debug("ğŸ” å¯ç”¨è§†è§‰åˆ†ææ¨¡å¼", debug_mode)
                    else:
                        self._log_debug("âš ï¸ å›¾åƒç¼–ç å¤±è´¥ï¼Œå›é€€åˆ°çº¯æ–‡æœ¬æ¨¡å¼", debug_mode)
                else:
                    self._log_debug(f"âš ï¸ æ¨¡å‹ {model} ä¸æ”¯æŒè§†è§‰åˆ†æï¼Œå¿½ç•¥è§†è§‰è¾“å…¥", debug_mode)
            
            # 5. è°ƒç”¨Ollamaç”Ÿæˆå¢å¼ºæŒ‡ä»¤ï¼ˆä½¿ç”¨å¼•å¯¼è¯æœ¯ç³»ç»Ÿæ„å»ºçš„enhanced_system_promptï¼‰
            enhanced_instructions = self._generate_with_ollama(
                url, model, enhanced_system_prompt, user_prompt,
                temperature, top_p, keep_alive, debug_mode, image_base64, seed
            )
            
            if not enhanced_instructions:
                # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                error_msg = f"Failed to generate enhanced instructions from Ollama. Model: {model}, URL: {url}"
                print(f"Error: {error_msg}")
                print("Troubleshooting tips:")
                print("1. Check if Ollama service is running: ollama serve")
                print("2. Verify the model is installed: ollama list")
                print(f"3. Test model manually: ollama run {model}")
                print("4. Check if the URL is accessible")
                return self._create_fallback_output(error_msg, debug_mode)
            
            # 5. æ ¼å¼åŒ–è¾“å‡º
            flux_instructions = self._format_flux_instructions(
                enhanced_instructions, output_format, debug_mode
            )
            
            self._log_debug("âœ… å¤„ç†å®Œæˆ", debug_mode)
            
            # ä¿å­˜åˆ°ç¼“å­˜
            result = (flux_instructions, enhanced_system_prompt)
            self.cache[cache_key] = {
                'result': result,
                'timestamp': time.time()
            }
            self._manage_cache()  # ç®¡ç†ç¼“å­˜å¤§å°
            print(f"ğŸ’¾ Result cached (cache size: {len(self.cache)})")
            
            return result
            
        except Exception as e:
            error_msg = f"Error in enhance_flux_instructions: {str(e)}"
            if debug_mode:
                error_msg += f"\n{traceback.format_exc()}"
            return self._create_fallback_output(error_msg, debug_mode)
    
    def _check_ollama_service(self, url: str) -> bool:
        """æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦å¯ç”¨"""
        try:
            import requests
            print(f"Checking Ollama service at: {url}")
            response = requests.get(f"{url}/api/tags", timeout=5)
            if response.status_code == 200:
                print("Ollama service is accessible")
                return True
            else:
                print(f"Ollama service returned status code: {response.status_code}")
                return False
        except Exception as e:
            print(f"Failed to connect to Ollama service: {e}")
            return False
    
    def _is_multimodal_model(self, model: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒè§†è§‰åˆ†æ"""
        multimodal_models = [
            "qwen-vl", "qwen2-vl", "qwen:vl", "qwen2:vl",
            "llava", "llava:latest", "llava:7b", "llava:13b", "llava:34b",
            "llava-llama3", "llava-phi3", "llava-code",
            "moondream", "cogvlm", "cogvlm2",
            "yi-vl", "internvl", "minicpm-v"
        ]
        
        model_lower = model.lower()
        for mm_model in multimodal_models:
            if mm_model in model_lower:
                return True
        return False
    
    def _encode_image_for_ollama(self, image, debug_mode: bool) -> Optional[str]:
        """å°†å›¾åƒç¼–ç ä¸ºOllamaå¯ç”¨çš„base64æ ¼å¼"""
        try:
            import torch
            import numpy as np
            from PIL import Image
            import io
            import base64
            
            # å¤„ç†ComfyUIçš„å›¾åƒæ ¼å¼ (tensor)
            if isinstance(image, torch.Tensor):
                # ComfyUIå›¾åƒæ ¼å¼: [batch, height, width, channels]
                if image.dim() == 4:
                    image = image[0]  # å–ç¬¬ä¸€å¼ å›¾åƒ
                
                # è½¬æ¢ä¸ºnumpy
                if image.dtype == torch.float32:
                    # ä»[0,1]èŒƒå›´è½¬æ¢åˆ°[0,255]
                    image_np = (image * 255).clamp(0, 255).byte().cpu().numpy()
                else:
                    image_np = image.cpu().numpy()
                
                # åˆ›å»ºPILå›¾åƒ
                pil_image = Image.fromarray(image_np, mode='RGB')
            else:
                # å¦‚æœå·²ç»æ˜¯PILå›¾åƒ
                pil_image = image
            
            # è½¬æ¢ä¸ºJPEGæ ¼å¼çš„base64
            buffer = io.BytesIO()
            pil_image.save(buffer, format='JPEG', quality=85)
            img_bytes = buffer.getvalue()
            img_base64 = base64.b64encode(img_bytes).decode('utf-8')
            
            self._log_debug(f"ğŸ–¼ï¸ å›¾åƒç¼–ç æˆåŠŸï¼Œbase64é•¿åº¦: {len(img_base64)} å­—ç¬¦", debug_mode)
            return img_base64
            
        except Exception as e:
            self._log_debug(f"âŒ å›¾åƒç¼–ç å¤±è´¥: {e}", debug_mode)
            return None
    
    def _auto_detect_strategy(self, annotation_data: str, debug_mode: bool) -> str:
        """æ ¹æ®annotationæ•°æ®è‡ªåŠ¨æ£€æµ‹æœ€ä½³ç¼–è¾‘ç­–ç•¥"""
        try:
            if not annotation_data or not annotation_data.strip():
                return "spatial_precise"  # é»˜è®¤ç­–ç•¥
            
            parsed_data = json.loads(annotation_data)
            operation_type = parsed_data.get('operation_type', '')
            
            # Visual Prompt Editoræ“ä½œç±»å‹åˆ°ç¼–è¾‘ç­–ç•¥çš„æ˜ å°„
            operation_to_strategy = {
                # ç©ºé—´ç²¾å‡†ç±»æ“ä½œ
                "change_color": "spatial_precise",
                "replace_object": "spatial_precise", 
                "resize_object": "spatial_precise",
                "geometric_warp": "spatial_precise",
                "perspective_transform": "spatial_precise",
                "precision_cutout": "spatial_precise",
                
                # è¯­ä¹‰å¢å¼ºç±»æ“ä½œ
                "add_object": "semantic_enhanced",
                "remove_object": "semantic_enhanced",
                "change_expression": "semantic_enhanced",
                "character_expression": "semantic_enhanced",
                "content_aware_fill": "semantic_enhanced",
                "seamless_removal": "semantic_enhanced",
                
                # é£æ ¼ä¸€è‡´æ€§ç±»æ“ä½œ
                "change_style": "style_coherent",
                "change_texture": "style_coherent",
                "global_style_transfer": "style_coherent",
                "style_blending": "style_coherent",
                "global_color_grade": "style_coherent",
                
                # å†…å®¹æ„ŸçŸ¥ç±»æ“ä½œ
                "enhance_quality": "content_aware",
                "change_pose": "content_aware",
                "change_clothing": "content_aware",
                "enhance_skin_texture": "content_aware",
                "detail_enhance": "content_aware",
                "realism_enhance": "content_aware",
                
                # å¤šåŒºåŸŸåè°ƒç±»æ“ä½œï¼ˆæ£€æµ‹å¤šä¸ªæ ‡æ³¨ï¼‰
                "global_enhance": "multi_region",
                "global_filter": "multi_region",
                "collage_integration": "multi_region",
                "texture_mixing": "multi_region"
            }
            
            # æ ¹æ®æ“ä½œç±»å‹é€‰æ‹©ç­–ç•¥
            detected_strategy = operation_to_strategy.get(operation_type, "spatial_precise")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¤šä¸ªæ ‡æ³¨åŒºåŸŸ
            annotations = parsed_data.get('annotations', [])
            if len(annotations) > 2:
                detected_strategy = "multi_region"
            
            self._log_debug(f"ğŸ¯ æ“ä½œç±»å‹: {operation_type} â†’ ç­–ç•¥: {detected_strategy}", debug_mode)
            return detected_strategy
            
        except Exception as e:
            self._log_debug(f"âš ï¸ ç­–ç•¥è‡ªåŠ¨æ£€æµ‹å¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤ç­–ç•¥", debug_mode)
            return "spatial_precise"
    
    def _parse_annotation_data(self, annotation_data: str, debug_mode: bool) -> Tuple[List[Dict], Dict]:
        """è§£ææ ‡æ³¨æ•°æ®"""
        try:
            if not annotation_data or not annotation_data.strip():
                self._log_debug("âš ï¸ æ ‡æ³¨æ•°æ®ä¸ºç©º", debug_mode)
                return [], {}
            
            parsed_data = json.loads(annotation_data)
            self._log_debug(f"ğŸ“Š è§£ææ ‡æ³¨æ•°æ®æˆåŠŸï¼Œæ•°æ®ç±»å‹: {type(parsed_data)}", debug_mode)
            
            # æå–annotations
            annotations = []
            if isinstance(parsed_data, dict):
                if "annotations" in parsed_data:
                    annotations = parsed_data["annotations"]
                elif "layers_data" in parsed_data:
                    annotations = parsed_data["layers_data"]
            elif isinstance(parsed_data, list):
                annotations = parsed_data
            
            self._log_debug(f"ğŸ“ æå–åˆ° {len(annotations)} ä¸ªæ ‡æ³¨", debug_mode)
            return annotations, parsed_data
            
        except json.JSONDecodeError as e:
            self._log_debug(f"âŒ JSONè§£æå¤±è´¥: {e}", debug_mode)
            return [], {}
        except Exception as e:
            self._log_debug(f"âŒ æ ‡æ³¨æ•°æ®è§£æå¼‚å¸¸: {e}", debug_mode)
            return [], {}
    
    def _connect_ollama(self, url: str, debug_mode: bool) -> Optional[object]:
        """è¿æ¥OllamaæœåŠ¡"""
        try:
            if not OLLAMA_AVAILABLE:
                self._log_debug("âŒ Ollamaæ¨¡å—ä¸å¯ç”¨", debug_mode)
                return None
                
            from ollama import Client
            client = Client(host=url)
            # æµ‹è¯•è¿æ¥
            models = client.list()
            self._log_debug(f"ğŸ”— Ollamaè¿æ¥æˆåŠŸï¼Œå¯ç”¨æ¨¡å‹æ•°: {len(models.get('models', []))}", debug_mode)
            return client
        except Exception as e:
            self._log_debug(f"âŒ Ollamaè¿æ¥å¤±è´¥: {e}", debug_mode)
            return None
    
    def _build_system_prompt(self, edit_instruction_type: str, output_format: str) -> str:
        """æ„å»ºç³»ç»Ÿæç¤ºè¯"""
        
        # ç¼–è¾‘ç­–ç•¥è¯´æ˜åŠå…·ä½“æŒ‡å¯¼
        strategy_descriptions = {
            "spatial_precise": {
                "description": "ä¸“æ³¨äºç²¾ç¡®çš„ç©ºé—´ä½ç½®æè¿°å’Œåæ ‡å®šä½",
                "guidance": """Focus on:
- Precise spatial positioning and coordinates
- Exact boundary definitions
- Clear geometric transformations
- Maintain spatial relationships
- Include specific pixel/region coordinates when possible"""
            },
            "semantic_enhanced": {
                "description": "å¼ºè°ƒè¯­ä¹‰ç†è§£å’Œå†…å®¹è¯†åˆ«ï¼Œç”Ÿæˆè¯­ä¹‰ä¸°å¯Œçš„ç¼–è¾‘æŒ‡ä»¤",
                "guidance": """Focus on:
- Object recognition and semantic understanding
- Context-aware content modifications
- Intelligent object relationships
- Meaningful content additions/removals
- Preserve semantic coherence"""
            },
            "style_coherent": {
                "description": "æ³¨é‡æ•´ä½“é£æ ¼çš„åè°ƒç»Ÿä¸€ï¼Œç¡®ä¿ç¼–è¾‘åçš„è§†è§‰ä¸€è‡´æ€§",
                "guidance": """Focus on:
- Visual style consistency
- Color harmony and palette coherence
- Texture and material uniformity
- Lighting and shadow consistency
- Overall aesthetic unity"""
            },
            "content_aware": {
                "description": "æ™ºèƒ½ç†è§£å›¾åƒå†…å®¹å’Œä¸Šä¸‹æ–‡ï¼Œç”Ÿæˆå†…å®¹æ„ŸçŸ¥çš„ç¼–è¾‘æŒ‡ä»¤",
                "guidance": """Focus on:
- Context-sensitive modifications
- Content-appropriate enhancements
- Scene understanding
- Natural content integration
- Preserve content authenticity"""
            },
            "multi_region": {
                "description": "å¤„ç†å¤šä¸ªæ ‡æ³¨åŒºåŸŸçš„åè°ƒå…³ç³»ï¼Œç¡®ä¿æ•´ä½“ç¼–è¾‘çš„å’Œè°æ€§",
                "guidance": """Focus on:
- Coordinate multiple region edits
- Ensure region-to-region harmony
- Balance competing modifications
- Maintain overall composition
- Synchronize related changes"""
            },
            "custom": {
                "description": "æ ¹æ®ç”¨æˆ·éœ€æ±‚ç”Ÿæˆè‡ªå®šä¹‰çš„ç¼–è¾‘æŒ‡ä»¤",
                "guidance": """Focus on:
- User-specific requirements
- Flexible adaptation to needs
- Custom modification approaches
- Personalized editing strategies"""
            }
        }
        
        # è¾“å‡ºæ ¼å¼è¯´æ˜
        format_descriptions = {
            "flux_kontext_standard": """Output EXACTLY in this format:

[EDIT_OPERATIONS]
operation_1: change the marked area to [target color/object]

[SPATIAL_CONSTRAINTS]
preserve_regions: ["background"]
blend_boundaries: "seamless"

[QUALITY_CONTROLS]
detail_level: "high"
consistency: "maintain_original_quality"
""",
            "structured_json": "è¾“å‡ºç»“æ„åŒ–çš„JSONæ ¼å¼ï¼ŒåŒ…å«æ“ä½œã€çº¦æŸå’Œè´¨é‡æ§åˆ¶ä¿¡æ¯",
            "natural_language": "Output clean, natural language descriptions without technical details, annotation numbers, or structured formatting. Focus on the core editing action using color and spatial descriptions (e.g., 'transform the red area into blue', 'remove the object in the center'). Avoid any annotation references like 'annotation 0' or technical instructions."
        }
        
        # è·å–å½“å‰ç­–ç•¥çš„æŒ‡å¯¼
        current_strategy = strategy_descriptions.get(edit_instruction_type, strategy_descriptions["spatial_precise"])
        
        system_prompt = f"""Generate image editing instructions in Flux Kontext format.

EDITING STRATEGY: {edit_instruction_type}
{current_strategy["guidance"]}

OUTPUT FORMAT:
{format_descriptions.get(output_format, "Standard format")}

Rules:
- Output ONLY the formatted instructions
- Do NOT include code, explanations, or comments
- Apply the specific editing strategy focus areas listed above
- Ensure instructions match the strategy requirements"""
        
        return system_prompt
    
    def _build_user_prompt(self, annotations: List[Dict], parsed_data: Dict,
                          edit_description: str = "", reference_context: str = "", edit_intensity: float = 0.8,
                          preservation_mask: str = "", style_guidance: str = "", output_format: str = "natural_language") -> str:
        """æ„å»ºç”¨æˆ·æç¤ºè¯"""
        
        # For natural language format, use simplified English prompts
        if output_format == "natural_language":
            return self._build_natural_language_prompt(annotations, parsed_data, edit_description)
        
        # For other formats, use detailed Chinese prompts
        prompt_parts = []
        
        # 1. ç¼–è¾‘æ„å›¾æè¿°ï¼ˆæœ€é‡è¦çš„ä¿¡æ¯ï¼‰
        if edit_description and edit_description.strip():
            prompt_parts.append("=== ç¼–è¾‘æ„å›¾ ===\nç”¨æˆ·è¦æ±‚: " + edit_description.strip())
        
        # 2. å›¾åƒæ ‡æ³¨ä¿¡æ¯
        prompt_parts.append("\n=== å›¾åƒæ ‡æ³¨ä¿¡æ¯ ===")
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç¼–å·è®¾ç½®
        include_numbers = parsed_data.get("include_annotation_numbers", True)
        
        for i, annotation in enumerate(annotations):
            if include_numbers:
                # ä½¿ç”¨annotationä¸­çš„numberå­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ç´¢å¼•
                number = annotation.get('number', i+1)
                annotation_desc = f"æ ‡æ³¨ {number}:"
            else:
                annotation_desc = "æ ‡æ³¨:"
            
            annotation_desc += f" ç±»å‹={annotation.get('type', 'unknown')}"
            annotation_desc += f" é¢œè‰²={annotation.get('color', '#000000')}"
            
            if 'start' in annotation and 'end' in annotation:
                start = annotation['start']
                end = annotation['end']
                annotation_desc += f" åæ ‡=({start.get('x', 0)},{start.get('y', 0)})-({end.get('x', 0)},{end.get('y', 0)})"
            
            prompt_parts.append(annotation_desc)
        
        # 3. æ“ä½œä¿¡æ¯
        if 'operation_type' in parsed_data:
            prompt_parts.append(f"\n=== æ“ä½œç±»å‹ ===")
            prompt_parts.append(f"æ“ä½œ: {parsed_data['operation_type']}")
        
        if 'target_description' in parsed_data:
            prompt_parts.append(f"ç›®æ ‡æè¿°: {parsed_data['target_description']}")
        
        # 4. å¢å¼ºæç¤ºè¯
        if 'constraint_prompts' in parsed_data and parsed_data['constraint_prompts']:
            prompt_parts.append(f"\n=== çº¦æŸæ€§æç¤ºè¯ ===")
            constraints = parsed_data['constraint_prompts']
            if isinstance(constraints, list):
                prompt_parts.append(", ".join(constraints))
            else:
                prompt_parts.append(str(constraints))
        
        if 'decorative_prompts' in parsed_data and parsed_data['decorative_prompts']:
            prompt_parts.append(f"\n=== ä¿®é¥°æ€§æç¤ºè¯ ===")
            decoratives = parsed_data['decorative_prompts']
            if isinstance(decoratives, list):
                prompt_parts.append(", ".join(decoratives))
            else:
                prompt_parts.append(str(decoratives))
        
        # 5. å‚è€ƒä¸Šä¸‹æ–‡
        if reference_context:
            prompt_parts.append(f"\n=== å‚è€ƒä¸Šä¸‹æ–‡ ===")
            prompt_parts.append(reference_context)
        
        # 6. ç¼–è¾‘å‚æ•°
        prompt_parts.append(f"\n=== ç¼–è¾‘å‚æ•° ===")
        prompt_parts.append(f"ç¼–è¾‘å¼ºåº¦: {edit_intensity}")
        
        if preservation_mask:
            prompt_parts.append(f"ä¿æŠ¤åŒºåŸŸ: {preservation_mask}")
        
        if style_guidance:
            prompt_parts.append(f"é£æ ¼æŒ‡å¯¼: {style_guidance}")
        
        # 7. ç”Ÿæˆè¦æ±‚
        prompt_parts.append(f"\n=== ç”Ÿæˆè¦æ±‚ ===")
        prompt_parts.append("è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ç”Ÿæˆä¼˜åŒ–çš„Flux Kontextç¼–è¾‘æŒ‡ä»¤ã€‚")
        prompt_parts.append("ç¡®ä¿æŒ‡ä»¤ç²¾ç¡®ã€å¯æ‰§è¡Œï¼Œå¹¶ç¬¦åˆæŒ‡å®šçš„è¾“å‡ºæ ¼å¼ã€‚")
        prompt_parts.append("é‡ç‚¹æ ¹æ®ç¼–è¾‘æ„å›¾å’Œæ ‡æ³¨ä¿¡æ¯çš„ç»“åˆæ¥ç”ŸæˆæŒ‡ä»¤ã€‚")
        
        return "\n".join(prompt_parts)
    
    def _build_natural_language_prompt(self, annotations: List[Dict], parsed_data: Dict, edit_description: str = "") -> str:
        """æ„å»ºè‡ªç„¶è¯­è¨€æ ¼å¼çš„ç®€æ´æç¤ºè¯"""
        prompt_parts = []
        
        # 1. User editing intent (most important)
        if edit_description and edit_description.strip():
            prompt_parts.append(f"User request: {edit_description.strip()}")
        
        # 2. Simplified annotation information (without numbers)
        if annotations:
            prompt_parts.append("\nImage annotations:")
            for annotation in annotations:
                # Describe by color and type without annotation numbers
                color = annotation.get('color', '#000000')
                annotation_type = annotation.get('type', 'rectangle')
                
                # Convert color hex to color name if possible
                color_name = self._get_color_name(color)
                
                # Create spatial description
                if 'start' in annotation and 'end' in annotation:
                    start = annotation['start']
                    end = annotation['end']
                    width = abs(end.get('x', 0) - start.get('x', 0))
                    height = abs(end.get('y', 0) - start.get('y', 0))
                    
                    if width > height:
                        area_desc = f"{color_name} horizontal {annotation_type} area"
                    elif height > width:
                        area_desc = f"{color_name} vertical {annotation_type} area"
                    else:
                        area_desc = f"{color_name} {annotation_type} area"
                else:
                    area_desc = f"{color_name} {annotation_type} area"
                
                prompt_parts.append(f"- {area_desc}")
        
        # 3. Operation type (if available)
        if 'operation_type' in parsed_data:
            operation = parsed_data['operation_type']
            prompt_parts.append(f"\nOperation: {operation}")
        
        # 4. Target description (if available)
        if 'target_description' in parsed_data:
            prompt_parts.append(f"Target: {parsed_data['target_description']}")
        
        # 5. Generation instructions
        prompt_parts.append("\nGenerate a clean, natural language editing instruction.")
        prompt_parts.append("Focus on the core editing action using color and spatial descriptions.")
        prompt_parts.append("Do not include annotation numbers, technical details, or structured formatting.")
        
        return "\n".join(prompt_parts)
    
    def _get_color_name(self, hex_color: str) -> str:
        """Convert hex color to color name"""
        color_map = {
            '#ff0000': 'red', '#ff4444': 'red', '#cc0000': 'red',
            '#00ff00': 'green', '#44ff44': 'green', '#00cc00': 'green',
            '#0000ff': 'blue', '#4444ff': 'blue', '#0000cc': 'blue',
            '#ffff00': 'yellow', '#ffff44': 'yellow', '#cccc00': 'yellow',
            '#ff00ff': 'magenta', '#ff44ff': 'magenta', '#cc00cc': 'magenta',
            '#00ffff': 'cyan', '#44ffff': 'cyan', '#00cccc': 'cyan',
            '#ffa500': 'orange', '#ff8800': 'orange', '#cc6600': 'orange',
            '#800080': 'purple', '#9966cc': 'purple', '#663399': 'purple',
            '#000000': 'black', '#333333': 'dark gray', '#666666': 'gray',
            '#999999': 'light gray', '#cccccc': 'light gray', '#ffffff': 'white'
        }
        
        # Try exact match first
        if hex_color.lower() in color_map:
            return color_map[hex_color.lower()]
        
        # For other colors, try to guess based on RGB values
        try:
            if hex_color.startswith('#'):
                hex_color = hex_color[1:]
            if len(hex_color) == 6:
                r = int(hex_color[0:2], 16)
                g = int(hex_color[2:4], 16) 
                b = int(hex_color[4:6], 16)
                
                # Simple color detection
                if r > 200 and g < 100 and b < 100:
                    return 'red'
                elif r < 100 and g > 200 and b < 100:
                    return 'green'
                elif r < 100 and g < 100 and b > 200:
                    return 'blue'
                elif r > 200 and g > 200 and b < 100:
                    return 'yellow'
                elif r > 200 and g < 100 and b > 200:
                    return 'magenta'
                elif r < 100 and g > 200 and b > 200:
                    return 'cyan'
                elif r > 200 and g > 150 and b < 100:
                    return 'orange'
                elif r > 150 and g < 100 and b > 150:
                    return 'purple'
                elif r < 100 and g < 100 and b < 100:
                    return 'dark'
                elif r > 200 and g > 200 and b > 200:
                    return 'light'
                else:
                    return 'colored'
        except:
            pass
        
        return 'colored'
    
    def _generate_with_ollama(self, url: str, model: str, system_prompt: str,
                             user_prompt: str, temperature: float, top_p: float = 0.9,
                             keep_alive: int = 5, debug_mode: bool = False, 
                             image_base64: Optional[str] = None, seed: int = 42) -> Optional[str]:
        """ä½¿ç”¨Ollama HTTP APIç”Ÿæˆå¢å¼ºæŒ‡ä»¤"""
        try:
            import requests
            import json
            
            self._log_debug(f"ğŸ¤– è°ƒç”¨Ollamaæ¨¡å‹: {model} (HTTP API)", debug_mode)
            
            # é…ç½®ç”Ÿæˆå‚æ•° - ä¸ºäº†æé«˜é€Ÿåº¦ï¼Œé™åˆ¶æœ€å¤§temperature
            # ä½¿ç”¨seedæ§åˆ¶éšæœºæ€§
            adjusted_temperature = min(temperature, 0.7) if seed != 0 else temperature
            if seed != 0:
                # å¦‚æœæä¾›äº†seedï¼Œé€‚å½“é™ä½temperatureä»¥ç¡®ä¿æ›´ä¸€è‡´çš„ç»“æœ
                adjusted_temperature = min(adjusted_temperature, 0.5)
            
            options = {
                "temperature": adjusted_temperature,
                "top_p": min(top_p, 0.9),
                "seed": seed,  # æ·»åŠ seedå‚æ•°æ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§
            }
            
            # ä¸ºå°æ¨¡å‹æ·»åŠ é¢å¤–çš„é€Ÿåº¦ä¼˜åŒ–é€‰é¡¹
            if "1.7b" in model.lower() or "1.5b" in model.lower():
                options.update({
                    "num_predict": 500,  # é™åˆ¶è¾“å‡ºé•¿åº¦
                    "num_ctx": 2048,     # é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
                })
                print(f"Applying speed optimizations for small model: {model}")
            
            # å¯¹äºqwen3ç­‰æ”¯æŒthinkingæ¨¡å¼çš„æ¨¡å‹ï¼Œåœ¨system promptä¸­æ˜ç¡®è¦æ±‚ä¸è¦thinking
            if "qwen3" in model.lower() or "qwen" in model.lower():
                # åœ¨system promptä¸­æ˜ç¡®è¦æ±‚ä¸è¦thinkingï¼ˆä¸ä½¿ç”¨ä¸æ”¯æŒçš„é€‰é¡¹ï¼‰
                system_prompt += "\n\nIMPORTANT: Do not include any thinking process, reasoning steps, or <think> tags in your response. Output only the final formatted instructions."
            
            # æ„å»ºè¯·æ±‚æ•°æ®
            if image_base64:
                # å¯¹äºå¤šæ¨¡æ€æ¨¡å‹ï¼Œä½¿ç”¨chat APIæ ¼å¼
                payload = {
                    "model": model,
                    "messages": [
                        {
                            "role": "system",
                            "content": system_prompt
                        },
                        {
                            "role": "user",
                            "content": user_prompt,
                            "images": [image_base64]
                        }
                    ],
                    "options": options,
                    "keep_alive": f"{keep_alive}m",
                    "stream": False
                }
                api_endpoint = f"{url}/api/chat"
                self._log_debug("ğŸ–¼ï¸ ä½¿ç”¨å¤šæ¨¡æ€Chat API", debug_mode)
            else:
                # å¯¹äºçº¯æ–‡æœ¬æ¨¡å‹ï¼Œä½¿ç”¨ä¼ ç»Ÿçš„generate API
                payload = {
                    "model": model,
                    "prompt": user_prompt,
                    "system": system_prompt,
                    "options": options,
                    "keep_alive": f"{keep_alive}m"
                }
                api_endpoint = f"{url}/api/generate"
                self._log_debug("ğŸ“ ä½¿ç”¨çº¯æ–‡æœ¬Generate API", debug_mode)
            
            # å‘é€è¯·æ±‚åˆ°Ollama HTTP API
            print(f"Sending request to Ollama API: {api_endpoint}")
            print(f"Using model: {model}")
            if "1.7b" in model.lower() or "1.5b" in model.lower():
                print("Note: Small models may take longer to process complex prompts...")
            else:
                print("Note: This may take a while for complex prompts...")
            
            try:
                response = requests.post(
                    api_endpoint,
                    json=payload,
                    timeout=300  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°5åˆ†é’Ÿ
                )
            except requests.exceptions.Timeout:
                print("Request timed out. Trying with a shorter, simplified prompt...")
                # å°è¯•ä½¿ç”¨ç®€åŒ–çš„prompté‡æ–°ç”Ÿæˆ
                return self._generate_with_simplified_prompt(url, model, system_prompt, user_prompt, options, api_endpoint, debug_mode, seed)
            
            print(f"Ollama API response status: {response.status_code}")
            if response.status_code == 200:
                # å…ˆæ£€æŸ¥å“åº”å†…å®¹
                response_text = response.text
                print(f"Raw response length: {len(response_text)}")
                print(f"Response preview: {response_text[:200]}...")
                
                try:
                    # å¤„ç†å¯èƒ½çš„æµå¼å“åº”
                    if '\n' in response_text:
                        # å¯èƒ½æ˜¯NDJSONæ ¼å¼ï¼ˆæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼‰
                        lines = response_text.strip().split('\n')
                        print(f"Found {len(lines)} lines in response")
                        
                        # å¯¹äºNDJSONæµå¼å“åº”ï¼Œéœ€è¦æ”¶é›†æ‰€æœ‰chunksçš„å†…å®¹
                        collected_response = ""
                        final_result = None
                        
                        for line in lines:
                            line = line.strip()
                            if not line:
                                continue
                            
                            try:
                                chunk = json.loads(line)
                                
                                # æ”¶é›†responseå†…å®¹
                                if 'response' in chunk and chunk['response']:
                                    collected_response += chunk['response']
                                
                                # ä¿å­˜æœ€åä¸€ä¸ªæœ‰æ•ˆçš„ç»“æœå¯¹è±¡ï¼ˆç”¨äºè·å–å…ƒæ•°æ®ï¼‰
                                if chunk:
                                    final_result = chunk
                                    
                            except json.JSONDecodeError as e:
                                print(f"Warning: Failed to parse line: {line[:100]}... Error: {e}")
                                continue
                        
                        # å¦‚æœæ”¶é›†åˆ°äº†å“åº”å†…å®¹ï¼Œåˆ›å»ºå®Œæ•´çš„ç»“æœå¯¹è±¡
                        if collected_response:
                            result = final_result or {}
                            result['response'] = collected_response
                            print(f"Successfully collected streamed response, total length: {len(collected_response)}")
                        else:
                            # å¦‚æœæ²¡æœ‰æ”¶é›†åˆ°å†…å®¹ï¼Œä½¿ç”¨æœ€åä¸€ä¸ªJSONå¯¹è±¡
                            result = final_result
                            if not result:
                                print("Error: No valid JSON found in response lines")
                                return None
                    else:
                        # å•è¡ŒJSONå“åº”
                        result = json.loads(response_text)
                    
                    print(f"Parsed JSON successfully, result keys: {list(result.keys()) if result else 'None'}")
                    self._log_debug(f"ğŸ” Ollama APIå“åº”: {str(result)[:200]}...", debug_mode)
                except json.JSONDecodeError as e:
                    print(f"JSON parsing error: {e}")
                    print(f"Problematic response text: {response_text}")
                    return None
                
                generated_text = None
                
                # å¤„ç†ä¸åŒçš„APIå“åº”æ ¼å¼
                if image_base64:
                    # Chat APIå“åº”æ ¼å¼
                    if result and 'message' in result and 'content' in result['message']:
                        generated_text = result['message']['content'].strip()
                        self._log_debug("ğŸ–¼ï¸ è§£æChat APIå“åº”æˆåŠŸ", debug_mode)
                    else:
                        self._log_debug(f"âŒ Chat APIå“åº”æ ¼å¼é”™è¯¯: {result}", debug_mode)
                        return None
                else:
                    # Generate APIå“åº”æ ¼å¼
                    if result and 'response' in result:
                        generated_text = result['response'].strip()
                        print(f"Generated text length: {len(generated_text)}")
                        self._log_debug("ğŸ“ è§£æGenerate APIå“åº”æˆåŠŸ", debug_mode)
                    else:
                        print(f"Error: Generate API response missing 'response' field. Available fields: {list(result.keys()) if result else 'None'}")
                        self._log_debug(f"âŒ Generate APIå“åº”ç¼ºå°‘'response'å­—æ®µ: {result}", debug_mode)
                        return None
                
                if generated_text:
                    # è¿‡æ»¤æ‰qwen3ç­‰æ¨¡å‹çš„thinkingå†…å®¹
                    filtered_text = self._filter_thinking_content(generated_text, debug_mode)
                    
                    print(f"Success: Generated text original length: {len(generated_text)}, filtered length: {len(filtered_text)}")
                    self._log_debug(f"âœ… Ollamaç”ŸæˆæˆåŠŸï¼ŒåŸå§‹é•¿åº¦: {len(generated_text)}, è¿‡æ»¤åé•¿åº¦: {len(filtered_text)} å­—ç¬¦", debug_mode)
                    return filtered_text
                else:
                    print("Error: Generated text is empty after parsing")
                    return None
            else:
                error_msg = f"Ollama API request failed - Status: {response.status_code}, Response: {response.text[:200]}"
                print(f"Error: {error_msg}")
                self._log_debug(f"âŒ {error_msg}", debug_mode)
                return None
                
        except Exception as e:
            error_msg = f"Ollama generation exception: {str(e)}"
            print(f"Error: {error_msg}")
            self._log_debug(f"âŒ {error_msg}", debug_mode)
            return None
    
    def _generate_with_simplified_prompt(self, url: str, model: str, system_prompt: str, 
                                       user_prompt: str, options: dict, api_endpoint: str, 
                                       debug_mode: bool, seed: int = 42) -> Optional[str]:
        """ä½¿ç”¨ç®€åŒ–çš„prompté‡æ–°å°è¯•ç”Ÿæˆ"""
        try:
            import requests
            
            # ç®€åŒ–system prompt
            simplified_system = "You are an AI assistant that creates image editing instructions. Be concise and direct."
            
            # ç®€åŒ–user prompt - åªä¿ç•™æ ¸å¿ƒå†…å®¹
            user_lines = user_prompt.split('\n')
            simplified_user = '\n'.join(user_lines[:10])  # åªä¿ç•™å‰10è¡Œ
            if len(user_lines) > 10:
                simplified_user += "\n[Content truncated for faster processing]"
            
            print("Trying with simplified prompt due to timeout...")
            
            # æ„å»ºç®€åŒ–çš„payload
            if "chat" in api_endpoint:
                payload = {
                    "model": model,
                    "messages": [
                        {"role": "system", "content": simplified_system},
                        {"role": "user", "content": simplified_user}
                    ],
                    "options": options,
                    "keep_alive": "5m"
                }
            else:
                payload = {
                    "model": model,
                    "prompt": simplified_user,
                    "system": simplified_system,
                    "options": options,
                    "keep_alive": "5m"
                }
            
            # ä½¿ç”¨æ›´çŸ­çš„è¶…æ—¶æ—¶é—´
            response = requests.post(api_endpoint, json=payload, timeout=60)
            
            if response.status_code == 200:
                result = response.json()
                
                # å¤„ç†å“åº”
                if "chat" in api_endpoint:
                    if result and 'message' in result and 'content' in result['message']:
                        generated_text = result['message']['content'].strip()
                    else:
                        return None
                else:
                    if result and 'response' in result:
                        generated_text = result['response'].strip()
                    else:
                        return None
                
                if generated_text:
                    filtered_text = self._filter_thinking_content(generated_text, debug_mode)
                    print("Simplified prompt generation successful")
                    return filtered_text
                
            return None
            
        except Exception as e:
            print(f"Simplified prompt generation also failed: {e}")
            return None
    
    def _filter_thinking_content(self, text: str, debug_mode: bool) -> str:
        """è¿‡æ»¤æ‰æ¨¡å‹çš„thinkingå†…å®¹ (å¦‚qwen3çš„<think>æ ‡ç­¾)"""
        try:
            import re
            
            # è¿‡æ»¤å¸¸è§çš„thinkingæ ‡ç­¾æ ¼å¼
            thinking_patterns = [
                r'<think>.*?</think>',  # qwen3çš„<think>æ ‡ç­¾
                r'<thinking>.*?</thinking>',  # å…¶ä»–å¯èƒ½çš„thinkingæ ‡ç­¾
                r'<thought>.*?</thought>',  # thoughtæ ‡ç­¾
                r'æ€è€ƒ[:ï¼š].*?(?=\n|$)',  # ä¸­æ–‡"æ€è€ƒ:"å¼€å¤´çš„è¡Œ
                r'Let me think.*?(?=\n|$)',  # è‹±æ–‡thinkingå¼€å¤´
                r'I need to think.*?(?=\n|$)',  # å…¶ä»–thinkingè¡¨è¾¾
            ]
            
            filtered_text = text
            original_length = len(text)
            
            # åº”ç”¨æ‰€æœ‰è¿‡æ»¤è§„åˆ™
            for pattern in thinking_patterns:
                filtered_text = re.sub(pattern, '', filtered_text, flags=re.DOTALL | re.IGNORECASE)
            
            # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
            filtered_text = re.sub(r'\n\s*\n\s*\n', '\n\n', filtered_text)  # å¤šä¸ªç©ºè¡Œå˜æˆä¸¤ä¸ª
            filtered_text = filtered_text.strip()
            
            # å¦‚æœè¿‡æ»¤æ‰äº†å†…å®¹ï¼Œè®°å½•æ—¥å¿—
            if len(filtered_text) < original_length:
                self._log_debug(f"ğŸ§¹ è¿‡æ»¤æ‰thinkingå†…å®¹ï¼Œå‡å°‘äº† {original_length - len(filtered_text)} å­—ç¬¦", debug_mode)
            
            return filtered_text
            
        except Exception as e:
            self._log_debug(f"âš ï¸ thinkingå†…å®¹è¿‡æ»¤å¤±è´¥: {e}ï¼Œè¿”å›åŸå§‹å†…å®¹", debug_mode)
            return text
    
    def _format_flux_instructions(self, instructions: str, output_format: str, debug_mode: bool) -> str:
        """æ ¼å¼åŒ–FluxæŒ‡ä»¤"""
        try:
            if output_format == "flux_kontext_standard":
                # å¦‚æœå·²ç»æ˜¯æ ‡å‡†æ ¼å¼ï¼Œç›´æ¥è¿”å›
                if "[EDIT_OPERATIONS]" in instructions:
                    return instructions
                else:
                    # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                    formatted = f"""[EDIT_OPERATIONS]
operation_1: {instructions}

[SPATIAL_CONSTRAINTS]
preserve_regions: ["original_composition"]
blend_boundaries: "seamless"

[QUALITY_CONTROLS]
detail_level: "high"
consistency: "maintain_original_quality"
realism: "photorealistic"
"""
                    return formatted
            elif output_format == "structured_json":
                # è½¬æ¢ä¸ºJSONæ ¼å¼
                try:
                    json_output = {
                        "operations": [{"description": instructions}],
                        "constraints": ["preserve_original_composition"],
                        "quality": {"detail_level": "high", "consistency": "high"}
                    }
                    return json.dumps(json_output, indent=2, ensure_ascii=False)
                except:
                    return instructions
            elif output_format == "natural_language":
                # For natural language, clean up the output
                return self._clean_natural_language_output(instructions)
            else:
                # å…¶ä»–æ ¼å¼ç›´æ¥è¿”å›
                return instructions
                
        except Exception as e:
            self._log_debug(f"âš ï¸ æ ¼å¼åŒ–å¤±è´¥ï¼Œè¿”å›åŸå§‹å†…å®¹: {e}", debug_mode)
            return instructions
    
    def _clean_natural_language_output(self, instructions: str) -> str:
        """Clean natural language output to remove technical details and annotation numbers"""
        try:
            # Remove annotation numbers like "(annotation 0)", "(annotation 1)", etc.
            import re
            
            # Remove annotation references
            instructions = re.sub(r'\(annotation\s+\d+\)', '', instructions, flags=re.IGNORECASE)
            instructions = re.sub(r'annotation\s+\d+:?', '', instructions, flags=re.IGNORECASE)
            
            # Remove technical instruction sections
            lines = instructions.split('\n')
            cleaned_lines = []
            skip_section = False
            
            for line in lines:
                line = line.strip()
                
                # Skip technical instruction sections
                if line.startswith('**Instruction:**') or line.startswith('**Instructions:**'):
                    skip_section = True
                    continue
                elif line.startswith('**') and skip_section:
                    # End of instruction section
                    skip_section = False
                    continue
                elif skip_section and (line.startswith('-') or line.startswith('*') or 'Apply' in line or 'Ensure' in line or 'Maintain' in line):
                    # Skip technical instruction items
                    continue
                elif skip_section and not line:
                    # Skip empty lines in instruction sections
                    continue
                else:
                    skip_section = False
                
                # Keep non-technical content
                if line and not skip_section:
                    # Additional cleanup
                    if not (line.startswith('- Apply') or line.startswith('- Ensure') or line.startswith('- Maintain')):
                        cleaned_lines.append(line)
            
            # Join and clean up spacing
            result = ' '.join(cleaned_lines)
            
            # Remove extra spaces and clean up
            result = re.sub(r'\s+', ' ', result).strip()
            
            # Remove remaining technical patterns
            result = re.sub(r'global_color_grade|intensify.*hue|transition.*smoothly', '', result, flags=re.IGNORECASE)
            
            return result if result else instructions
            
        except Exception as e:
            # If cleaning fails, return original
            return instructions
    
    def _generate_spatial_mappings(self, annotations: List[Dict], debug_mode: bool, include_numbers: bool = True) -> str:
        """ç”Ÿæˆç©ºé—´æ˜ å°„ä¿¡æ¯"""
        try:
            mappings = {
                "regions": [],
                "coordinate_system": "absolute_pixels",
                "total_annotations": len(annotations)
            }
            
            for i, annotation in enumerate(annotations):
                region = {
                    "id": annotation.get("id", f"annotation_{i+1}"),
                    "type": annotation.get("type", "unknown"),
                    "color_code": annotation.get("color", "#000000")
                }
                
                # åªåœ¨éœ€è¦æ—¶åŒ…å«ç¼–å·
                if include_numbers:
                    region["number"] = annotation.get("number", i+1)
                
                # æ·»åŠ åæ ‡ä¿¡æ¯
                if 'start' in annotation and 'end' in annotation:
                    start = annotation['start']
                    end = annotation['end']
                    region["coordinates"] = [
                        start.get('x', 0), start.get('y', 0),
                        end.get('x', 0), end.get('y', 0)
                    ]
                elif 'points' in annotation:
                    region["points"] = annotation['points']
                
                mappings["regions"].append(region)
            
            return json.dumps(mappings, indent=2, ensure_ascii=False)
            
        except Exception as e:
            self._log_debug(f"âš ï¸ ç©ºé—´æ˜ å°„ç”Ÿæˆå¤±è´¥: {e}", debug_mode)
            return f'{{"error": "Failed to generate spatial mappings: {str(e)}"}}'
    
    def _generate_processing_metadata(self, model: str, strategy: str, 
                                    annotation_count: int, debug_mode: bool) -> str:
        """ç”Ÿæˆå¤„ç†å…ƒæ•°æ®"""
        try:
            processing_time = time.time() - self.start_time if self.start_time else 0
            
            metadata = {
                "processing_time": f"{processing_time:.2f}s",
                "timestamp": datetime.now().isoformat(),
                "ollama_model_used": model,
                "edit_strategy": strategy,
                "annotations_processed": annotation_count,
                "enhancement_applied": True,
                "status": "success"
            }
            
            if debug_mode and self.debug_logs:
                metadata["debug_logs"] = self.debug_logs
            
            return json.dumps(metadata, indent=2, ensure_ascii=False)
            
        except Exception as e:
            return f'{{"error": "Failed to generate metadata: {str(e)}"}}'
    
    def _log_debug(self, message: str, debug_mode: bool):
        """è®°å½•è°ƒè¯•ä¿¡æ¯"""
        if debug_mode:
            timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
            log_message = f"[{timestamp}] {message}"
            self.debug_logs.append(log_message)
            print(log_message)  # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
    
    def _create_fallback_output(self, error_msg: str, debug_mode: bool) -> Tuple[str, str]:
        """åˆ›å»ºå¤±è´¥æ—¶çš„å›é€€è¾“å‡º"""
        self._log_debug(f"âŒ åˆ›å»ºå›é€€è¾“å‡º: {error_msg}", debug_mode)
        
        fallback_instructions = f"""[EDIT_OPERATIONS]
operation_1: Apply standard edit to marked regions
# Error: {error_msg}

[SPATIAL_CONSTRAINTS]
preserve_regions: ["all_unmarked_areas"]
blend_boundaries: "seamless"

[QUALITY_CONTROLS]
detail_level: "standard"
consistency: "maintain_original"
"""
        
        fallback_system_prompt = f"Error occurred during processing: {error_msg}"
        
        return (fallback_instructions, fallback_system_prompt)


# æ·»åŠ APIç«¯ç‚¹ç”¨äºåŠ¨æ€è·å–æ¨¡å‹
if WEB_AVAILABLE:
    @PromptServer.instance.routes.post("/ollama_flux_enhancer/get_models")
    async def get_models_endpoint(request):
        """è·å–å¯ç”¨çš„Ollamaæ¨¡å‹åˆ—è¡¨ - äº‘ç«¯ç¯å¢ƒå…¼å®¹ç‰ˆæœ¬"""
        try:
            data = await request.json()
            url = data.get("url", "http://127.0.0.1:11434")
            
            print(f"ğŸ”„ API endpoint: Starting to fetch Ollama model list")
            print(f"ğŸ“¡ API endpoint: Request URL: {url}")
            print(f"ğŸŒ API endpoint: Client source: {request.remote}")
            
            # Special handling for cloud environments: if localhost, may need different address
            if "127.0.0.1" in url or "localhost" in url:
                print("âš ï¸ API endpoint: Detected localhost address, may not be accessible in cloud environments")
                print("ğŸ’¡ API endpoint: Recommend checking Ollama service configuration and network connection")
            
            # Use exactly the same model detection logic as main node
            print("ğŸ” API endpoint: Calling get_available_models method")
            model_names = OllamaFluxKontextEnhancerV2.get_available_models(url=url, force_refresh=True)
            
            print(f"âœ… API endpoint: Detection complete, found {len(model_names)} models")
            if model_names:
                print(f"ğŸ“‹ API endpoint: Model list: {model_names}")
            else:
                print("âŒ API endpoint: No models detected")
                print("ğŸ”§ API endpoint: Possible reasons:")
                print("   1. Ollama service not running")
                print("   2. Network connection issues (common in cloud environments)")
                print("   3. URL configuration error")
                print("   4. Firewall blocking")
            
            return web.json_response(model_names)
            
        except Exception as e:
            print(f"âŒ API endpoint critical error: {e}")
            import traceback
            error_details = traceback.format_exc()
            print(f"ğŸ” API endpoint error details:\n{error_details}")
            
            # è¿”å›é”™è¯¯ä¿¡æ¯ç»™å‰ç«¯
            return web.json_response({
                "error": str(e),
                "details": error_details,
                "models": []
            }, status=500)


# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "OllamaFluxKontextEnhancerV2": OllamaFluxKontextEnhancerV2,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "OllamaFluxKontextEnhancerV2": "ğŸ¤– Ollama Flux Kontext Enhancer V2",
}