"""
OllamaFluxKontextEnhancer Node
Ollamaé›†æˆçš„Flux Kontextæç¤ºè¯å¢å¼ºèŠ‚ç‚¹

å°†VisualPromptEditorçš„æ ‡æ³¨æ•°æ®é€šè¿‡æœ¬åœ°Ollamaæ¨¡å‹è½¬æ¢ä¸º
Flux Kontextä¼˜åŒ–çš„ç»“æ„åŒ–ç¼–è¾‘æŒ‡ä»¤
"""

import json
import time
import traceback
from typing import Dict, List, Any, Tuple, Optional
from datetime import datetime

try:
    from ollama import Client
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False
    print("Warning: ollama package not found. Please install with: pip install ollama")

try:
    from aiohttp import web
    from server import PromptServer
    WEB_AVAILABLE = True
except ImportError:
    WEB_AVAILABLE = False


class OllamaFluxKontextEnhancer:
    """
    ğŸ¤– Ollama Flux Kontext Enhancer
    
    é€šè¿‡æœ¬åœ°Ollamaæ¨¡å‹å°†VisualPromptEditorçš„æ ‡æ³¨æ•°æ®
    è½¬æ¢ä¸ºFlux Kontextä¼˜åŒ–çš„ç»“æ„åŒ–ç¼–è¾‘æŒ‡ä»¤
    """
    
    @classmethod
    def get_available_models(cls, url="http://127.0.0.1:11434"):
        """åŠ¨æ€è·å–å¯ç”¨çš„Ollamaæ¨¡å‹åˆ—è¡¨"""
        try:
            if not OLLAMA_AVAILABLE:
                return ["qwen:0.5b"]  # é»˜è®¤å›é€€æ¨¡å‹
            
            from ollama import Client
            client = Client(host=url)
            models_response = client.list()
            models = models_response.get('models', [])
            
            # æå–æ¨¡å‹åç§°
            model_names = []
            for model in models:
                if isinstance(model, dict):
                    name = model.get('model') or model.get('name')
                    if name:
                        model_names.append(name)
                elif hasattr(model, 'model'):
                    # å¤„ç†å¯¹è±¡ç±»å‹çš„æ¨¡å‹
                    model_names.append(model.model)
                else:
                    # å°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶æå–æ¨¡å‹å
                    model_str = str(model)
                    if "model='" in model_str:
                        # ä»å­—ç¬¦ä¸²ä¸­æå–æ¨¡å‹å ä¾‹: "model='qwen3:0.6b'"
                        start = model_str.find("model='") + 7
                        end = model_str.find("'", start)
                        if end > start:
                            model_names.append(model_str[start:end])
                        else:
                            # å¦‚æœæå–å¤±è´¥ï¼Œè·³è¿‡è¿™ä¸ªæ¨¡å‹
                            print(f"Warning: Failed to extract model name from: {model_str[:100]}...")
                    else:
                        # å¦‚æœæ ¼å¼ä¸åŒ¹é…ï¼Œè·³è¿‡è¿™ä¸ªæ¨¡å‹
                        print(f"Warning: Unknown model format: {model_str[:100]}...")
            
            # å¦‚æœæ²¡æœ‰æ¨¡å‹ï¼Œè¿”å›é»˜è®¤æ¨¡å‹
            if not model_names:
                return ["qwen:0.5b"]
            
            return model_names
        except Exception as e:
            print(f"Warning: Failed to get Ollama models: {e}")
            # è¿”å›å¸¸è§çš„é»˜è®¤æ¨¡å‹åˆ—è¡¨ä½œä¸ºå›é€€
            return ["qwen3:0.6b", "qwen2.5:0.5b", "qwen:0.5b", "llama3.2:3b", "llama3.1:8b"]

    @classmethod
    def INPUT_TYPES(cls):
        # åŠ¨æ€è·å–æ¨¡å‹åˆ—è¡¨
        available_models = cls.get_available_models()
        default_model = available_models[0] if available_models else "qwen:0.5b"
        
        return {
            "required": {
                "annotation_data": ("STRING", {
                    "multiline": True, 
                    "default": "",
                    "tooltip": "æ¥è‡ªVisualPromptEditorçš„æ ‡æ³¨JSONæ•°æ®"
                }),
                "model": (available_models, {
                    "default": default_model,
                    "tooltip": "é€‰æ‹©Ollamaæ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆ (è‡ªåŠ¨ä»OllamaæœåŠ¡è·å–)"
                }),  # åŠ¨æ€å¡«å……æ¨¡å‹åˆ—è¡¨
                "edit_instruction_type": ([
                    "auto_detect",          # ğŸ”„ è‡ªåŠ¨æ ¹æ®æ“ä½œç±»å‹é€‰æ‹©æœ€ä½³ç­–ç•¥
                    "spatial_precise",      # ç©ºé—´ç²¾å‡†ç¼–è¾‘
                    "semantic_enhanced",    # è¯­ä¹‰å¢å¼ºç¼–è¾‘  
                    "style_coherent",       # é£æ ¼ä¸€è‡´æ€§ç¼–è¾‘
                    "content_aware",        # å†…å®¹æ„ŸçŸ¥ç¼–è¾‘
                    "multi_region",         # å¤šåŒºåŸŸåè°ƒç¼–è¾‘
                    "custom"                # è‡ªå®šä¹‰æŒ‡ä»¤
                ], {
                    "default": "auto_detect",
                    "tooltip": "é€‰æ‹©ç¼–è¾‘æŒ‡ä»¤çš„ç”Ÿæˆç­–ç•¥ (auto_detectæ ¹æ®æ“ä½œç±»å‹è‡ªåŠ¨é€‰æ‹©)"
                }),
                "output_format": ([
                    "flux_kontext_standard",  # Flux Kontextæ ‡å‡†æ ¼å¼
                    "structured_json",        # ç»“æ„åŒ–JSON
                    "natural_language",       # è‡ªç„¶è¯­è¨€æè¿°
                    "prompt_engineering"      # æç¤ºå·¥ç¨‹ä¼˜åŒ–
                ], {
                    "default": "flux_kontext_standard",
                    "tooltip": "é€‰æ‹©è¾“å‡ºçš„æç¤ºè¯æ ¼å¼"
                }),
            },
            "optional": {
                "reference_context": ("STRING", {
                    "multiline": True,
                    "default": "",
                    "tooltip": "å‚è€ƒä¸Šä¸‹æ–‡ï¼Œç”¨äºå¢å¼ºç¼–è¾‘æŒ‡ä»¤çš„è¿è´¯æ€§"
                }),
                "edit_intensity": ("FLOAT", {
                    "default": 0.8,
                    "min": 0.1,
                    "max": 2.0,
                    "step": 0.1,
                    "tooltip": "ç¼–è¾‘å¼ºåº¦ (0.1=è½»å¾®, 1.0=æ ‡å‡†, 2.0=å¼ºçƒˆ)"
                }),
                "preservation_mask": ("STRING", {
                    "default": "",
                    "tooltip": "éœ€è¦ä¿æŠ¤ä¸å˜çš„åŒºåŸŸæè¿°"
                }),
                "style_guidance": ("STRING", {
                    "default": "",
                    "tooltip": "é£æ ¼å¼•å¯¼å‚æ•°"
                }),
                "url": ("STRING", {
                    "default": "http://127.0.0.1:11434",
                    "tooltip": "OllamaæœåŠ¡åœ°å€"
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.1,
                    "max": 1.0,
                    "step": 0.1,
                    "tooltip": "ç”Ÿæˆæ¸©åº¦ (åˆ›æ„æ€§æ§åˆ¶)"
                }),
                "top_p": ("FLOAT", {
                    "default": 0.9,
                    "min": 0.1,
                    "max": 1.0,
                    "step": 0.1,
                    "tooltip": "æ ¸å¿ƒé‡‡æ ·å‚æ•°"
                }),
                "keep_alive": ("INT", {
                    "default": 5,
                    "min": -1,
                    "max": 60,
                    "tooltip": "æ¨¡å‹ä¿æŒæ´»è·ƒæ—¶é—´(åˆ†é’Ÿ)"
                }),
                "debug_mode": ("BOOLEAN", {
                    "default": False,
                    "tooltip": "å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œè¾“å‡ºè¯¦ç»†å¤„ç†æ—¥å¿—"
                }),
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING", "STRING")
    RETURN_NAMES = (
        "flux_edit_instructions",  # Flux Kontextæ ¼å¼çš„ç¼–è¾‘æŒ‡ä»¤
        "spatial_mappings",        # ç©ºé—´æ˜ å°„ä¿¡æ¯
        "processing_metadata"      # å¤„ç†å…ƒæ•°æ®å’Œè°ƒè¯•ä¿¡æ¯
    )
    
    FUNCTION = "enhance_flux_instructions"
    CATEGORY = "kontext/ai_enhanced"
    DESCRIPTION = "ğŸ¤– é€šè¿‡Ollamaå¢å¼ºVisualPromptEditorçš„æ ‡æ³¨æ•°æ®ï¼Œç”ŸæˆFlux Kontextä¼˜åŒ–çš„ç»“æ„åŒ–ç¼–è¾‘æŒ‡ä»¤"
    
    def __init__(self):
        self.start_time = None
        self.debug_logs = []
    
    def enhance_flux_instructions(self, annotation_data: str, model: str, 
                                edit_instruction_type: str, output_format: str,
                                reference_context: str = "", edit_intensity: float = 0.8,
                                preservation_mask: str = "", style_guidance: str = "",
                                url: str = "http://127.0.0.1:11434", temperature: float = 0.7,
                                top_p: float = 0.9, keep_alive: int = 5,
                                debug_mode: bool = False):
        """é€šè¿‡Ollamaå¢å¼ºæ ‡æ³¨æ•°æ®ï¼Œç”ŸæˆFlux Kontextä¼˜åŒ–çš„ç¼–è¾‘æŒ‡ä»¤"""
        
        self.start_time = time.time()
        self.debug_logs = []
        
        try:
            # æ£€æŸ¥Ollamaå¯ç”¨æ€§
            if not OLLAMA_AVAILABLE:
                return self._create_fallback_output(
                    "Ollama package not available. Please install with: pip install ollama",
                    debug_mode
                )
            
            # è‡ªåŠ¨æ£€æµ‹æœ€ä½³ç¼–è¾‘ç­–ç•¥
            if edit_instruction_type == "auto_detect":
                edit_instruction_type = self._auto_detect_strategy(annotation_data, debug_mode)
                self._log_debug(f"ğŸ”„ è‡ªåŠ¨æ£€æµ‹åˆ°æœ€ä½³ç­–ç•¥: {edit_instruction_type}", debug_mode)
            
            self._log_debug(f"ğŸš€ å¼€å§‹å¤„ç† - æ¨¡å‹: {model}, ç­–ç•¥: {edit_instruction_type}", debug_mode)
            
            # 1. è§£ææ ‡æ³¨æ•°æ®
            annotations, parsed_data = self._parse_annotation_data(annotation_data, debug_mode)
            if not annotations:
                return self._create_fallback_output(
                    "No valid annotations found in annotation_data",
                    debug_mode
                )
            
            # 2. è¿æ¥OllamaæœåŠ¡
            client = self._connect_ollama(url, debug_mode)
            if not client:
                return self._create_fallback_output(
                    f"Failed to connect to Ollama at {url}",
                    debug_mode
                )
            
            # 3. æ„å»ºæç¤ºè¯
            system_prompt = self._build_system_prompt(edit_instruction_type, output_format)
            user_prompt = self._build_user_prompt(
                annotations, parsed_data, reference_context, 
                edit_intensity, preservation_mask, style_guidance
            )
            
            self._log_debug(f"ğŸ“ ç”Ÿæˆçš„ç”¨æˆ·æç¤ºè¯é•¿åº¦: {len(user_prompt)} å­—ç¬¦", debug_mode)
            
            # 4. è°ƒç”¨Ollamaç”Ÿæˆå¢å¼ºæŒ‡ä»¤
            enhanced_instructions = self._generate_with_ollama(
                client, model, system_prompt, user_prompt,
                temperature, top_p, keep_alive, debug_mode
            )
            
            if not enhanced_instructions:
                return self._create_fallback_output(
                    "Failed to generate enhanced instructions from Ollama",
                    debug_mode
                )
            
            # 5. æ ¼å¼åŒ–è¾“å‡º
            flux_instructions = self._format_flux_instructions(
                enhanced_instructions, output_format, debug_mode
            )
            
            # 6. ç”Ÿæˆç©ºé—´æ˜ å°„
            spatial_mappings = self._generate_spatial_mappings(annotations, debug_mode)
            
            # 7. ç”Ÿæˆå¤„ç†å…ƒæ•°æ®
            processing_metadata = self._generate_processing_metadata(
                model, edit_instruction_type, len(annotations), debug_mode
            )
            
            self._log_debug("âœ… å¤„ç†å®Œæˆ", debug_mode)
            
            return (flux_instructions, spatial_mappings, processing_metadata)
            
        except Exception as e:
            error_msg = f"Error in enhance_flux_instructions: {str(e)}"
            if debug_mode:
                error_msg += f"\n{traceback.format_exc()}"
            return self._create_fallback_output(error_msg, debug_mode)
    
    def _auto_detect_strategy(self, annotation_data: str, debug_mode: bool) -> str:
        """æ ¹æ®annotationæ•°æ®è‡ªåŠ¨æ£€æµ‹æœ€ä½³ç¼–è¾‘ç­–ç•¥"""
        try:
            if not annotation_data or not annotation_data.strip():
                return "spatial_precise"  # é»˜è®¤ç­–ç•¥
            
            parsed_data = json.loads(annotation_data)
            operation_type = parsed_data.get('operation_type', '')
            
            # Visual Prompt Editoræ“ä½œç±»å‹åˆ°ç¼–è¾‘ç­–ç•¥çš„æ˜ å°„
            operation_to_strategy = {
                # ç©ºé—´ç²¾å‡†ç±»æ“ä½œ
                "change_color": "spatial_precise",
                "replace_object": "spatial_precise", 
                "resize_object": "spatial_precise",
                "geometric_warp": "spatial_precise",
                "perspective_transform": "spatial_precise",
                "precision_cutout": "spatial_precise",
                
                # è¯­ä¹‰å¢å¼ºç±»æ“ä½œ
                "add_object": "semantic_enhanced",
                "remove_object": "semantic_enhanced",
                "change_expression": "semantic_enhanced",
                "character_expression": "semantic_enhanced",
                "content_aware_fill": "semantic_enhanced",
                "seamless_removal": "semantic_enhanced",
                
                # é£æ ¼ä¸€è‡´æ€§ç±»æ“ä½œ
                "change_style": "style_coherent",
                "change_texture": "style_coherent",
                "global_style_transfer": "style_coherent",
                "style_blending": "style_coherent",
                "global_color_grade": "style_coherent",
                
                # å†…å®¹æ„ŸçŸ¥ç±»æ“ä½œ
                "enhance_quality": "content_aware",
                "change_pose": "content_aware",
                "change_clothing": "content_aware",
                "enhance_skin_texture": "content_aware",
                "detail_enhance": "content_aware",
                "realism_enhance": "content_aware",
                
                # å¤šåŒºåŸŸåè°ƒç±»æ“ä½œï¼ˆæ£€æµ‹å¤šä¸ªæ ‡æ³¨ï¼‰
                "global_enhance": "multi_region",
                "global_filter": "multi_region",
                "collage_integration": "multi_region",
                "texture_mixing": "multi_region"
            }
            
            # æ ¹æ®æ“ä½œç±»å‹é€‰æ‹©ç­–ç•¥
            detected_strategy = operation_to_strategy.get(operation_type, "spatial_precise")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¤šä¸ªæ ‡æ³¨åŒºåŸŸ
            annotations = parsed_data.get('annotations', [])
            if len(annotations) > 2:
                detected_strategy = "multi_region"
            
            self._log_debug(f"ğŸ¯ æ“ä½œç±»å‹: {operation_type} â†’ ç­–ç•¥: {detected_strategy}", debug_mode)
            return detected_strategy
            
        except Exception as e:
            self._log_debug(f"âš ï¸ ç­–ç•¥è‡ªåŠ¨æ£€æµ‹å¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤ç­–ç•¥", debug_mode)
            return "spatial_precise"
    
    def _parse_annotation_data(self, annotation_data: str, debug_mode: bool) -> Tuple[List[Dict], Dict]:
        """è§£ææ ‡æ³¨æ•°æ®"""
        try:
            if not annotation_data or not annotation_data.strip():
                self._log_debug("âš ï¸ æ ‡æ³¨æ•°æ®ä¸ºç©º", debug_mode)
                return [], {}
            
            parsed_data = json.loads(annotation_data)
            self._log_debug(f"ğŸ“Š è§£ææ ‡æ³¨æ•°æ®æˆåŠŸï¼Œæ•°æ®ç±»å‹: {type(parsed_data)}", debug_mode)
            
            # æå–annotations
            annotations = []
            if isinstance(parsed_data, dict):
                if "annotations" in parsed_data:
                    annotations = parsed_data["annotations"]
                elif "layers_data" in parsed_data:
                    annotations = parsed_data["layers_data"]
            elif isinstance(parsed_data, list):
                annotations = parsed_data
            
            self._log_debug(f"ğŸ“ æå–åˆ° {len(annotations)} ä¸ªæ ‡æ³¨", debug_mode)
            return annotations, parsed_data
            
        except json.JSONDecodeError as e:
            self._log_debug(f"âŒ JSONè§£æå¤±è´¥: {e}", debug_mode)
            return [], {}
        except Exception as e:
            self._log_debug(f"âŒ æ ‡æ³¨æ•°æ®è§£æå¼‚å¸¸: {e}", debug_mode)
            return [], {}
    
    def _connect_ollama(self, url: str, debug_mode: bool) -> Optional[object]:
        """è¿æ¥OllamaæœåŠ¡"""
        try:
            if not OLLAMA_AVAILABLE:
                self._log_debug("âŒ Ollamaæ¨¡å—ä¸å¯ç”¨", debug_mode)
                return None
                
            from ollama import Client
            client = Client(host=url)
            # æµ‹è¯•è¿æ¥
            models = client.list()
            self._log_debug(f"ğŸ”— Ollamaè¿æ¥æˆåŠŸï¼Œå¯ç”¨æ¨¡å‹æ•°: {len(models.get('models', []))}", debug_mode)
            return client
        except Exception as e:
            self._log_debug(f"âŒ Ollamaè¿æ¥å¤±è´¥: {e}", debug_mode)
            return None
    
    def _build_system_prompt(self, edit_instruction_type: str, output_format: str) -> str:
        """æ„å»ºç³»ç»Ÿæç¤ºè¯"""
        
        # ç¼–è¾‘ç­–ç•¥è¯´æ˜åŠå…·ä½“æŒ‡å¯¼
        strategy_descriptions = {
            "spatial_precise": {
                "description": "ä¸“æ³¨äºç²¾ç¡®çš„ç©ºé—´ä½ç½®æè¿°å’Œåæ ‡å®šä½",
                "guidance": """Focus on:
- Precise spatial positioning and coordinates
- Exact boundary definitions
- Clear geometric transformations
- Maintain spatial relationships
- Include specific pixel/region coordinates when possible"""
            },
            "semantic_enhanced": {
                "description": "å¼ºè°ƒè¯­ä¹‰ç†è§£å’Œå†…å®¹è¯†åˆ«ï¼Œç”Ÿæˆè¯­ä¹‰ä¸°å¯Œçš„ç¼–è¾‘æŒ‡ä»¤",
                "guidance": """Focus on:
- Object recognition and semantic understanding
- Context-aware content modifications
- Intelligent object relationships
- Meaningful content additions/removals
- Preserve semantic coherence"""
            },
            "style_coherent": {
                "description": "æ³¨é‡æ•´ä½“é£æ ¼çš„åè°ƒç»Ÿä¸€ï¼Œç¡®ä¿ç¼–è¾‘åçš„è§†è§‰ä¸€è‡´æ€§",
                "guidance": """Focus on:
- Visual style consistency
- Color harmony and palette coherence
- Texture and material uniformity
- Lighting and shadow consistency
- Overall aesthetic unity"""
            },
            "content_aware": {
                "description": "æ™ºèƒ½ç†è§£å›¾åƒå†…å®¹å’Œä¸Šä¸‹æ–‡ï¼Œç”Ÿæˆå†…å®¹æ„ŸçŸ¥çš„ç¼–è¾‘æŒ‡ä»¤",
                "guidance": """Focus on:
- Context-sensitive modifications
- Content-appropriate enhancements
- Scene understanding
- Natural content integration
- Preserve content authenticity"""
            },
            "multi_region": {
                "description": "å¤„ç†å¤šä¸ªæ ‡æ³¨åŒºåŸŸçš„åè°ƒå…³ç³»ï¼Œç¡®ä¿æ•´ä½“ç¼–è¾‘çš„å’Œè°æ€§",
                "guidance": """Focus on:
- Coordinate multiple region edits
- Ensure region-to-region harmony
- Balance competing modifications
- Maintain overall composition
- Synchronize related changes"""
            },
            "custom": {
                "description": "æ ¹æ®ç”¨æˆ·éœ€æ±‚ç”Ÿæˆè‡ªå®šä¹‰çš„ç¼–è¾‘æŒ‡ä»¤",
                "guidance": """Focus on:
- User-specific requirements
- Flexible adaptation to needs
- Custom modification approaches
- Personalized editing strategies"""
            }
        }
        
        # è¾“å‡ºæ ¼å¼è¯´æ˜
        format_descriptions = {
            "flux_kontext_standard": """Output EXACTLY in this format:

[EDIT_OPERATIONS]
operation_1: change the marked area to [target color/object]

[SPATIAL_CONSTRAINTS]
preserve_regions: ["background"]
blend_boundaries: "seamless"

[QUALITY_CONTROLS]
detail_level: "high"
consistency: "maintain_original_quality"
""",
            "structured_json": "è¾“å‡ºç»“æ„åŒ–çš„JSONæ ¼å¼ï¼ŒåŒ…å«æ“ä½œã€çº¦æŸå’Œè´¨é‡æ§åˆ¶ä¿¡æ¯",
            "natural_language": "è¾“å‡ºè‡ªç„¶æµç•…çš„è¯­è¨€æè¿°ï¼Œé€‚åˆç›´æ¥ä½œä¸ºæç¤ºè¯ä½¿ç”¨",
            "prompt_engineering": "è¾“å‡ºç»è¿‡æç¤ºå·¥ç¨‹ä¼˜åŒ–çš„æ ¼å¼ï¼ŒåŒ…å«æ˜ç¡®çš„æ ‡ç­¾å’Œç»“æ„"
        }
        
        # è·å–å½“å‰ç­–ç•¥çš„æŒ‡å¯¼
        current_strategy = strategy_descriptions.get(edit_instruction_type, strategy_descriptions["spatial_precise"])
        
        system_prompt = f"""Generate image editing instructions in Flux Kontext format.

EDITING STRATEGY: {edit_instruction_type}
{current_strategy["guidance"]}

OUTPUT FORMAT:
{format_descriptions.get(output_format, "Standard format")}

Rules:
- Output ONLY the formatted instructions
- Do NOT include code, explanations, or comments
- Apply the specific editing strategy focus areas listed above
- Ensure instructions match the strategy requirements"""
        
        return system_prompt
    
    def _build_user_prompt(self, annotations: List[Dict], parsed_data: Dict,
                          reference_context: str, edit_intensity: float,
                          preservation_mask: str, style_guidance: str) -> str:
        """æ„å»ºç”¨æˆ·æç¤ºè¯"""
        
        prompt_parts = []
        
        # 1. æ ‡æ³¨ä¿¡æ¯
        prompt_parts.append("=== å›¾åƒæ ‡æ³¨ä¿¡æ¯ ===")
        for i, annotation in enumerate(annotations):
            annotation_desc = f"æ ‡æ³¨ {i+1}:"
            annotation_desc += f" ç±»å‹={annotation.get('type', 'unknown')}"
            annotation_desc += f" é¢œè‰²={annotation.get('color', '#000000')}"
            
            if 'start' in annotation and 'end' in annotation:
                start = annotation['start']
                end = annotation['end']
                annotation_desc += f" åæ ‡=({start.get('x', 0)},{start.get('y', 0)})-({end.get('x', 0)},{end.get('y', 0)})"
            
            prompt_parts.append(annotation_desc)
        
        # 2. æ“ä½œä¿¡æ¯
        if 'operation_type' in parsed_data:
            prompt_parts.append(f"\n=== æ“ä½œç±»å‹ ===")
            prompt_parts.append(f"æ“ä½œ: {parsed_data['operation_type']}")
        
        if 'target_description' in parsed_data:
            prompt_parts.append(f"ç›®æ ‡æè¿°: {parsed_data['target_description']}")
        
        # 3. å¢å¼ºæç¤ºè¯
        if 'constraint_prompts' in parsed_data and parsed_data['constraint_prompts']:
            prompt_parts.append(f"\n=== çº¦æŸæ€§æç¤ºè¯ ===")
            constraints = parsed_data['constraint_prompts']
            if isinstance(constraints, list):
                prompt_parts.append(", ".join(constraints))
            else:
                prompt_parts.append(str(constraints))
        
        if 'decorative_prompts' in parsed_data and parsed_data['decorative_prompts']:
            prompt_parts.append(f"\n=== ä¿®é¥°æ€§æç¤ºè¯ ===")
            decoratives = parsed_data['decorative_prompts']
            if isinstance(decoratives, list):
                prompt_parts.append(", ".join(decoratives))
            else:
                prompt_parts.append(str(decoratives))
        
        # 4. å‚è€ƒä¸Šä¸‹æ–‡
        if reference_context:
            prompt_parts.append(f"\n=== å‚è€ƒä¸Šä¸‹æ–‡ ===")
            prompt_parts.append(reference_context)
        
        # 5. ç¼–è¾‘å‚æ•°
        prompt_parts.append(f"\n=== ç¼–è¾‘å‚æ•° ===")
        prompt_parts.append(f"ç¼–è¾‘å¼ºåº¦: {edit_intensity}")
        
        if preservation_mask:
            prompt_parts.append(f"ä¿æŠ¤åŒºåŸŸ: {preservation_mask}")
        
        if style_guidance:
            prompt_parts.append(f"é£æ ¼æŒ‡å¯¼: {style_guidance}")
        
        # 6. ç”Ÿæˆè¦æ±‚
        prompt_parts.append(f"\n=== ç”Ÿæˆè¦æ±‚ ===")
        prompt_parts.append("è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ç”Ÿæˆä¼˜åŒ–çš„Flux Kontextç¼–è¾‘æŒ‡ä»¤ã€‚")
        prompt_parts.append("ç¡®ä¿æŒ‡ä»¤ç²¾ç¡®ã€å¯æ‰§è¡Œï¼Œå¹¶ç¬¦åˆæŒ‡å®šçš„è¾“å‡ºæ ¼å¼ã€‚")
        
        return "\n".join(prompt_parts)
    
    def _generate_with_ollama(self, client: object, model: str, system_prompt: str,
                             user_prompt: str, temperature: float, top_p: float,
                             keep_alive: int, debug_mode: bool) -> Optional[str]:
        """ä½¿ç”¨Ollamaç”Ÿæˆå¢å¼ºæŒ‡ä»¤"""
        try:
            self._log_debug(f"ğŸ¤– è°ƒç”¨Ollamaæ¨¡å‹: {model}", debug_mode)
            
            # é…ç½®ç”Ÿæˆå‚æ•°
            options = {
                "temperature": temperature,
                "top_p": top_p,
            }
            
            # å¯¹äºqwen3ç­‰æ”¯æŒthinkingæ¨¡å¼çš„æ¨¡å‹ï¼Œå°è¯•ç¦ç”¨thinkingè¾“å‡º
            if "qwen3" in model.lower():
                options.update({
                    "thinking": False,  # å°è¯•ç¦ç”¨thinkingæ¨¡å¼
                    "stream": False,    # ç¦ç”¨æµå¼è¾“å‡º
                })
                # åœ¨system promptä¸­æ˜ç¡®è¦æ±‚ä¸è¦thinking
                system_prompt += "\n\nIMPORTANT: Do not include any thinking process, reasoning steps, or <think> tags in your response. Output only the final formatted instructions."
            
            # ç”Ÿæˆå“åº”
            response = client.generate(
                model=model,
                prompt=user_prompt,
                system=system_prompt,
                options=options,
                keep_alive=keep_alive
            )
            
            if response and 'response' in response:
                generated_text = response['response'].strip()
                
                # è¿‡æ»¤æ‰qwen3ç­‰æ¨¡å‹çš„thinkingå†…å®¹
                filtered_text = self._filter_thinking_content(generated_text, debug_mode)
                
                self._log_debug(f"âœ… Ollamaç”ŸæˆæˆåŠŸï¼ŒåŸå§‹é•¿åº¦: {len(generated_text)}, è¿‡æ»¤åé•¿åº¦: {len(filtered_text)} å­—ç¬¦", debug_mode)
                return filtered_text
            else:
                self._log_debug("âŒ Ollamaå“åº”æ ¼å¼å¼‚å¸¸", debug_mode)
                return None
                
        except Exception as e:
            self._log_debug(f"âŒ Ollamaç”Ÿæˆå¤±è´¥: {e}", debug_mode)
            return None
    
    def _filter_thinking_content(self, text: str, debug_mode: bool) -> str:
        """è¿‡æ»¤æ‰æ¨¡å‹çš„thinkingå†…å®¹ (å¦‚qwen3çš„<think>æ ‡ç­¾)"""
        try:
            import re
            
            # è¿‡æ»¤å¸¸è§çš„thinkingæ ‡ç­¾æ ¼å¼
            thinking_patterns = [
                r'<think>.*?</think>',  # qwen3çš„<think>æ ‡ç­¾
                r'<thinking>.*?</thinking>',  # å…¶ä»–å¯èƒ½çš„thinkingæ ‡ç­¾
                r'<thought>.*?</thought>',  # thoughtæ ‡ç­¾
                r'æ€è€ƒ[:ï¼š].*?(?=\n|$)',  # ä¸­æ–‡"æ€è€ƒ:"å¼€å¤´çš„è¡Œ
                r'Let me think.*?(?=\n|$)',  # è‹±æ–‡thinkingå¼€å¤´
                r'I need to think.*?(?=\n|$)',  # å…¶ä»–thinkingè¡¨è¾¾
            ]
            
            filtered_text = text
            original_length = len(text)
            
            # åº”ç”¨æ‰€æœ‰è¿‡æ»¤è§„åˆ™
            for pattern in thinking_patterns:
                filtered_text = re.sub(pattern, '', filtered_text, flags=re.DOTALL | re.IGNORECASE)
            
            # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
            filtered_text = re.sub(r'\n\s*\n\s*\n', '\n\n', filtered_text)  # å¤šä¸ªç©ºè¡Œå˜æˆä¸¤ä¸ª
            filtered_text = filtered_text.strip()
            
            # å¦‚æœè¿‡æ»¤æ‰äº†å†…å®¹ï¼Œè®°å½•æ—¥å¿—
            if len(filtered_text) < original_length:
                self._log_debug(f"ğŸ§¹ è¿‡æ»¤æ‰thinkingå†…å®¹ï¼Œå‡å°‘äº† {original_length - len(filtered_text)} å­—ç¬¦", debug_mode)
            
            return filtered_text
            
        except Exception as e:
            self._log_debug(f"âš ï¸ thinkingå†…å®¹è¿‡æ»¤å¤±è´¥: {e}ï¼Œè¿”å›åŸå§‹å†…å®¹", debug_mode)
            return text
    
    def _format_flux_instructions(self, instructions: str, output_format: str, debug_mode: bool) -> str:
        """æ ¼å¼åŒ–FluxæŒ‡ä»¤"""
        try:
            if output_format == "flux_kontext_standard":
                # å¦‚æœå·²ç»æ˜¯æ ‡å‡†æ ¼å¼ï¼Œç›´æ¥è¿”å›
                if "[EDIT_OPERATIONS]" in instructions:
                    return instructions
                else:
                    # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                    formatted = f"""[EDIT_OPERATIONS]
operation_1: {instructions}

[SPATIAL_CONSTRAINTS]
preserve_regions: ["original_composition"]
blend_boundaries: "seamless"

[QUALITY_CONTROLS]
detail_level: "high"
consistency: "maintain_original_quality"
realism: "photorealistic"
"""
                    return formatted
            elif output_format == "structured_json":
                # è½¬æ¢ä¸ºJSONæ ¼å¼
                try:
                    json_output = {
                        "operations": [{"description": instructions}],
                        "constraints": ["preserve_original_composition"],
                        "quality": {"detail_level": "high", "consistency": "high"}
                    }
                    return json.dumps(json_output, indent=2, ensure_ascii=False)
                except:
                    return instructions
            else:
                # å…¶ä»–æ ¼å¼ç›´æ¥è¿”å›
                return instructions
                
        except Exception as e:
            self._log_debug(f"âš ï¸ æ ¼å¼åŒ–å¤±è´¥ï¼Œè¿”å›åŸå§‹å†…å®¹: {e}", debug_mode)
            return instructions
    
    def _generate_spatial_mappings(self, annotations: List[Dict], debug_mode: bool) -> str:
        """ç”Ÿæˆç©ºé—´æ˜ å°„ä¿¡æ¯"""
        try:
            mappings = {
                "regions": [],
                "coordinate_system": "absolute_pixels",
                "total_annotations": len(annotations)
            }
            
            for i, annotation in enumerate(annotations):
                region = {
                    "id": annotation.get("id", f"annotation_{i+1}"),
                    "type": annotation.get("type", "unknown"),
                    "color_code": annotation.get("color", "#000000"),
                    "number": annotation.get("number", i+1)
                }
                
                # æ·»åŠ åæ ‡ä¿¡æ¯
                if 'start' in annotation and 'end' in annotation:
                    start = annotation['start']
                    end = annotation['end']
                    region["coordinates"] = [
                        start.get('x', 0), start.get('y', 0),
                        end.get('x', 0), end.get('y', 0)
                    ]
                elif 'points' in annotation:
                    region["points"] = annotation['points']
                
                mappings["regions"].append(region)
            
            return json.dumps(mappings, indent=2, ensure_ascii=False)
            
        except Exception as e:
            self._log_debug(f"âš ï¸ ç©ºé—´æ˜ å°„ç”Ÿæˆå¤±è´¥: {e}", debug_mode)
            return f'{{"error": "Failed to generate spatial mappings: {str(e)}"}}'
    
    def _generate_processing_metadata(self, model: str, strategy: str, 
                                    annotation_count: int, debug_mode: bool) -> str:
        """ç”Ÿæˆå¤„ç†å…ƒæ•°æ®"""
        try:
            processing_time = time.time() - self.start_time if self.start_time else 0
            
            metadata = {
                "processing_time": f"{processing_time:.2f}s",
                "timestamp": datetime.now().isoformat(),
                "ollama_model_used": model,
                "edit_strategy": strategy,
                "annotations_processed": annotation_count,
                "enhancement_applied": True,
                "status": "success"
            }
            
            if debug_mode and self.debug_logs:
                metadata["debug_logs"] = self.debug_logs
            
            return json.dumps(metadata, indent=2, ensure_ascii=False)
            
        except Exception as e:
            return f'{{"error": "Failed to generate metadata: {str(e)}"}}'
    
    def _log_debug(self, message: str, debug_mode: bool):
        """è®°å½•è°ƒè¯•ä¿¡æ¯"""
        if debug_mode:
            timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
            log_message = f"[{timestamp}] {message}"
            self.debug_logs.append(log_message)
            print(log_message)  # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
    
    def _create_fallback_output(self, error_msg: str, debug_mode: bool) -> Tuple[str, str, str]:
        """åˆ›å»ºå¤±è´¥æ—¶çš„å›é€€è¾“å‡º"""
        self._log_debug(f"âŒ åˆ›å»ºå›é€€è¾“å‡º: {error_msg}", debug_mode)
        
        fallback_instructions = f"""[EDIT_OPERATIONS]
operation_1: Apply standard edit to marked regions
# Error: {error_msg}

[SPATIAL_CONSTRAINTS]
preserve_regions: ["all_unmarked_areas"]
blend_boundaries: "seamless"

[QUALITY_CONTROLS]
detail_level: "standard"
consistency: "maintain_original"
"""
        
        fallback_mappings = f'{{"error": "{error_msg}", "regions": []}}'
        
        fallback_metadata = f'{{"error": "{error_msg}", "status": "failed", "timestamp": "{datetime.now().isoformat()}"}}'
        
        return (fallback_instructions, fallback_mappings, fallback_metadata)


# æ·»åŠ APIç«¯ç‚¹ç”¨äºåŠ¨æ€è·å–æ¨¡å‹
if WEB_AVAILABLE:
    @PromptServer.instance.routes.post("/ollama_flux_enhancer/get_models")
    async def get_models_endpoint(request):
        """è·å–å¯ç”¨çš„Ollamaæ¨¡å‹åˆ—è¡¨"""
        try:
            data = await request.json()
            url = data.get("url", "http://127.0.0.1:11434")
            
            if not OLLAMA_AVAILABLE:
                return web.json_response([])
            
            from ollama import Client
            client = Client(host=url)
            models_response = client.list()
            models = models_response.get('models', [])
            
            # æå–æ¨¡å‹åç§° - ä½¿ç”¨ä¸get_available_modelsç›¸åŒçš„é€»è¾‘
            model_names = []
            for model in models:
                if isinstance(model, dict):
                    name = model.get('model') or model.get('name')
                    if name:
                        model_names.append(name)
                elif hasattr(model, 'model'):
                    # å¤„ç†å¯¹è±¡ç±»å‹çš„æ¨¡å‹
                    model_names.append(model.model)
                else:
                    # å°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶æå–æ¨¡å‹å
                    model_str = str(model)
                    if "model='" in model_str:
                        # ä»å­—ç¬¦ä¸²ä¸­æå–æ¨¡å‹å ä¾‹: "model='qwen3:0.6b'"
                        start = model_str.find("model='") + 7
                        end = model_str.find("'", start)
                        if end > start:
                            model_names.append(model_str[start:end])
                        else:
                            # å¦‚æœæå–å¤±è´¥ï¼Œè·³è¿‡è¿™ä¸ªæ¨¡å‹
                            print(f"Warning: Failed to extract model name from: {model_str[:100]}...")
                    else:
                        # å¦‚æœæ ¼å¼ä¸åŒ¹é…ï¼Œè·³è¿‡è¿™ä¸ªæ¨¡å‹
                        print(f"Warning: Unknown model format: {model_str[:100]}...")
            
            return web.json_response(model_names)
            
        except Exception as e:
            print(f"Error fetching Ollama models: {e}")
            return web.json_response([])


# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "OllamaFluxKontextEnhancer": OllamaFluxKontextEnhancer,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "OllamaFluxKontextEnhancer": "ğŸ¤– Ollama Flux Kontext Enhancer",
}