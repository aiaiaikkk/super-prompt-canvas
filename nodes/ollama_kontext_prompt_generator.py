"""
Ollama Kontext Prompt Generator - çº¯åç«¯Ollamaæç¤ºè¯ç”ŸæˆèŠ‚ç‚¹
è§£å†³äº‘ç«¯ç¯å¢ƒHTTPS/HTTPæ··åˆå†…å®¹é—®é¢˜çš„å®Œç¾æ–¹æ¡ˆ

åŠŸèƒ½ç‰¹ç‚¹ï¼š
- å®Œå…¨åç«¯å¤„ç†ï¼Œæ— éœ€å‰ç«¯Ollamaè¿æ¥
- é›†æˆå¼•å¯¼è¯é€‰æ‹©ç³»ç»Ÿ
- è‡ªåŠ¨æ£€æµ‹å¯ç”¨Ollamaæ¨¡å‹
- æ”¯æŒäº‘ç«¯å’Œæœ¬åœ°ç¯å¢ƒ
- ä¸“ä¸šçš„æç¤ºè¯æ¨¡æ¿ç³»ç»Ÿ
"""

import json
import time
import random
import os
import sys
from typing import Dict, List, Any, Tuple, Optional

try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False
    requests = None

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None

CATEGORY_TYPE = "ğŸ¨ LRPG Canvas"

class OllamaKontextPromptGenerator:
    """
    Ollama Kontextæç¤ºè¯ç”Ÿæˆå™¨ - çº¯åç«¯å®ç°
    è§£å†³äº‘ç«¯HTTPS/HTTPæ··åˆå†…å®¹é—®é¢˜
    """
    
    def __init__(self):
        self.ollama_url = "http://127.0.0.1:11434"
        
    @classmethod
    def INPUT_TYPES(cls):
        # è·å–å¯ç”¨çš„Ollamaæ¨¡å‹
        available_models = cls._get_available_models()
        
        # ç¼–è¾‘æ„å›¾é€‰é¡¹ - 16ç§æ“ä½œ
        editing_intents = [
            "é¢œè‰²ä¿®æ”¹", "ç‰©ä½“ç§»é™¤", "ç‰©ä½“æ›¿æ¢", "ç‰©ä½“æ·»åŠ ",
            "èƒŒæ™¯æ›´æ¢", "æ¢è„¸", "è´¨é‡å¢å¼º", "å›¾åƒä¿®å¤",
            "é£æ ¼è½¬æ¢", "æ–‡å­—ç¼–è¾‘", "å…‰çº¿è°ƒæ•´", "é€è§†æ ¡æ­£",
            "æ¨¡ç³Š/é”åŒ–", "å±€éƒ¨å˜å½¢", "æ„å›¾è°ƒæ•´", "é€šç”¨ç¼–è¾‘"
        ]
        
        # åº”ç”¨åœºæ™¯é€‰é¡¹ - 16ç§åœºæ™¯
        application_scenarios = [
            "ç”µå•†äº§å“", "ç¤¾äº¤åª’ä½“", "è¥é”€æ´»åŠ¨", "äººåƒæ‘„å½±",
            "ç”Ÿæ´»æ–¹å¼", "ç¾é£Ÿæ‘„å½±", "æˆ¿åœ°äº§", "æ—¶å°šé›¶å”®",
            "æ±½è½¦å±•ç¤º", "ç¾å¦†åŒ–å¦†å“", "ä¼ä¸šå“ç‰Œ", "æ´»åŠ¨æ‘„å½±",
            "äº§å“ç›®å½•", "è‰ºæœ¯åˆ›ä½œ", "çºªå®æ‘„å½±", "è‡ªåŠ¨é€‰æ‹©"
        ]
        
        return {
            "required": {
                "description": ("STRING", {
                    "default": "å°†é€‰å®šåŒºåŸŸçš„é¢œè‰²æ”¹ä¸ºçº¢è‰²",
                    "multiline": True,
                    "placeholder": "è¯·æè¿°æ‚¨æƒ³è¦è¿›è¡Œçš„ç¼–è¾‘..."
                }),
                "editing_intent": (editing_intents, {
                    "default": "é¢œè‰²ä¿®æ”¹"
                }),
                "application_scenario": (application_scenarios, {
                    "default": "ç”µå•†äº§å“"
                }),
                "ollama_model": (available_models, {
                    "default": available_models[0] if available_models else "deepseek-r1:1.5b"
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.0,
                    "max": 2.0,
                    "step": 0.1,
                    "display": "slider"
                }),
                "seed": ("INT", {
                    "default": 42,
                    "min": 0,
                    "max": 1000000
                }),
            },
            "optional": {
                "custom_guidance": ("STRING", {
                    "default": "",
                    "multiline": True,
                    "placeholder": "å¯é€‰ï¼šè‡ªå®šä¹‰å¼•å¯¼è¯..."
                }),
                "ollama_url": ("STRING", {
                    "default": "http://127.0.0.1:11434",
                    "placeholder": "OllamaæœåŠ¡åœ°å€"
                }),
            }
        }
    
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("generated_prompt",)
    FUNCTION = "generate_prompt"
    CATEGORY = CATEGORY_TYPE
    OUTPUT_NODE = False
    
    @classmethod
    def _get_available_models(cls):
        """è·å–å¯ç”¨çš„Ollamaæ¨¡å‹åˆ—è¡¨"""
        try:
            if not REQUESTS_AVAILABLE:
                return ["deepseek-r1:1.5b", "qwen3:4b", "qwen3:8b"]
            
            response = requests.get("http://127.0.0.1:11434/api/tags", timeout=3)
            if response.status_code == 200:
                models_data = response.json()
                models = [model['name'] for model in models_data.get('models', [])]
                return models if models else ["deepseek-r1:1.5b"]
            else:
                return ["deepseek-r1:1.5b", "qwen3:4b", "qwen3:8b"]
        except:
            return ["deepseek-r1:1.5b", "qwen3:4b", "qwen3:8b"]
    
    def _get_guidance_template(self, editing_intent: str, application_scenario: str) -> str:
        """æ ¹æ®ç¼–è¾‘æ„å›¾å’Œåº”ç”¨åœºæ™¯ç”Ÿæˆå¼•å¯¼è¯æ¨¡æ¿"""
        
        # ç¼–è¾‘æ„å›¾æ¨¡æ¿
        intent_templates = {
            "é¢œè‰²ä¿®æ”¹": "Transform the selected area to the specified color with natural blending and seamless integration",
            "ç‰©ä½“ç§»é™¤": "Remove the selected object completely while reconstructing the background naturally",
            "ç‰©ä½“æ›¿æ¢": "Replace the selected object with the described item maintaining proper lighting and perspective",
            "ç‰©ä½“æ·»åŠ ": "Add the described element to the selected area with realistic placement and lighting",
            "èƒŒæ™¯æ›´æ¢": "Change the background to the specified environment while preserving subject lighting",
            "æ¢è„¸": "Replace the face in the selected area with natural expression and proper lighting match",
            "è´¨é‡å¢å¼º": "Enhance the overall quality with improved sharpness, detail, and professional finish",
            "å›¾åƒä¿®å¤": "Repair and restore the damaged areas with seamless texture reconstruction",
            "é£æ ¼è½¬æ¢": "Apply the specified artistic style while maintaining the core composition",
            "æ–‡å­—ç¼–è¾‘": "Modify or add text elements with professional typography and layout",
            "å…‰çº¿è°ƒæ•´": "Adjust lighting conditions to create the desired mood and atmosphere",
            "é€è§†æ ¡æ­£": "Correct perspective distortion while maintaining natural proportions",
            "æ¨¡ç³Š/é”åŒ–": "Apply selective focus adjustments to enhance visual hierarchy",
            "å±€éƒ¨å˜å½¢": "Apply geometric transformations to the selected area naturally",
            "æ„å›¾è°ƒæ•´": "Recompose the image elements for improved visual balance",
            "é€šç”¨ç¼–è¾‘": "Apply the requested editing with professional quality and attention to detail"
        }
        
        # åº”ç”¨åœºæ™¯ä¼˜åŒ–è¯
        scenario_enhancements = {
            "ç”µå•†äº§å“": "with clean, professional appearance suitable for product catalogs",
            "ç¤¾äº¤åª’ä½“": "optimized for social media with engaging visual appeal",
            "è¥é”€æ´»åŠ¨": "designed for marketing campaigns with strong visual impact",
            "äººåƒæ‘„å½±": "with professional portrait quality and natural skin tones",
            "ç”Ÿæ´»æ–¹å¼": "capturing authentic lifestyle moments with warm atmosphere",
            "ç¾é£Ÿæ‘„å½±": "with appetizing presentation and proper food styling",
            "æˆ¿åœ°äº§": "showcasing architectural features with professional real estate quality",
            "æ—¶å°šé›¶å”®": "with fashion-forward styling and premium brand aesthetic",
            "æ±½è½¦å±•ç¤º": "highlighting automotive features with showroom quality",
            "ç¾å¦†åŒ–å¦†å“": "with beauty-focused enhancement and glamorous appeal",
            "ä¼ä¸šå“ç‰Œ": "maintaining corporate professionalism and brand consistency",
            "æ´»åŠ¨æ‘„å½±": "capturing dynamic moments with event documentation quality",
            "äº§å“ç›®å½•": "with catalog-ready presentation and consistent lighting",
            "è‰ºæœ¯åˆ›ä½œ": "with artistic interpretation and creative visual expression",
            "çºªå®æ‘„å½±": "maintaining authenticity with documentary-style realism",
            "è‡ªåŠ¨é€‰æ‹©": "with intelligent optimization for the specific content type"
        }
        
        base_template = intent_templates.get(editing_intent, intent_templates["é€šç”¨ç¼–è¾‘"])
        scenario_enhancement = scenario_enhancements.get(application_scenario, scenario_enhancements["è‡ªåŠ¨é€‰æ‹©"])
        
        return f"{base_template} {scenario_enhancement}"
    
    def _call_ollama_api(self, prompt: str, model: str, temperature: float, 
                        seed: int, ollama_url: str) -> str:
        """è°ƒç”¨Ollama APIç”Ÿæˆæç¤ºè¯"""
        try:
            if not REQUESTS_AVAILABLE:
                raise Exception("requestsåº“æœªå®‰è£…ï¼Œæ— æ³•è°ƒç”¨Ollama API")
            
            # æ„å»ºç³»ç»Ÿæç¤ºè¯
            system_prompt = """You are an ENGLISH-ONLY image editing prompt generator.

CRITICAL RULES:
1. Output in ENGLISH ONLY - NO Chinese characters allowed
2. Generate ONE clear, concise editing instruction (30-80 words)
3. Use professional photography and editing terminology
4. Focus on technical accuracy and visual quality
5. Include specific details about lighting, composition, and quality

FORMAT: Start with an action verb, describe the target and method, end with quality terms.
EXAMPLE: "Transform the selected area to vibrant red color while maintaining natural lighting and seamless edge blending with professional quality finish"

REMEMBER: ENGLISH ONLY OUTPUT - Any Chinese characters will be rejected."""
            
            # æ„å»ºç”¨æˆ·æç¤ºè¯
            user_prompt = f"""Based on this editing request: "{prompt}"

Generate a professional English editing instruction that:
- Clearly describes the editing action
- Includes technical details for quality results
- Uses professional editing terminology
- Focuses on achieving realistic, natural results

Output format: Single paragraph, 30-80 words, English only."""
            
            # è°ƒç”¨Ollama API
            api_url = f"{ollama_url}/api/generate"
            payload = {
                "model": model,
                "prompt": user_prompt,
                "system": system_prompt,
                "stream": False,
                "options": {
                    "temperature": temperature,
                    "seed": seed,
                    "num_predict": 150,
                    "top_k": 50,
                    "top_p": 0.9,
                    "repeat_penalty": 1.1
                }
            }
            
            response = requests.post(api_url, json=payload, timeout=60)
            response.raise_for_status()
            
            result = response.json()
            generated_text = result.get('response', '').strip()
            
            # æ¸…ç†å“åº”
            cleaned_text = self._clean_response(generated_text)
            
            return cleaned_text
            
        except Exception as e:
            print(f"[Ollama Kontext] APIè°ƒç”¨å¤±è´¥: {e}")
            # è¿”å›å¤‡ç”¨æ¨¡æ¿
            return self._get_fallback_prompt(prompt)
    
    def _clean_response(self, response: str) -> str:
        """æ¸…ç†Ollamaå“åº”ï¼Œç¡®ä¿è¾“å‡ºè‹±æ–‡"""
        import re
        
        if not response:
            return "Apply professional editing to the selected area with high quality results"
        
        # æ£€æµ‹ä¸­æ–‡å­—ç¬¦
        chinese_pattern = re.compile(r'[\u4e00-\u9fff]+')
        has_chinese = bool(chinese_pattern.search(response))
        
        if has_chinese:
            print("[Ollama Kontext] æ£€æµ‹åˆ°ä¸­æ–‡è¾“å‡ºï¼Œä½¿ç”¨è‹±æ–‡å¤‡ç”¨æ–¹æ¡ˆ")
            # å°è¯•æå–è‹±æ–‡éƒ¨åˆ†
            english_sentences = re.findall(r'[A-Z][a-zA-Z\s,\.;:\-!?]+[\.!?]', response)
            if english_sentences:
                longest = max(english_sentences, key=len)
                if len(longest) > 20:
                    return longest.strip()
        
        # æ¸…ç†æ ¼å¼
        cleaned = re.sub(r'^[:\-\s]*', '', response)  # ç§»é™¤å¼€å¤´çš„ç¬¦å·
        cleaned = re.sub(r'\n+', ' ', cleaned)        # æ›¿æ¢æ¢è¡Œç¬¦
        cleaned = re.sub(r'\s+', ' ', cleaned)        # åˆå¹¶å¤šä½™ç©ºæ ¼
        
        return cleaned.strip()
    
    def _get_fallback_prompt(self, description: str) -> str:
        """ç”Ÿæˆå¤‡ç”¨æç¤ºè¯"""
        # åŸºäºæè¿°å†…å®¹ç”Ÿæˆæ™ºèƒ½å¤‡ç”¨è¯
        desc_lower = description.lower()
        
        if any(word in desc_lower for word in ['color', 'é¢œè‰²', 'red', 'blue', 'green', 'çº¢', 'è“', 'ç»¿']):
            return "Transform the selected area to the specified color with natural blending and seamless integration"
        elif any(word in desc_lower for word in ['remove', 'ç§»é™¤', 'delete', 'åˆ é™¤']):
            return "Remove the selected object completely while reconstructing the background naturally"
        elif any(word in desc_lower for word in ['replace', 'æ›¿æ¢', 'change', 'æ›´æ¢']):
            return "Replace the selected element with the described item maintaining proper lighting and perspective"
        elif any(word in desc_lower for word in ['add', 'æ·»åŠ ', 'insert', 'æ’å…¥']):
            return "Add the described element to the selected area with realistic placement and lighting"
        elif any(word in desc_lower for word in ['enhance', 'å¢å¼º', 'improve', 'æ”¹å–„']):
            return "Enhance the selected area with improved quality, sharpness, and professional finish"
        else:
            return "Apply professional editing to the selected area according to the specified requirements with high quality results"
    
    def generate_prompt(self, description: str, editing_intent: str, application_scenario: str,
                       ollama_model: str, temperature: float, seed: int,
                       custom_guidance: str = "", ollama_url: str = "http://127.0.0.1:11434"):
        """ç”ŸæˆKontextæç¤ºè¯"""
        try:
            print(f"[Ollama Kontext] å¼€å§‹ç”Ÿæˆæç¤ºè¯...")
            print(f"[Ollama Kontext] æ¨¡å‹: {ollama_model}, æ„å›¾: {editing_intent}, åœºæ™¯: {application_scenario}")
            
            # æ„å»ºå®Œæ•´çš„æç¤ºè¯
            if custom_guidance:
                full_prompt = f"{description}\n\nè‡ªå®šä¹‰å¼•å¯¼: {custom_guidance}"
            else:
                guidance_template = self._get_guidance_template(editing_intent, application_scenario)
                full_prompt = f"{description}\n\nå¼•å¯¼æ¨¡æ¿: {guidance_template}"
            
            # è°ƒç”¨Ollama API
            generated_prompt = self._call_ollama_api(
                prompt=full_prompt,
                model=ollama_model,
                temperature=temperature,
                seed=seed,
                ollama_url=ollama_url
            )
            
            print(f"[Ollama Kontext] æç¤ºè¯ç”Ÿæˆå®Œæˆ: {generated_prompt[:50]}...")
            
            return (generated_prompt,)
            
        except Exception as e:
            print(f"[Ollama Kontext] ç”Ÿæˆå¤±è´¥: {e}")
            # è¿”å›å¤‡ç”¨æç¤ºè¯
            fallback_prompt = self._get_fallback_prompt(description)
            return (fallback_prompt,)

# æ³¨å†ŒèŠ‚ç‚¹
NODE_CLASS_MAPPINGS = {
    "OllamaKontextPromptGenerator": OllamaKontextPromptGenerator,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "OllamaKontextPromptGenerator": "ğŸ¦™ Ollama Kontext Prompt Generator",
}

print("[Ollama Kontext] Ollama Kontext Prompt Generator node registered")