"""
Global Image Processor Node
Global Image Processor Node - For image enhancement, desaturation, style transfer and other global operations

æ”¯æŒå¤šç§å…¨å›¾å¤„ç†æ¨¡å¼ï¼š
- é«˜æ¸…åŒ– (Upscaling)
- å»è‰²/å•è‰²åŒ– (Desaturation/Monochrome)
- è‰²å½©è°ƒæ•´ (Color Adjustment)
- é£æ ¼è½¬æ¢ (Style Transfer)
- é”åŒ–/æ¨¡ç³Š (Sharpening/Blur)
- å™ªç‚¹å¤„ç† (Noise Processing)
"""

import json
import numpy as np
import torch
from typing import Dict, List, Any, Tuple, Optional
from datetime import datetime

try:
    import comfy.model_management as model_management
    from nodes import MAX_RESOLUTION
    COMFY_AVAILABLE = True
except ImportError:
    COMFY_AVAILABLE = False
    MAX_RESOLUTION = 8192

class GlobalImageProcessor:
    """å…¨å›¾å¤„ç†èŠ‚ç‚¹ - é«˜æ¸…åŒ–ã€å»è‰²ã€é£æ ¼è½¬æ¢ç­‰æ“ä½œ"""
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image": ("IMAGE",),
                "processing_mode": ([
                    "enhance_quality",     # è´¨é‡å¢å¼º
                    "upscale_2x",         # 2å€æ”¾å¤§
                    "upscale_4x",         # 4å€æ”¾å¤§
                    "desaturate",         # å»è‰²
                    "monochrome_warm",    # æš–è‰²å•è‰²
                    "monochrome_cool",    # å†·è‰²å•è‰²
                    "vintage_film",       # èƒ¶ç‰‡é£æ ¼
                    "high_contrast",      # é«˜å¯¹æ¯”åº¦
                    "soft_blur",          # æŸ”å’Œæ¨¡ç³Š
                    "sharpen",            # é”åŒ–
                    "denoise",            # é™å™ª
                    "color_pop",          # è‰²å½©å¢å¼º
                    "dramatic_lighting",  # æˆå‰§æ€§å…‰çº¿
                    "custom"              # è‡ªå®šä¹‰
                ], {"default": "enhance_quality"}),
            },
            "optional": {
                "strength": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 2.0, "step": 0.1, "tooltip": "Processing strength"}),
                "custom_params": ("STRING", {"multiline": True, "default": "", "tooltip": "Custom parameters JSON"}),
                "preserve_details": ("BOOLEAN", {"default": True, "tooltip": "Preserve details"}),
                "clip": ("CLIP", {"tooltip": "Optional: CLIP model for style understanding"}),
                "guidance": ("FLOAT", {"default": 7.5, "min": 0.0, "max": 20.0, "step": 0.1, "tooltip": "Guidance strength"}),
            }
        }
    
    RETURN_TYPES = ("IMAGE", "STRING", "STRING", "CONDITIONING", "STRING")
    RETURN_NAMES = (
        "processed_image",
        "processing_metadata", 
        "suggested_prompt",
        "conditioning",
        "processing_log"
    )
    FUNCTION = "process_global_image"
    CATEGORY = "kontext/global"
    DESCRIPTION = "Global Image Processor Node: Image enhancement, desaturation, style transfer and other global operations, supports multiple preset modes and custom parameters"
    
    def process_global_image(self, image: torch.Tensor, processing_mode: str = "enhance_quality",
                           strength: float = 1.0, custom_params: str = "", preserve_details: bool = True,
                           clip=None, guidance: float = 7.5):
        """å…¨å›¾å¤„ç†ä¸»å‡½æ•°"""
        
        try:
            # è§£æè‡ªå®šä¹‰å‚æ•°
            custom_settings = {}
            if custom_params.strip():
                try:
                    custom_settings = json.loads(custom_params)
                except json.JSONDecodeError:
                    custom_settings = {}
            
            # æ‰§è¡Œå›¾åƒå¤„ç†
            processed_image, processing_info = self._apply_processing(
                image, processing_mode, strength, custom_settings, preserve_details
            )
            
            # ç”Ÿæˆå»ºè®®çš„æç¤ºè¯
            suggested_prompt = self._generate_prompt_for_mode(processing_mode, strength)
            
            # åˆ›å»ºconditioningï¼ˆå¦‚æœæœ‰CLIPï¼‰
            if clip is not None:
                conditioning = self._create_conditioning(clip, suggested_prompt, guidance)
            else:
                conditioning = self._create_fallback_conditioning(suggested_prompt, guidance)
            
            # åˆ›å»ºå¤„ç†å…ƒæ•°æ®
            metadata = {
                "processing_mode": processing_mode,
                "strength": strength,
                "preserve_details": preserve_details,
                "guidance": guidance,
                "custom_params": custom_settings,
                "processing_info": processing_info,
                "timestamp": datetime.now().isoformat()
            }
            
            # åˆ›å»ºå¤„ç†æ—¥å¿—
            log = self._create_processing_log(processing_mode, processing_info, strength)
            
            return (
                processed_image,
                json.dumps(metadata, indent=2),
                suggested_prompt,
                conditioning,
                log
            )
            
        except Exception as e:
            return self._create_fallback_output(image, str(e))
    
    def _apply_processing(self, image: torch.Tensor, mode: str, strength: float, 
                         custom_settings: dict, preserve_details: bool) -> Tuple[torch.Tensor, dict]:
        """åº”ç”¨å›¾åƒå¤„ç†"""
        
        try:
            from PIL import Image, ImageEnhance, ImageFilter
            import cv2
            
            # è½¬æ¢ä¸ºPILå›¾åƒ
            if len(image.shape) == 4:
                img_array = image[0].cpu().numpy()
            else:
                img_array = image.cpu().numpy()
                
            if img_array.max() <= 1.0:
                img_array = (img_array * 255).astype(np.uint8)
            else:
                img_array = img_array.astype(np.uint8)
                
            pil_image = Image.fromarray(img_array, 'RGB')
            original_size = pil_image.size
            
            processing_info = {
                "original_size": original_size,
                "mode": mode,
                "strength": strength
            }
            
            # æ ¹æ®æ¨¡å¼è¿›è¡Œå¤„ç†
            if mode == "enhance_quality":
                processed_pil = self._enhance_quality(pil_image, strength, preserve_details)
                processing_info["enhancement"] = "quality_boost"
                
            elif mode in ["upscale_2x", "upscale_4x"]:
                scale_factor = 2 if mode == "upscale_2x" else 4
                processed_pil = self._upscale_image(pil_image, scale_factor, preserve_details)
                processing_info["scale_factor"] = scale_factor
                processing_info["new_size"] = processed_pil.size
                
            elif mode == "desaturate":
                processed_pil = self._desaturate_image(pil_image, strength)
                processing_info["saturation_level"] = 1.0 - strength
                
            elif mode in ["monochrome_warm", "monochrome_cool"]:
                processed_pil = self._apply_monochrome(pil_image, mode, strength)
                processing_info["monochrome_type"] = mode
                
            elif mode == "vintage_film":
                processed_pil = self._apply_vintage_film(pil_image, strength)
                processing_info["vintage_intensity"] = strength
                
            elif mode == "high_contrast":
                processed_pil = self._apply_high_contrast(pil_image, strength)
                processing_info["contrast_boost"] = strength
                
            elif mode == "soft_blur":
                processed_pil = self._apply_soft_blur(pil_image, strength)
                processing_info["blur_radius"] = strength * 2
                
            elif mode == "sharpen":
                processed_pil = self._apply_sharpen(pil_image, strength)
                processing_info["sharpen_factor"] = strength
                
            elif mode == "denoise":
                processed_pil = self._apply_denoise(pil_image, strength)
                processing_info["denoise_level"] = strength
                
            elif mode == "color_pop":
                processed_pil = self._apply_color_pop(pil_image, strength)
                processing_info["saturation_boost"] = strength
                
            elif mode == "dramatic_lighting":
                processed_pil = self._apply_dramatic_lighting(pil_image, strength)
                processing_info["lighting_intensity"] = strength
                
            elif mode == "custom":
                processed_pil = self._apply_custom_processing(pil_image, custom_settings, strength)
                processing_info["custom_applied"] = custom_settings
                
            else:
                processed_pil = pil_image
                processing_info["warning"] = f"Unknown mode: {mode}"
            
            # è½¬æ¢å›torch tensor
            processed_array = np.array(processed_pil)
            processed_tensor = torch.from_numpy(processed_array).float() / 255.0
            
            # ä¿æŒåŸå§‹ç»´åº¦
            if len(image.shape) == 4:
                processed_tensor = processed_tensor.unsqueeze(0)
                
            return processed_tensor, processing_info
            
        except Exception as e:
            # å¦‚æœå¤„ç†å¤±è´¥ï¼Œè¿”å›åŸå›¾
            processing_info = {"error": str(e), "fallback": "original_image"}
            return image, processing_info
    
    def _enhance_quality(self, image: Image.Image, strength: float, preserve_details: bool) -> Image.Image:
        """è´¨é‡å¢å¼º"""
        # é”åŒ–
        sharpness = ImageEnhance.Sharpness(image)
        image = sharpness.enhance(1.0 + strength * 0.3)
        
        # å¯¹æ¯”åº¦å¢å¼º
        contrast = ImageEnhance.Contrast(image)
        image = contrast.enhance(1.0 + strength * 0.2)
        
        # è‰²å½©é¥±å’Œåº¦
        color = ImageEnhance.Color(image)
        image = color.enhance(1.0 + strength * 0.1)
        
        return image
    
    def _upscale_image(self, image: Image.Image, scale_factor: int, preserve_details: bool) -> Image.Image:
        """å›¾åƒæ”¾å¤§"""
        width, height = image.size
        new_size = (width * scale_factor, height * scale_factor)
        
        # ä½¿ç”¨LANCZOSç®—æ³•è¿›è¡Œé«˜è´¨é‡æ”¾å¤§
        if preserve_details:
            upscaled = image.resize(new_size, Image.Resampling.LANCZOS)
            # åå¤„ç†é”åŒ–
            sharpness = ImageEnhance.Sharpness(upscaled)
            upscaled = sharpness.enhance(1.1)
        else:
            upscaled = image.resize(new_size, Image.Resampling.BICUBIC)
            
        return upscaled
    
    def _desaturate_image(self, image: Image.Image, strength: float) -> Image.Image:
        """å»è‰²å¤„ç†"""
        # è½¬æ¢ä¸ºç°åº¦ç„¶åæ··åˆ
        grayscale = image.convert('L').convert('RGB')
        
        # æ··åˆåŸå›¾å’Œç°åº¦å›¾
        if strength >= 1.0:
            return grayscale
        else:
            return Image.blend(image, grayscale, strength)
    
    def _apply_monochrome(self, image: Image.Image, mode: str, strength: float) -> Image.Image:
        """å•è‰²åŒ–å¤„ç†"""
        import numpy as np
        
        img_array = np.array(image)
        
        if mode == "monochrome_warm":
            # æš–è‰²è°ƒï¼šåé»„è¤è‰²
            tint_color = np.array([1.0, 0.95, 0.8])
        else:  # monochrome_cool
            # å†·è‰²è°ƒï¼šåè“è‰²
            tint_color = np.array([0.8, 0.95, 1.0])
        
        # è½¬ä¸ºç°åº¦
        gray = np.dot(img_array[...,:3], [0.299, 0.587, 0.114])
        
        # åº”ç”¨è‰²è°ƒ
        tinted = np.stack([gray * tint_color[0], gray * tint_color[1], gray * tint_color[2]], axis=2)
        tinted = np.clip(tinted, 0, 255).astype(np.uint8)
        
        result = Image.fromarray(tinted)
        
        # æ··åˆå¼ºåº¦
        return Image.blend(image, result, strength)
    
    def _apply_vintage_film(self, image: Image.Image, strength: float) -> Image.Image:
        """èƒ¶ç‰‡é£æ ¼"""
        # é™ä½å¯¹æ¯”åº¦
        contrast = ImageEnhance.Contrast(image)
        image = contrast.enhance(0.9)
        
        # å¢åŠ æš–è‰²è°ƒ
        import numpy as np
        img_array = np.array(image).astype(np.float32)
        
        # èƒ¶ç‰‡è‰²è°ƒè°ƒæ•´
        img_array[:,:,0] *= 1.1  # å¢åŠ çº¢è‰²
        img_array[:,:,1] *= 1.05  # ç•¥å¢ç»¿è‰²
        img_array[:,:,2] *= 0.95  # å‡å°‘è“è‰²
        
        img_array = np.clip(img_array, 0, 255).astype(np.uint8)
        result = Image.fromarray(img_array)
        
        # æ·»åŠ è½»å¾®æ¨¡ç³Š
        result = result.filter(ImageFilter.GaussianBlur(radius=0.5 * strength))
        
        return Image.blend(image, result, strength)
    
    def _apply_high_contrast(self, image: Image.Image, strength: float) -> Image.Image:
        """é«˜å¯¹æ¯”åº¦"""
        contrast = ImageEnhance.Contrast(image)
        return contrast.enhance(1.0 + strength * 0.8)
    
    def _apply_soft_blur(self, image: Image.Image, strength: float) -> Image.Image:
        """æŸ”å’Œæ¨¡ç³Š"""
        radius = strength * 2.0
        return image.filter(ImageFilter.GaussianBlur(radius=radius))
    
    def _apply_sharpen(self, image: Image.Image, strength: float) -> Image.Image:
        """é”åŒ–"""
        sharpness = ImageEnhance.Sharpness(image)
        return sharpness.enhance(1.0 + strength * 0.5)
    
    def _apply_denoise(self, image: Image.Image, strength: float) -> Image.Image:
        """é™å™ª"""
        # ä½¿ç”¨åŒè¾¹æ»¤æ³¢é™å™ª
        try:
            import cv2
            img_array = np.array(image)
            
            # åŒè¾¹æ»¤æ³¢å‚æ•°
            d = int(5 + strength * 5)  # é‚»åŸŸç›´å¾„
            sigma_color = 50 + strength * 50  # é¢œè‰²ç©ºé—´æ ‡å‡†å·®
            sigma_space = 50 + strength * 50  # åæ ‡ç©ºé—´æ ‡å‡†å·®
            
            denoised = cv2.bilateralFilter(img_array, d, sigma_color, sigma_space)
            return Image.fromarray(denoised)
        except:
            # å¦‚æœæ²¡æœ‰OpenCVï¼Œä½¿ç”¨ç®€å•çš„æ¨¡ç³Š
            return image.filter(ImageFilter.GaussianBlur(radius=strength * 0.5))
    
    def _apply_color_pop(self, image: Image.Image, strength: float) -> Image.Image:
        """è‰²å½©å¢å¼º"""
        # å¢åŠ é¥±å’Œåº¦
        color = ImageEnhance.Color(image)
        image = color.enhance(1.0 + strength * 0.4)
        
        # è½»å¾®å¢åŠ å¯¹æ¯”åº¦
        contrast = ImageEnhance.Contrast(image)
        image = contrast.enhance(1.0 + strength * 0.2)
        
        return image
    
    def _apply_dramatic_lighting(self, image: Image.Image, strength: float) -> Image.Image:
        """æˆå‰§æ€§å…‰çº¿"""
        import numpy as np
        
        img_array = np.array(image).astype(np.float32)
        
        # å¢åŠ å¯¹æ¯”åº¦
        img_array = (img_array - 128) * (1.0 + strength * 0.5) + 128
        
        # åˆ›å»ºæ¸æ™•æ•ˆæœ
        h, w = img_array.shape[:2]
        center_x, center_y = w // 2, h // 2
        
        y, x = np.ogrid[:h, :w]
        distance = np.sqrt((x - center_x)**2 + (y - center_y)**2)
        max_distance = np.sqrt(center_x**2 + center_y**2)
        
        # æ¸æ™•é®ç½©
        vignette = 1.0 - (distance / max_distance) * strength * 0.3
        vignette = np.clip(vignette, 0.7, 1.0)
        
        # åº”ç”¨æ¸æ™•
        for i in range(3):
            img_array[:,:,i] *= vignette
            
        img_array = np.clip(img_array, 0, 255).astype(np.uint8)
        return Image.fromarray(img_array)
    
    def _apply_custom_processing(self, image: Image.Image, custom_settings: dict, strength: float) -> Image.Image:
        """è‡ªå®šä¹‰å¤„ç†"""
        processed = image
        
        # æ”¯æŒçš„è‡ªå®šä¹‰å‚æ•°
        if "brightness" in custom_settings:
            brightness = ImageEnhance.Brightness(processed)
            processed = brightness.enhance(custom_settings["brightness"] * strength)
            
        if "contrast" in custom_settings:
            contrast = ImageEnhance.Contrast(processed)
            processed = contrast.enhance(custom_settings["contrast"] * strength)
            
        if "saturation" in custom_settings:
            color = ImageEnhance.Color(processed)
            processed = color.enhance(custom_settings["saturation"] * strength)
            
        if "sharpness" in custom_settings:
            sharpness = ImageEnhance.Sharpness(processed)
            processed = sharpness.enhance(custom_settings["sharpness"] * strength)
            
        if "blur_radius" in custom_settings:
            radius = custom_settings["blur_radius"] * strength
            processed = processed.filter(ImageFilter.GaussianBlur(radius=radius))
            
        return processed
    
    def _generate_prompt_for_mode(self, mode: str, strength: float) -> str:
        """æ ¹æ®å¤„ç†æ¨¡å¼ç”Ÿæˆå»ºè®®çš„æç¤ºè¯"""
        
        prompts = {
            "enhance_quality": "high quality, enhanced details, sharp focus, professional photography",
            "upscale_2x": "high resolution, detailed, crisp, upscaled image, enhanced quality",
            "upscale_4x": "ultra high resolution, extremely detailed, crystal clear, 4K quality",
            "desaturate": "monochrome, black and white, artistic, dramatic mood",
            "monochrome_warm": "sepia tone, warm monochrome, vintage aesthetic, nostalgic mood",
            "monochrome_cool": "cool monochrome, blue tint, modern artistic style, dramatic atmosphere",
            "vintage_film": "vintage film photography, analog aesthetic, retro style, film grain",
            "high_contrast": "high contrast, dramatic lighting, bold shadows, striking visual impact",
            "soft_blur": "soft focus, dreamy atmosphere, gentle blur, artistic bokeh effect",
            "sharpen": "sharp focus, crisp details, enhanced clarity, professional sharpness",
            "denoise": "clean image, smooth texture, noise-free, professional quality",
            "color_pop": "vibrant colors, enhanced saturation, vivid hues, colorful and bright",
            "dramatic_lighting": "dramatic lighting, cinematic mood, strong shadows, artistic illumination",
            "custom": "custom processed image, artistic enhancement, creative editing"
        }
        
        base_prompt = prompts.get(mode, "processed image")
        
        # æ ¹æ®å¼ºåº¦è°ƒæ•´
        if strength > 1.5:
            base_prompt += ", intense effect, bold artistic vision"
        elif strength > 1.0:
            base_prompt += ", enhanced effect, strong artistic style"
        elif strength < 0.5:
            base_prompt += ", subtle effect, gentle enhancement"
            
        return base_prompt
    
    def _create_conditioning(self, clip, prompt: str, guidance: float):
        """åˆ›å»ºCLIP conditioning"""
        try:
            tokens = clip.tokenize(prompt)
            cond, pooled = clip.encode_from_tokens(tokens, return_pooled=True)
            return [[cond, {"pooled_output": pooled, "guidance": guidance}]]
        except Exception as e:
            return self._create_fallback_conditioning(prompt, guidance)
    
    def _create_fallback_conditioning(self, prompt: str, guidance: float):
        """åˆ›å»ºfallback conditioning"""
        return [{
            "model_cond": prompt,
            "guidance": guidance,
            "type": "global_processing",
            "timestamp": datetime.now().isoformat()
        }]
    
    def _create_processing_log(self, mode: str, processing_info: dict, strength: float) -> str:
        """åˆ›å»ºå¤„ç†æ—¥å¿—"""
        log_lines = [
            f"Global Image Processing Log",
            f"=" * 40,
            f"Mode: {mode}",
            f"Strength: {strength}",
            f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        ]
        
        if "original_size" in processing_info:
            log_lines.append(f"Original Size: {processing_info['original_size']}")
            
        if "new_size" in processing_info:
            log_lines.append(f"New Size: {processing_info['new_size']}")
            
        if "error" in processing_info:
            log_lines.append(f"Error: {processing_info['error']}")
            
        for key, value in processing_info.items():
            if key not in ["original_size", "new_size", "error"]:
                log_lines.append(f"{key.replace('_', ' ').title()}: {value}")
                
        return "\n".join(log_lines)
    
    def _create_fallback_output(self, image: torch.Tensor, error_msg: str):
        """åˆ›å»ºfallbackè¾“å‡º"""
        fallback_metadata = {
            "status": "error",
            "error": error_msg,
            "timestamp": datetime.now().isoformat()
        }
        
        fallback_conditioning = [{
            "model_cond": "error in global processing",
            "type": "fallback"
        }]
        
        return (
            image,
            json.dumps(fallback_metadata),
            "error in processing",
            [fallback_conditioning],
            f"Error: {error_msg}"
        )

# Node registration
NODE_CLASS_MAPPINGS = {
    "GlobalImageProcessor": GlobalImageProcessor,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "GlobalImageProcessor": "ğŸŒ Global Image Processor",
}