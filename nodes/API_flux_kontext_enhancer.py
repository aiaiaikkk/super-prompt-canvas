"""
API Flux Kontext Enhancer Node
APIé›†æˆçš„Flux Kontextæç¤ºè¯å¢å¼ºèŠ‚ç‚¹

å°†VisualPromptEditorçš„æ ‡æ³¨æ•°æ®é€šè¿‡APIæ¨¡å‹è½¬æ¢ä¸º
Flux Kontextä¼˜åŒ–çš„ç»“æ„åŒ–ç¼–è¾‘æŒ‡ä»¤

æ”¯æŒå¤šä¸ªAPIæä¾›å•†:
- DeepSeek (Â¥0.001/1K tokens)
- Qianwen/åƒé—® (Â¥0.002/1K tokens)  
- OpenAI (Â¥0.015/1K tokens)
"""

import json
import time
import traceback
from typing import Dict, List, Any, Tuple, Optional
from datetime import datetime

try:
    import openai
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    OpenAI = None
    print("Warning: openai package not found. Please install with: pip install openai")

try:
    from aiohttp import web
    from server import PromptServer
    WEB_AVAILABLE = True
except ImportError:
    WEB_AVAILABLE = False


class APIFluxKontextEnhancer:
    """
    ğŸ¤– API Flux Kontext Enhancer
    
    é€šè¿‡APIæ¨¡å‹å°†VisualPromptEditorçš„æ ‡æ³¨æ•°æ®
    è½¬æ¢ä¸ºFlux Kontextä¼˜åŒ–çš„ç»“æ„åŒ–ç¼–è¾‘æŒ‡ä»¤
    """
    
    # ç±»çº§åˆ«çš„ç¼“å­˜å˜é‡
    _cached_models = {}
    _cache_timestamp = {}
    _cache_duration = 300  # ç¼“å­˜5åˆ†é’Ÿ
    
    # APIæä¾›å•†é…ç½®
    API_PROVIDERS = {
        "deepseek": {
            "name": "DeepSeek",
            "base_url": "https://api.deepseek.com/v1",
            "default_model": "deepseek-chat",
            "cost_per_1k": 0.001,
            "description": "é«˜æ€§ä»·æ¯”ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹"
        },
        "qianwen": {
            "name": "åƒé—®/Qianwen",
            "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
            "default_model": "qwen-turbo",
            "cost_per_1k": 0.002,
            "description": "é˜¿é‡Œäº‘åƒé—®æ¨¡å‹"
        },
        "openai": {
            "name": "OpenAI",
            "base_url": "https://api.openai.com/v1",
            "default_model": "gpt-3.5-turbo",
            "cost_per_1k": 0.015,
            "description": "OpenAIå®˜æ–¹æ¨¡å‹"
        }
    }
    
    @classmethod
    def get_available_models(cls, provider="deepseek", api_key=None, force_refresh=False):
        """åŠ¨æ€è·å–å¯ç”¨çš„APIæ¨¡å‹åˆ—è¡¨"""
        
        if not OPENAI_AVAILABLE:
            print("âŒ OpenAIåº“æœªå®‰è£…ï¼Œæ— æ³•è·å–APIæ¨¡å‹")
            return [cls.API_PROVIDERS[provider]["default_model"]]
            
        if not api_key:
            print(f"âŒ {provider} APIå¯†é’¥æœªæä¾›")
            return [cls.API_PROVIDERS[provider]["default_model"]]
            
        import time
        current_time = time.time()
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        if (not force_refresh and 
            provider in cls._cached_models and 
            provider in cls._cache_timestamp and
            current_time - cls._cache_timestamp[provider] < cls._cache_duration):
            print(f"ğŸ“‹ ä½¿ç”¨ç¼“å­˜çš„{provider}æ¨¡å‹åˆ—è¡¨: {cls._cached_models[provider]}")
            return cls._cached_models[provider]
        
        try:
            if not OPENAI_AVAILABLE or OpenAI is None:
                print(f"âŒ OpenAIåº“æœªå®‰è£…ï¼Œæ— æ³•è·å–{provider}æ¨¡å‹")
                return [cls.API_PROVIDERS[provider]["default_model"]]
            
            provider_config = cls.API_PROVIDERS.get(provider, cls.API_PROVIDERS["deepseek"])
            
            client = OpenAI(
                api_key=api_key,
                base_url=provider_config["base_url"]
            )
            
            # è·å–æ¨¡å‹åˆ—è¡¨
            models_response = client.models.list()
            model_names = []
            
            for model in models_response.data:
                model_names.append(model.id)
                print(f"âœ… {provider_config['name']} æ£€æµ‹åˆ°æ¨¡å‹: {model.id}")
            
            # å¦‚æœæ²¡æœ‰è·å–åˆ°æ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹
            if not model_names:
                model_names = [provider_config["default_model"]]
                print(f"âš ï¸ æœªè·å–åˆ°{provider}æ¨¡å‹åˆ—è¡¨ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹: {provider_config['default_model']}")
            
            # æ›´æ–°ç¼“å­˜
            cls._cached_models[provider] = model_names
            cls._cache_timestamp[provider] = current_time
            
            print(f"ğŸ”„ {provider_config['name']} æ¨¡å‹åˆ—è¡¨å·²æ›´æ–°ï¼Œå…±{len(model_names)}ä¸ªæ¨¡å‹")
            return model_names
            
        except Exception as e:
            print(f"âŒ è·å–{provider}æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}")
            # è¿”å›é»˜è®¤æ¨¡å‹
            default_model = cls.API_PROVIDERS[provider]["default_model"]
            return [default_model]
    
    @classmethod
    def INPUT_TYPES(cls):
        """å®šä¹‰èŠ‚ç‚¹è¾“å…¥ç±»å‹"""
        return {
            "required": {
                "api_provider": (["deepseek", "qianwen", "openai"], {
                    "default": "deepseek"
                }),
                "api_key": ("STRING", {
                    "default": "",
                    "multiline": False,
                    "placeholder": "Enter your API key here..."
                }),
                "model_name": ("STRING", {
                    "default": "deepseek-chat",
                    "multiline": False,
                    "placeholder": "Model name (auto-detected if API key provided)"
                }),
                "image": ("IMAGE",),
                "annotations_json": ("STRING", {
                    "default": "[]",
                    "multiline": True,
                    "placeholder": "Annotation data from VisualPromptEditor"
                }),
                "base_prompt": ("STRING", {
                    "default": "",
                    "multiline": True,
                    "placeholder": "Base prompt to enhance..."
                }),
                "style_preset": ([
                    "photorealistic",
                    "artistic",
                    "cinematic", 
                    "portrait",
                    "landscape",
                    "anime",
                    "concept_art",
                    "commercial",
                    "fashion",
                    "architectural"
                ], {
                    "default": "photorealistic"
                }),
                "enhancement_level": ([
                    "minimal",
                    "moderate", 
                    "comprehensive",
                    "professional"
                ], {
                    "default": "moderate"
                }),
                "language": (["chinese", "english", "bilingual"], {
                    "default": "chinese"
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.0,
                    "max": 2.0,
                    "step": 0.1
                }),
                "max_tokens": ("INT", {
                    "default": 1000,
                    "min": 100,
                    "max": 4000,
                    "step": 100
                }),
                "enable_caching": ("BOOLEAN", {
                    "default": True
                }),
                "debug_mode": ("BOOLEAN", {
                    "default": False
                })
            },
            "optional": {
                "custom_instructions": ("STRING", {
                    "default": "",
                    "multiline": True,
                    "placeholder": "Additional custom instructions..."
                }),
                "negative_prompt": ("STRING", {
                    "default": "",
                    "multiline": True,
                    "placeholder": "What to avoid in the image..."
                })
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING", "STRING", "STRING")
    RETURN_NAMES = ("enhanced_prompt", "kontext_instructions", "api_response", "debug_info")
    
    FUNCTION = "enhance_flux_instructions"
    CATEGORY = "KontextVisualPromptWindow/API"
    
    def __init__(self):
        self.cache = {}
        self.cache_max_size = 100
        self.session_stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "total_tokens": 0,
            "estimated_cost": 0.0
        }
    
    def _get_cache_key(self, annotations_json: str, base_prompt: str, 
                      style_preset: str, enhancement_level: str, 
                      language: str, model_name: str, temperature: float) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib
        content = f"{annotations_json}|{base_prompt}|{style_preset}|{enhancement_level}|{language}|{model_name}|{temperature}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _manage_cache(self):
        """ç®¡ç†ç¼“å­˜å¤§å°"""
        if len(self.cache) > self.cache_max_size:
            # åˆ é™¤æœ€æ—§çš„æ¡ç›®
            oldest_key = min(self.cache.keys(), key=lambda k: self.cache[k]['timestamp'])
            del self.cache[oldest_key]
    
    def _create_api_client(self, provider: str, api_key: str):
        """åˆ›å»ºAPIå®¢æˆ·ç«¯"""
        if not OPENAI_AVAILABLE or OpenAI is None:
            raise Exception("OpenAIåº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install openai")
        
        if not api_key:
            raise Exception(f"è¯·æä¾›{provider} APIå¯†é’¥")
        
        provider_config = self.API_PROVIDERS.get(provider, self.API_PROVIDERS["deepseek"])
        
        return OpenAI(
            api_key=api_key,
            base_url=provider_config["base_url"]
        )
    
    def _build_system_prompt(self, language: str, style_preset: str, 
                           enhancement_level: str) -> str:
        """æ„å»ºç³»ç»Ÿæç¤ºè¯"""
        
        language_instructions = {
            "chinese": "è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œä½¿ç”¨ä¸“ä¸šçš„å›¾åƒç¼–è¾‘å’ŒAIç»˜ç”»æœ¯è¯­ã€‚",
            "english": "Please respond in English using professional image editing and AI art terminology.",
            "bilingual": "Please provide responses in both Chinese and English, with Chinese first."
        }
        
        enhancement_instructions = {
            "minimal": "è¿›è¡ŒåŸºç¡€çš„æç¤ºè¯ä¼˜åŒ–ï¼Œä¿æŒç®€æ´",
            "moderate": "è¿›è¡Œä¸­ç­‰ç¨‹åº¦çš„æç¤ºè¯å¢å¼ºï¼Œå¹³è¡¡ç»†èŠ‚å’Œå¯è¯»æ€§",
            "comprehensive": "è¿›è¡Œå…¨é¢çš„æç¤ºè¯ä¼˜åŒ–ï¼ŒåŒ…å«ä¸°å¯Œçš„ç»†èŠ‚æè¿°",
            "professional": "è¿›è¡Œä¸“ä¸šçº§çš„æç¤ºè¯ä¼˜åŒ–ï¼Œé€‚åˆå•†ä¸šç”¨é€”"
        }
        
        style_instructions = {
            "photorealistic": "ä¸“æ³¨äºçœŸå®æ„Ÿæ‘„å½±æ•ˆæœï¼ŒåŒ…å«å…‰å½±ã€è´¨æ„Ÿç­‰ç»†èŠ‚",
            "artistic": "å¼ºè°ƒè‰ºæœ¯æ€§è¡¨è¾¾ï¼ŒåŒ…å«è‰²å½©ã€æ„å›¾ç­‰è‰ºæœ¯å…ƒç´ ",
            "cinematic": "ç”µå½±çº§è§†è§‰æ•ˆæœï¼ŒåŒ…å«é•œå¤´è¯­è¨€å’Œæ°›å›´è¥é€ ",
            "portrait": "äººåƒæ‘„å½±ä¸“ä¸šæŠ€æ³•ï¼ŒåŒ…å«è¡¨æƒ…ã€å…‰çº¿ã€æ„å›¾",
            "landscape": "é£æ™¯æ‘„å½±æŠ€æ³•ï¼ŒåŒ…å«è‡ªç„¶å…‰ã€æ™¯æ·±ã€æ„å›¾",
            "anime": "åŠ¨æ¼«é£æ ¼ç‰¹å¾ï¼ŒåŒ…å«è‰²å½©ã€çº¿æ¡ã€é£æ ¼åŒ–å¤„ç†",
            "concept_art": "æ¦‚å¿µè‰ºæœ¯é£æ ¼ï¼ŒåŒ…å«åˆ›æ„è®¾è®¡å’Œè§†è§‰æ¦‚å¿µ",
            "commercial": "å•†ä¸šæ‘„å½±æ ‡å‡†ï¼ŒåŒ…å«äº§å“å±•ç¤ºå’Œå“ç‰Œè°ƒæ€§",
            "fashion": "æ—¶å°šæ‘„å½±æŠ€æ³•ï¼ŒåŒ…å«é€ å‹ã€å…‰å½±ã€è¶‹åŠ¿å…ƒç´ ",
            "architectural": "å»ºç­‘æ‘„å½±æŠ€æ³•ï¼ŒåŒ…å«ç©ºé—´ã€çº¿æ¡ã€å…‰å½±"
        }
        
        return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIå›¾åƒç¼–è¾‘ä¸“å®¶ï¼Œç‰¹åˆ«æ“…é•¿Fluxæ¨¡å‹çš„Kontextç¼–è¾‘åŠŸèƒ½ã€‚

{language_instructions[language]}

ä»»åŠ¡ç›®æ ‡ï¼š
1. åˆ†æVisualPromptEditoræä¾›çš„æ ‡æ³¨æ•°æ®
2. å°†æ ‡æ³¨ä¿¡æ¯è½¬æ¢ä¸ºFlux Kontextä¼˜åŒ–çš„ç¼–è¾‘æŒ‡ä»¤
3. ç»“åˆåŸºç¡€æç¤ºè¯ç”Ÿæˆå¢å¼ºç‰ˆæç¤ºè¯
4. ç¡®ä¿æŒ‡ä»¤ç¬¦åˆ{style_preset}é£æ ¼çš„{enhancement_instructions[enhancement_level]}è¦æ±‚

é£æ ¼æŒ‡å¯¼ï¼š
{style_instructions[style_preset]}

è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
1. enhanced_prompt: å¢å¼ºåçš„å®Œæ•´æç¤ºè¯
2. kontext_instructions: Flux Kontextæ ¼å¼çš„ç¼–è¾‘æŒ‡ä»¤
3. ç¡®ä¿æç¤ºè¯è‡ªç„¶æµç•…ï¼Œç¬¦åˆAIç»˜ç”»æœ€ä½³å®è·µ
4. åŒ…å«é€‚å½“çš„æŠ€æœ¯å‚æ•°å’Œè´¨é‡å…³é”®è¯

è¯·å§‹ç»ˆéµå¾ªä¸“ä¸šçš„AIå›¾åƒç¼–è¾‘æ ‡å‡†å’ŒFluxæ¨¡å‹çš„æœ€ä½³å®è·µã€‚"""
    
    def _build_user_prompt(self, annotations_json: str, base_prompt: str, 
                          custom_instructions: str = "", negative_prompt: str = "") -> str:
        """æ„å»ºç”¨æˆ·æç¤ºè¯"""
        
        user_prompt = f"""è¯·åˆ†æä»¥ä¸‹å†…å®¹å¹¶ç”Ÿæˆä¼˜åŒ–çš„Flux Kontextç¼–è¾‘æŒ‡ä»¤ï¼š

**æ ‡æ³¨æ•°æ®ï¼š**
```json
{annotations_json}
```

**åŸºç¡€æç¤ºè¯ï¼š**
{base_prompt}

**è‡ªå®šä¹‰æŒ‡ä»¤ï¼š**
{custom_instructions}

**è´Ÿé¢æç¤ºè¯ï¼š**
{negative_prompt}

è¯·ç”Ÿæˆï¼š
1. **enhanced_prompt** - å¢å¼ºåçš„å®Œæ•´æç¤ºè¯
2. **kontext_instructions** - Flux Kontextæ ¼å¼çš„ç¼–è¾‘æŒ‡ä»¤

ç¡®ä¿è¾“å‡ºçš„æŒ‡ä»¤èƒ½å¤Ÿç²¾ç¡®æ§åˆ¶å›¾åƒç¼–è¾‘ï¼ŒåŒæ—¶ä¿æŒè‡ªç„¶æµç•…çš„è¯­è¨€è¡¨è¾¾ã€‚"""
        
        return user_prompt
    
    def _generate_with_api(self, client, model_name: str, 
                         system_prompt: str, user_prompt: str, 
                         temperature: float, max_tokens: int, 
                         provider: str) -> Tuple[str, Dict[str, Any]]:
        """ä½¿ç”¨APIç”Ÿæˆå†…å®¹"""
        try:
            response = client.chat.completions.create(
                model=model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=temperature,
                max_tokens=max_tokens,
                stream=False
            )
            
            # æå–å“åº”å†…å®¹
            generated_text = response.choices[0].message.content
            
            # è®¡ç®—æˆæœ¬
            provider_config = self.API_PROVIDERS[provider]
            prompt_tokens = response.usage.prompt_tokens
            completion_tokens = response.usage.completion_tokens
            total_tokens = response.usage.total_tokens
            
            estimated_cost = (total_tokens / 1000) * provider_config["cost_per_1k"]
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.session_stats["total_requests"] += 1
            self.session_stats["successful_requests"] += 1
            self.session_stats["total_tokens"] += total_tokens
            self.session_stats["estimated_cost"] += estimated_cost
            
            response_info = {
                "provider": provider_config["name"],
                "model": model_name,
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens,
                "total_tokens": total_tokens,
                "estimated_cost": estimated_cost,
                "cost_currency": "CNY",
                "timestamp": datetime.now().isoformat()
            }
            
            return generated_text, response_info
            
        except Exception as e:
            self.session_stats["total_requests"] += 1
            error_msg = f"APIè°ƒç”¨å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            
            response_info = {
                "error": error_msg,
                "provider": provider,
                "model": model_name,
                "timestamp": datetime.now().isoformat()
            }
            
            raise Exception(error_msg)
    
    def _parse_api_response(self, response_text: str) -> Tuple[str, str]:
        """è§£æAPIå“åº”å¹¶æå–å¢å¼ºæç¤ºè¯å’ŒKontextæŒ‡ä»¤"""
        try:
            # å°è¯•é€šè¿‡æ ‡è®°åˆ†ç¦»å†…å®¹
            enhanced_prompt = ""
            kontext_instructions = ""
            
            lines = response_text.split('\n')
            current_section = None
            
            for line in lines:
                line = line.strip()
                
                if 'enhanced_prompt' in line.lower() or 'å¢å¼ºæç¤ºè¯' in line:
                    current_section = 'enhanced'
                    continue
                elif 'kontext_instructions' in line.lower() or 'kontext' in line.lower():
                    current_section = 'kontext'
                    continue
                elif line.startswith('**') and line.endswith('**'):
                    # å¯èƒ½æ˜¯æ–°çš„sectionæ ‡é¢˜
                    if 'å¢å¼º' in line or 'enhanced' in line.lower():
                        current_section = 'enhanced'
                    elif 'kontext' in line.lower() or 'ç¼–è¾‘æŒ‡ä»¤' in line:
                        current_section = 'kontext'
                    continue
                
                if current_section == 'enhanced' and line:
                    enhanced_prompt += line + '\n'
                elif current_section == 'kontext' and line:
                    kontext_instructions += line + '\n'
            
            # å¦‚æœè§£æå¤±è´¥ï¼Œå°†æ•´ä¸ªå“åº”ä½œä¸ºå¢å¼ºæç¤ºè¯
            if not enhanced_prompt.strip():
                enhanced_prompt = response_text
            
            return enhanced_prompt.strip(), kontext_instructions.strip()
            
        except Exception as e:
            print(f"âš ï¸ è§£æAPIå“åº”å¤±è´¥: {str(e)}")
            return response_text, ""
    
    def enhance_flux_instructions(self, api_provider, api_key, model_name, image, 
                                annotations_json, base_prompt, style_preset, 
                                enhancement_level, language, temperature, max_tokens, 
                                enable_caching, debug_mode, custom_instructions="", 
                                negative_prompt=""):
        """ä¸»è¦å¤„ç†å‡½æ•°"""
        
        try:
            start_time = time.time()
            
            # è¾“å…¥éªŒè¯
            if not api_key or not api_key.strip():
                return (
                    "é”™è¯¯ï¼šè¯·æä¾›æœ‰æ•ˆçš„APIå¯†é’¥",
                    "",
                    json.dumps({"error": "APIå¯†é’¥ä¸ºç©º"}, ensure_ascii=False, indent=2),
                    "APIå¯†é’¥éªŒè¯å¤±è´¥"
                )
            
            # æ£€æŸ¥ç¼“å­˜
            cache_key = None
            if enable_caching:
                cache_key = self._get_cache_key(
                    annotations_json, base_prompt, style_preset, 
                    enhancement_level, language, model_name, temperature
                )
                
                if cache_key in self.cache:
                    cached_result = self.cache[cache_key]
                    if debug_mode:
                        print(f"ğŸ¯ ä½¿ç”¨ç¼“å­˜ç»“æœ: {cache_key}")
                    return (
                        cached_result['enhanced_prompt'],
                        cached_result['kontext_instructions'],
                        cached_result['api_response'],
                        f"ç¼“å­˜å‘½ä¸­ | ç”Ÿæˆæ—¶é—´: {cached_result['generation_time']:.2f}ç§’"
                    )
            
            # åˆ›å»ºAPIå®¢æˆ·ç«¯
            client = self._create_api_client(api_provider, api_key)
            
            # æ„å»ºæç¤ºè¯
            system_prompt = self._build_system_prompt(language, style_preset, enhancement_level)
            user_prompt = self._build_user_prompt(annotations_json, base_prompt, custom_instructions, negative_prompt)
            
            if debug_mode:
                print(f"ğŸ” ç³»ç»Ÿæç¤ºè¯: {system_prompt[:200]}...")
                print(f"ğŸ” ç”¨æˆ·æç¤ºè¯: {user_prompt[:200]}...")
            
            # è°ƒç”¨API
            response_text, response_info = self._generate_with_api(
                client, model_name, system_prompt, user_prompt, 
                temperature, max_tokens, api_provider
            )
            
            # è§£æå“åº”
            enhanced_prompt, kontext_instructions = self._parse_api_response(response_text)
            
            # ç”Ÿæˆè°ƒè¯•ä¿¡æ¯
            generation_time = time.time() - start_time
            debug_info = f"""ç”Ÿæˆå®Œæˆ | æä¾›å•†: {response_info.get('provider', api_provider)} | æ¨¡å‹: {model_name} | 
æ—¶é—´: {generation_time:.2f}ç§’ | Token: {response_info.get('total_tokens', 0)} | 
æˆæœ¬: Â¥{response_info.get('estimated_cost', 0):.4f}"""
            
            # å‡†å¤‡è¿”å›çš„APIå“åº”ä¿¡æ¯
            api_response = json.dumps({
                "response_info": response_info,
                "generation_time": generation_time,
                "session_stats": self.session_stats,
                "raw_response": response_text[:500] + "..." if len(response_text) > 500 else response_text
            }, ensure_ascii=False, indent=2)
            
            # ç¼“å­˜ç»“æœ
            if enable_caching and cache_key:
                self._manage_cache()
                self.cache[cache_key] = {
                    'enhanced_prompt': enhanced_prompt,
                    'kontext_instructions': kontext_instructions,
                    'api_response': api_response,
                    'generation_time': generation_time,
                    'timestamp': time.time()
                }
            
            return (enhanced_prompt, kontext_instructions, api_response, debug_info)
            
        except Exception as e:
            error_msg = f"å¤„ç†å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            
            error_response = json.dumps({
                "error": error_msg,
                "traceback": traceback.format_exc(),
                "timestamp": datetime.now().isoformat()
            }, ensure_ascii=False, indent=2)
            
            return (
                f"é”™è¯¯ï¼š{error_msg}",
                "",
                error_response,
                f"å¤„ç†å¤±è´¥ | é”™è¯¯: {error_msg}"
            )
    
    @classmethod
    def IS_CHANGED(cls, **kwargs):
        """æ£€æŸ¥è¾“å…¥æ˜¯å¦æ”¹å˜"""
        # å¯¹äºAPIè°ƒç”¨ï¼Œæ€»æ˜¯é‡æ–°ç”Ÿæˆä»¥ç¡®ä¿æœ€æ–°ç»“æœ
        return float("nan")


# æ³¨å†ŒèŠ‚ç‚¹
NODE_CLASS_MAPPINGS = {
    "APIFluxKontextEnhancer": APIFluxKontextEnhancer
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "APIFluxKontextEnhancer": "ğŸ¤– API Flux Kontext Enhancer"
}

# æ·»åŠ Web UIæ”¯æŒ
if WEB_AVAILABLE:
    @PromptServer.instance.routes.get("/kontextapi/providers")
    async def get_providers(request):
        """è·å–å¯ç”¨çš„APIæä¾›å•†"""
        providers = []
        for key, config in APIFluxKontextEnhancer.API_PROVIDERS.items():
            providers.append({
                "id": key,
                "name": config["name"],
                "cost_per_1k": config["cost_per_1k"],
                "description": config["description"],
                "default_model": config["default_model"]
            })
        return web.json_response(providers)
    
    @PromptServer.instance.routes.post("/kontextapi/models")
    async def get_models(request):
        """è·å–æŒ‡å®šæä¾›å•†çš„æ¨¡å‹åˆ—è¡¨"""
        data = await request.json()
        provider = data.get("provider", "deepseek")
        api_key = data.get("api_key", "")
        
        try:
            models = APIFluxKontextEnhancer.get_available_models(provider, api_key)
            return web.json_response({"models": models})
        except Exception as e:
            return web.json_response({"error": str(e)}, status=400)