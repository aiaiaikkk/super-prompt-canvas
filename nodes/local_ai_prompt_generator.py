"""
Custom Model Prompt Generator
æ”¯æŒåŠ è½½å¾®è°ƒ+é‡åŒ–çš„æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºæç¤ºè¯ç”Ÿæˆ

æ”¯æŒçš„æ¨¡å‹ï¼š
- qwen-8b-instruct (å¾®è°ƒ + 4ä½é‡åŒ–)
- deepseek-7b-base (å¾®è°ƒ + 4ä½é‡åŒ–)
"""

import os
import sys
import json
import torch
import traceback
from typing import Optional, Dict, Any, Tuple, List
import folder_paths
import glob

# å°è¯•å¯¼å…¥llama-cpp-python
try:
    from llama_cpp import Llama
    LLAMA_CPP_AVAILABLE = True
except ImportError:
    LLAMA_CPP_AVAILABLE = False

# è·å–ä¸“ç”¨æ¨¡å‹ç›®å½•è·¯å¾„
def get_custom_model_directory():
    """è·å–è‡ªå®šä¹‰æ¨¡å‹ç›®å½•è·¯å¾„"""
    # ComfyUI/models/custom_prompt_models/
    # __file__ = /path/to/ComfyUI/custom_nodes/kontext-super-prompt/nodes/local_ai_prompt_generator.py
    # éœ€è¦å‘ä¸Š4çº§: nodes -> kontext-super-prompt -> custom_nodes -> ComfyUI
    current_file = os.path.abspath(__file__)
    nodes_dir = os.path.dirname(current_file)  # .../nodes/
    plugin_dir = os.path.dirname(nodes_dir)   # .../kontext-super-prompt/
    custom_nodes_dir = os.path.dirname(plugin_dir)  # .../custom_nodes/
    comfyui_root = os.path.dirname(custom_nodes_dir)  # .../ComfyUI/
    
    models_dir = os.path.join(comfyui_root, "models", "custom_prompt_models")
    
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(models_dir, exist_ok=True)
    return models_dir

def scan_model_files():
    """æ‰«ææ¨¡å‹ç›®å½•ä¸­çš„.ggufæ–‡ä»¶"""
    model_dir = get_custom_model_directory()
    gguf_files = glob.glob(os.path.join(model_dir, "*.gguf"))
    
    # è°ƒè¯•ä¿¡æ¯
    
    # è¿”å›æ–‡ä»¶åï¼ˆä¸å«è·¯å¾„ï¼‰
    model_names = [os.path.basename(f) for f in gguf_files]
    
    if not model_names:
        model_names = ["è¯·å°†.ggufæ¨¡å‹æ–‡ä»¶æ”¾å…¥models/custom_prompt_modelsç›®å½•"]
    else:
        pass
    
    return model_names

def detect_model_names():
    """ä»æ¨¡å‹æ–‡ä»¶åæå–æ¨¡å‹åç§°"""
    model_files = scan_model_files()
    model_names = []
    
    for model_file in model_files:
        if model_file == "è¯·å°†.ggufæ¨¡å‹æ–‡ä»¶æ”¾å…¥models/custom_prompt_modelsç›®å½•":
            continue
            
        # æå–æ¨¡å‹åç§°ï¼ˆå»é™¤æ‰©å±•åï¼‰
        model_name = os.path.splitext(model_file)[0]
        if model_name not in model_names:
            model_names.append(model_name)
    
    if not model_names:
        model_names = ["è¯·å…ˆæ·»åŠ æ¨¡å‹æ–‡ä»¶"]
    
    return model_names


class CustomModelPromptGenerator:
    """
    Custom Model Prompt Generator
    æ”¯æŒå¾®è°ƒ+é‡åŒ–æ¨¡å‹çš„ComfyUIèŠ‚ç‚¹
    """
    
    def __init__(self):
        self.model = None
        self.current_model_path = None
        self.model_cache = {}
        
        # é»˜è®¤çš„æç¤ºè¯æ¨¡æ¿
        self.prompt_templates = {
            "qwen": {
                "system": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å›¾åƒç¼–è¾‘æç¤ºè¯ç”Ÿæˆå™¨ã€‚æ ¹æ®ç”¨æˆ·çš„ç¼–è¾‘è¦æ±‚ï¼Œç”Ÿæˆç²¾ç¡®çš„ã€ç»“æ„åŒ–çš„æç¤ºè¯ã€‚",
                "template": "<|im_start|>system\n{system}<|im_end|>\n<|im_start|>user\n{user_input}<|im_end|>\n<|im_start|>assistant\n"
            },
            "deepseek": {
                "system": "You are a professional image editing prompt generator. Generate precise, structured prompts based on user editing requirements.",
                "template": "### System:\n{system}\n\n### User:\n{user_input}\n\n### Assistant:\n"
            }
        }
    
    @classmethod
    def INPUT_TYPES(cls):
        model_files = scan_model_files()
        model_names = detect_model_names()
        return {
            "required": {
                "editing_request": ("STRING", {
                    "multiline": True,
                    "default": "è¯·æè¿°ä½ æƒ³è¦çš„å›¾åƒç¼–è¾‘æ•ˆæœ",
                    "placeholder": "ä¾‹å¦‚ï¼šå°†èƒŒæ™¯æ”¹ä¸ºè“å¤©ç™½äº‘ï¼Œå¢åŠ æ¸©æš–çš„é˜³å…‰æ•ˆæœ",
                    "rows": 4
                }),
                "model_name": (model_names, {
                    "default": model_names[0] if model_names else "è¯·å…ˆæ·»åŠ æ¨¡å‹æ–‡ä»¶"
                }),
                "model_file": (model_files, {
                    "default": model_files[0] if model_files else "è¯·å°†.ggufæ¨¡å‹æ–‡ä»¶æ”¾å…¥models/custom_prompt_modelsç›®å½•"
                }),
                "max_tokens": ("INT", {
                    "default": 512,
                    "min": 64,
                    "max": 2048,
                    "step": 64
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.1,
                    "max": 2.0,
                    "step": 0.1
                }),
                "top_p": ("FLOAT", {
                    "default": 0.9,
                    "min": 0.1,
                    "max": 1.0,
                    "step": 0.05
                })
            },
            "optional": {
                "layers_info": ("LAYERS_INFO",),
                "image": ("IMAGE",),
                "custom_system_prompt": ("STRING", {
                    "multiline": True,
                    "default": "",
                    "placeholder": "è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰",
                    "rows": 3
                })
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("enhanced_prompt", "raw_output")
    FUNCTION = "generate_prompt"
    CATEGORY = "ğŸ¨ Super Canvas"
    
    def load_model(self, model_file: str) -> bool:
        """åŠ è½½é‡åŒ–æ¨¡å‹"""
        if not LLAMA_CPP_AVAILABLE:
            raise Exception("llama-cpp-python not installed. Please run: pip install llama-cpp-python")
        
        # æ„å»ºå®Œæ•´çš„æ¨¡å‹è·¯å¾„
        model_dir = get_custom_model_directory()
        model_path = os.path.join(model_dir, model_file)
        
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»åŠ è½½äº†ç›¸åŒçš„æ¨¡å‹
        if self.current_model_path == model_path and self.model is not None:
            return True
        
        try:
            
            # æ ¹æ®æ¨¡å‹ç±»å‹è®¾ç½®å‚æ•°
            model_params = {
                "model_path": model_path,
                "n_ctx": 4096,  # ä¸Šä¸‹æ–‡é•¿åº¦
                "n_batch": 512,  # æ‰¹å¤„ç†å¤§å°
                "n_threads": -1,  # ä½¿ç”¨æ‰€æœ‰CPUçº¿ç¨‹
                "verbose": False
            }
            
            # æ£€æŸ¥æ˜¯å¦æœ‰GPUæ”¯æŒ
            if torch.cuda.is_available():
                model_params["n_gpu_layers"] = -1  # ä½¿ç”¨æ‰€æœ‰GPUå±‚
            
            self.model = Llama(**model_params)
            self.current_model_path = model_path
            
            return True
            
        except Exception as e:
            traceback.print_exc()
            self.model = None
            self.current_model_path = None
            return False
    
    def build_prompt(self, editing_request: str, model_name: str, custom_system: str = "") -> str:
        """æ„å»ºé€‚åˆæ¨¡å‹çš„æç¤ºè¯"""
        
        # æ ¹æ®æ¨¡å‹åç§°é€‰æ‹©æ¨¡æ¿
        if "qwen" in model_name.lower():
            template_config = self.prompt_templates["qwen"]
        elif "deepseek" in model_name.lower():
            template_config = self.prompt_templates["deepseek"]
        else:
            # å¯¹äºæœªçŸ¥æ¨¡å‹ï¼Œé»˜è®¤ä½¿ç”¨é€šç”¨æ ¼å¼
            template_config = self.prompt_templates["deepseek"]  # ä½¿ç”¨æ›´é€šç”¨çš„æ ¼å¼
        
        # ä½¿ç”¨è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯æˆ–é»˜è®¤æç¤ºè¯
        system_prompt = custom_system.strip() if custom_system.strip() else template_config["system"]
        
        # æ„å»ºç”¨æˆ·è¾“å…¥
        user_input = f"""è¯·æ ¹æ®ä»¥ä¸‹å›¾åƒç¼–è¾‘è¦æ±‚ï¼Œç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„ã€ç»“æ„åŒ–çš„æç¤ºè¯ï¼š

ç¼–è¾‘è¦æ±‚ï¼š{editing_request}

è¯·ç”Ÿæˆç¬¦åˆä»¥ä¸‹æ ¼å¼çš„æç¤ºè¯ï¼š
1. ä¸»è¦ç¼–è¾‘å†…å®¹æè¿°
2. é£æ ¼å’Œè´¨é‡è¦æ±‚  
3. æŠ€æœ¯å‚æ•°å»ºè®®

è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
- ä½¿ç”¨è‹±æ–‡æè¿°
- ç»“æ„æ¸…æ™°ï¼Œé€—å·åˆ†éš”
- åŒ…å«è´¨é‡æ ‡ç­¾å¦‚ "high quality, detailed, professional"
- é¿å…è´Ÿé¢æè¿°"""
        
        # åº”ç”¨æ¨¡æ¿
        full_prompt = template_config["template"].format(
            system=system_prompt,
            user_input=user_input
        )
        
        return full_prompt
    
    def generate_prompt(self, editing_request: str, model_name: str, model_file: str, 
                       max_tokens: int, temperature: float, top_p: float,
                       layers_info=None, image=None, custom_system_prompt: str = "") -> Tuple[str, str]:
        """ç”Ÿæˆå¢å¼ºçš„æç¤ºè¯"""
        
        try:
            # éªŒè¯æ¨¡å‹æ–‡ä»¶
            if not model_file.strip() or model_file == "è¯·å°†.ggufæ¨¡å‹æ–‡ä»¶æ”¾å…¥models/custom_prompt_modelsç›®å½•":
                raise ValueError("è¯·é€‰æ‹©æœ‰æ•ˆçš„æ¨¡å‹æ–‡ä»¶")
            
            if not model_file.endswith('.gguf'):
                raise ValueError("æ¨¡å‹æ–‡ä»¶å¿…é¡»æ˜¯ .gguf æ ¼å¼")
            
            # å¤„ç†è¾“å…¥æ•°æ®
            enhanced_request = editing_request
            if layers_info:
                # å¤„ç†Super Canvasçš„å›¾å±‚ä¿¡æ¯
                if isinstance(layers_info, dict):
                    layer_count = len(layers_info.get('layers', [])) if 'layers' in layers_info else 0
                    if layer_count > 0:
                        layer_info = f"\nå›¾å±‚ä¿¡æ¯: {layer_count} ä¸ªå›¾å±‚"
                        enhanced_request += layer_info
                elif isinstance(layers_info, (list, tuple)):
                    layer_info = f"\nå›¾å±‚ä¿¡æ¯: {len(layers_info)} ä¸ªå›¾å±‚"
                    enhanced_request += layer_info
                else:
                    # å…¶ä»–æ ¼å¼çš„å›¾å±‚ä¿¡æ¯
                    enhanced_request += f"\nå›¾å±‚æ•°æ®: {str(layers_info)[:100]}..."
            
            # åŠ è½½æ¨¡å‹
            if not self.load_model(model_file):
                raise Exception("æ¨¡å‹åŠ è½½å¤±è´¥")
            
            # æ„å»ºæç¤ºè¯
            full_prompt = self.build_prompt(enhanced_request, model_name, custom_system_prompt)
            
            
            # ç”Ÿæˆå‚æ•°
            generation_params = {
                "prompt": full_prompt,
                "max_tokens": max_tokens,
                "temperature": temperature,
                "top_p": top_p,
                "stop": ["<|im_end|>", "### User:", "\n\n###"],  # åœæ­¢æ ‡è®°
                "echo": False
            }
            
            # æ‰§è¡Œæ¨ç†
            response = self.model(**generation_params)
            
            # æå–ç”Ÿæˆçš„æ–‡æœ¬
            raw_output = response['choices'][0]['text'].strip()
            
            # åå¤„ç†ï¼šæå–æœ‰æ•ˆçš„æç¤ºè¯éƒ¨åˆ†
            enhanced_prompt = self.post_process_output(raw_output)
            
            
            return (enhanced_prompt, raw_output)
            
        except Exception as e:
            error_msg = f"æç¤ºè¯ç”Ÿæˆå¤±è´¥: {str(e)}"
            traceback.print_exc()
            return (error_msg, str(e))
    
    def post_process_output(self, raw_output: str) -> str:
        """åå¤„ç†æ¨¡å‹è¾“å‡ºï¼Œæå–å¹²å‡€çš„æç¤ºè¯"""
        
        # ç§»é™¤å¸¸è§çš„å‰ç¼€å’Œåç¼€
        prefixes_to_remove = [
            "æ ¹æ®æ‚¨çš„ç¼–è¾‘è¦æ±‚",
            "ä»¥ä¸‹æ˜¯ç”Ÿæˆçš„æç¤ºè¯",
            "ç”Ÿæˆçš„æç¤ºè¯å¦‚ä¸‹",
            "Based on your editing request",
            "Here is the generated prompt",
            "The generated prompt is"
        ]
        
        processed = raw_output.strip()
        
        # ç§»é™¤å‰ç¼€
        for prefix in prefixes_to_remove:
            if processed.lower().startswith(prefix.lower()):
                processed = processed[len(prefix):].strip()
                if processed.startswith("ï¼š") or processed.startswith(":"):
                    processed = processed[1:].strip()
        
        # ç§»é™¤å¤šä½™çš„æ¢è¡Œå’Œç©ºæ ¼
        processed = " ".join(processed.split())
        
        # ç¡®ä¿ä»¥é«˜è´¨é‡æ ‡ç­¾ç»“å°¾
        quality_tags = ["high quality", "detailed", "professional", "8k", "masterpiece"]
        has_quality_tag = any(tag in processed.lower() for tag in quality_tags)
        
        if not has_quality_tag:
            processed += ", high quality, detailed, professional"
        
        return processed

# Web APIæ¥å£ - ç”¨äºåŠ¨æ€åˆ·æ–°æ¨¡å‹åˆ—è¡¨
try:
    from server import PromptServer
    from aiohttp import web
    WEB_API_AVAILABLE = True
except ImportError:
    WEB_API_AVAILABLE = False

if WEB_API_AVAILABLE:
    @PromptServer.instance.routes.post("/custom_model_generator/refresh_models")
    async def refresh_custom_models(request):
        """åˆ·æ–°è‡ªå®šä¹‰æ¨¡å‹åˆ—è¡¨API"""
        try:
            # é‡æ–°æ‰«ææ¨¡å‹ç›®å½•
            model_files = scan_model_files()
            model_names = detect_model_names()
            
            
            return web.json_response({
                "success": True,
                "model_files": model_files,
                "model_names": model_names,
                "count": len([f for f in model_files if f != "è¯·å°†.ggufæ¨¡å‹æ–‡ä»¶æ”¾å…¥models/custom_prompt_modelsç›®å½•"])
            })
            
        except Exception as e:
            return web.json_response({
                "success": False,
                "error": str(e)
            }, status=500)

# æ³¨å†ŒèŠ‚ç‚¹
NODE_CLASS_MAPPINGS = {
    "CustomModelPromptGenerator": CustomModelPromptGenerator
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "CustomModelPromptGenerator": "ğŸ¤– Custom Model Prompt Generator"
}