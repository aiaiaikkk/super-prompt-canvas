"""
Simple Annotation Service
Simple annotation service using ComfyUI built-in models

Directly uses existing ComfyUI model resources without additional loading
"""

import logging
from typing import Dict, List, Optional, Tuple, Union
import uuid
from datetime import datetime

import numpy as np
import torch
from PIL import Image
import cv2

logger = logging.getLogger(__name__)

class SimpleAnnotationService:
    """Simple annotation service based on ComfyUI built-in models"""
    
    def __init__(self):
        self.yolo_model = None
        self.sam_model = None
        self._check_comfyui_models()
    
    def _check_comfyui_models(self):
        """Check available models in ComfyUI"""
        try:
            # Try to import ComfyUI model management
            import folder_paths
            import comfy.model_management as model_management
            
            # Check YOLO models
            yolo_models = folder_paths.get_filename_list("yolo")
            if yolo_models:
                logger.info(f"âœ… Found YOLO models in ComfyUI: {yolo_models}")
            
            # Check SAM models  
            sam_models = folder_paths.get_filename_list("sam")
            if sam_models:
                logger.info(f"âœ… Found SAM models in ComfyUI: {sam_models}")
                
            # Check other segmentation models
            checkpoints = folder_paths.get_filename_list("checkpoints")
            logger.info(f"ðŸ“‹ Available checkpoints: {len(checkpoints)} models")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Could not access ComfyUI models: {e}")
    
    def annotate_image(self, 
                      image: Union[Image.Image, np.ndarray, torch.Tensor],
                      detection_mode: str = "standard",
                      confidence_threshold: float = 0.7,
                      max_objects: int = 50,
                      include_masks: bool = True) -> Dict:
        """
        Intelligent annotation using ComfyUI built-in models
        """
        try:
            # Image preprocessing
            pil_image = self._preprocess_image(image)
            
            # Try to use ComfyUI detection nodes
            detections = self._detect_with_comfyui(pil_image, confidence_threshold, max_objects)
            
            # If ComfyUI detection fails, use simple region detection
            if not detections:
                detections = self._simple_region_detection(pil_image, max_objects)
            
            # Generate layer data
            layers = self._create_layers(detections, pil_image.size, include_masks)
            
            # Create metadata
            metadata = {
                "processing_time": 0.05,
                "model_version": "comfyui_integrated",
                "image_size": list(pil_image.size),
                "detection_mode": detection_mode,
                "confidence_threshold": confidence_threshold,
                "objects_detected": len(layers),
                "timestamp": datetime.now().isoformat()
            }
            
            logger.info(f"ðŸŽ¯ Simple annotation complete: {len(layers)} regions detected")
            
            return {
                "layers": layers,
                "metadata": metadata,
                "status": "success"
            }
            
        except Exception as e:
            logger.error(f"âŒ Annotation failed: {e}")
            return {
                "layers": [],
                "metadata": {"error": str(e), "status": "failed"},
                "status": "error"
            }
    
    def _preprocess_image(self, image: Union[Image.Image, np.ndarray, torch.Tensor]) -> Image.Image:
        """Preprocess image"""
        if isinstance(image, torch.Tensor):
            # Convert from tensor
            if len(image.shape) == 4:
                image = image.squeeze(0)
            if image.shape[0] == 3:  # CHW format
                image = image.permute(1, 2, 0)
            image_np = (image.cpu().numpy() * 255).astype(np.uint8)
            return Image.fromarray(image_np)
        elif isinstance(image, np.ndarray):
            return Image.fromarray(image)
        elif isinstance(image, Image.Image):
            return image
        else:
            raise ValueError(f"Unsupported image type: {type(image)}")
    
    def _detect_with_comfyui(self, image: Image.Image, confidence: float, max_objects: int) -> List[Dict]:
        """Try to use ComfyUI built-in detection functionality"""
        try:
            # Here we can call ComfyUI's YOLO or other detection nodes
            # Temporarily return empty list, letting it fall back to simple detection
            return []
            
        except Exception as e:
            logger.warning(f"ComfyUI detection failed: {e}")
            return []
    
    def _simple_region_detection(self, image: Image.Image, max_objects: int) -> List[Dict]:
        """Simple region detection algorithm"""
        try:
            # Convert to numpy array
            img_array = np.array(image)
            height, width = img_array.shape[:2]
            
            # Convert to grayscale
            if len(img_array.shape) == 3:
                gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
            else:
                gray = img_array
            
            # Use simple edge detection and contour finding
            edges = cv2.Canny(gray, 50, 150)
            contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            detections = []
            for i, contour in enumerate(contours[:max_objects]):
                # Calculate bounding box
                x, y, w, h = cv2.boundingRect(contour)
                area = cv2.contourArea(contour)
                
                # Filter out regions that are too small
                if area < (width * height * 0.01):  # Regions smaller than 1% of total area
                    continue
                
                # Calculate "confidence" (based on area size)
                confidence = min(area / (width * height * 0.1), 1.0)
                
                detections.append({
                    "bbox": [x, y, x + w, y + h],
                    "confidence": confidence,
                    "class_id": 0,
                    "class_name": f"region_{i+1}",
                    "area": area
                })
            
            # Sort by confidence
            detections.sort(key=lambda x: x["confidence"], reverse=True)
            
            return detections[:max_objects]
            
        except Exception as e:
            logger.error(f"Simple detection failed: {e}")
            return self._fallback_detection(image, max_objects)
    
    def _fallback_detection(self, image: Image.Image, max_objects: int) -> List[Dict]:
        """Fallback detection (grid segmentation)"""
        width, height = image.size
        
        # Create grid regions
        detections = []
        grid_size = 3  # 3x3 grid
        cell_w = width // grid_size
        cell_h = height // grid_size
        
        for i in range(grid_size):
            for j in range(grid_size):
                if len(detections) >= max_objects:
                    break
                    
                x1 = j * cell_w
                y1 = i * cell_h
                x2 = min((j + 1) * cell_w, width)
                y2 = min((i + 1) * cell_h, height)
                
                detections.append({
                    "bbox": [x1, y1, x2, y2],
                    "confidence": 0.5,
                    "class_id": 0,
                    "class_name": f"grid_{i}_{j}"
                })
        
        return detections
    
    def _create_layers(self, detections: List[Dict], image_size: Tuple[int, int], 
                      include_masks: bool) -> List[Dict]:
        """Create layer data from detection results"""
        layers = []
        
        for i, detection in enumerate(detections):
            bbox = detection["bbox"]
            layer_id = f"simple_{uuid.uuid4().hex[:8]}"
            
            layer_data = {
                "id": layer_id,
                "type": "detection",
                "name": f"{detection['class_name']}",
                "geometry": {
                    "type": "rectangle",
                    "coordinates": bbox
                },
                "style": {
                    "fill": f"rgba({50 + i*30}, {100 + i*20}, {200 - i*15}, 0.3)",
                    "stroke": f"#{50 + i*30:02x}{100 + i*20:02x}{200 - i*15:02x}",
                    "strokeWidth": 2
                },
                "confidence": detection["confidence"],
                "class_name": detection["class_name"],
                "class_id": detection["class_id"],
                "visible": True,
                "locked": False,
                "area": detection.get("area", 0)
            }
            
            # Add simple mask (if needed)
            if include_masks:
                layer_data["mask"] = self._generate_simple_mask(bbox, image_size)
            
            layers.append(layer_data)
        
        return layers
    
    def _generate_simple_mask(self, bbox: List[float], image_size: Tuple[int, int]) -> str:
        """Generate simple rectangular mask"""
        x1, y1, x2, y2 = [int(coord) for coord in bbox]
        width, height = image_size
        
        # Create mask
        mask = np.zeros((height, width), dtype=np.uint8)
        mask[y1:y2, x1:x2] = 255
        
        # Encode as base64
        _, buffer = cv2.imencode('.png', mask)
        import base64
        mask_b64 = base64.b64encode(buffer).decode('utf-8')
        
        return mask_b64

# Global service instance
_simple_service = None

def get_simple_annotation_service() -> SimpleAnnotationService:
    """Get global simple annotation service instance"""
    global _simple_service
    if _simple_service is None:
        _simple_service = SimpleAnnotationService()
    return _simple_service

# Convenience function
def simple_annotate(image, **kwargs) -> Dict:
    """Convenient simple annotation function"""
    service = get_simple_annotation_service()
    return service.annotate_image(image, **kwargs)

# Exports
__all__ = [
    "SimpleAnnotationService", 
    "get_simple_annotation_service", 
    "simple_annotate"
]